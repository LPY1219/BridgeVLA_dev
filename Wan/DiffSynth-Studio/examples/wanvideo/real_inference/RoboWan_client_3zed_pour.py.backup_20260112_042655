"""
RoboWan Client for Real Robot Control
Provides both test client and real robot control loop
"""

# ç¦ç”¨ä»£ç†ï¼Œç¡®ä¿å¯ä»¥è¿æ¥æœ¬åœ°SSHéš§é“
import os
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('all_proxy', None)
os.environ.pop('ALL_PROXY', None)

# è§£å†³ZED SDKä¸PyTorch/CUDAçš„TLSå†²çª
# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥é¿å…OpenMPå†²çª
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'

# é¢„åŠ è½½libgompä»¥é¿å…TLSåˆå§‹åŒ–é”™è¯¯
import ctypes
try:
    # å°è¯•é¢„åŠ è½½libgompï¼ˆOpenMPåº“ï¼‰
    ctypes.CDLL('libgomp.so.1', mode=ctypes.RTLD_GLOBAL)
except:
    pass

# å…ˆå¯¼å…¥ZEDç›¸å…³çš„åº“ï¼Œå†å¯¼å…¥PyTorch
try:
    import pyzed.sl as sl
except:
    pass

import requests
from PIL import Image
import io
import numpy as np
from typing import List, Optional
from pathlib import Path
from datetime import datetime
import rospy
import open3d as o3d
import sys
import torch
import cv2
import matplotlib.pyplot as plt
import atexit

# æ·»åŠ è®­ç»ƒä»£ç è·¯å¾„
diffsynth_path = "/media/casia/data4/lpy/RoboWan/BridgeVLA_dev/Wan/DiffSynth-Studio"
sys.path.insert(0, diffsynth_path)

# å¯¼å…¥æœºå™¨äººæ¥å£æ¨¡å—
try:
    from robot_interface import RobotController, action_to_pose
    ROBOT_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Robot interface not available: {e}")
    ROBOT_AVAILABLE = False

# å¯¼å…¥ç›¸æœºæ¥å£æ¨¡å—
try:
    # ä½¿ç”¨æ”¯æŒä¸‰ç›¸æœºçš„Cameraç±»
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„å¯¼å…¥
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_collection_path = os.path.join(current_dir, "../../../../../data_collection")
    data_collection_path = os.path.abspath(data_collection_path)
    sys.path.insert(0, data_collection_path)
    from real_camera_utils_lpy import Camera, get_cam_extrinsic
    CAMERA_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Camera interface not available: {e}")
    CAMERA_AVAILABLE = False

# ====================== æŠ•å½±æ¨¡å¼é…ç½® ======================
# âš ï¸ é‡è¦ï¼šå¿…é¡»ä¸æœåŠ¡å™¨ç«¯çš„ use_different_projection ä¿æŒä¸€è‡´ï¼
# True: ä½¿ç”¨ä¸åŒæŠ•å½±æ¨¡å¼ï¼ˆæ¯ä¸ªç›¸æœºå•ç‹¬æŠ•å½±åˆ°æœ€ä½³è§†è§’ï¼‰
# False: ä½¿ç”¨é»˜è®¤æŠ•å½±æ¨¡å¼ï¼ˆç‚¹äº‘æ‹¼æ¥åç»Ÿä¸€æŠ•å½±ï¼‰
USE_DIFFERENT_PROJECTION = True  # ä¸æœåŠ¡å™¨ run_server.sh ä¸­çš„é…ç½®ä¿æŒä¸€è‡´


def get_projection_interface_class(use_different_projection: bool):
    """
    æ ¹æ®æŠ•å½±æ¨¡å¼è¿”å›å¯¹åº”çš„ProjectionInterfaceç±»å’Œç›¸å…³å‡½æ•°

    Args:
        use_different_projection: æ˜¯å¦ä½¿ç”¨ä¸åŒæŠ•å½±æ¨¡å¼ï¼ˆæ¯ä¸ªç›¸æœºå•ç‹¬æŠ•å½±ï¼‰

    Returns:
        tuple: (ProjectionInterface, build_extrinsic_matrix, convert_pcd_to_base, _norm_rgb)
    """
    if use_different_projection:
        from diffsynth.trainers.base_multi_view_dataset_with_rot_grip_3cam_different_projection import (
            ProjectionInterface,
            build_extrinsic_matrix,
            convert_pcd_to_base,
            _norm_rgb
        )
        print("[ProjectionInterface] Using DIFFERENT projection mode (3cam_different_projection)")
        print("  -> æ¯ä¸ªç›¸æœºçš„ç‚¹äº‘å•ç‹¬æŠ•å½±åˆ°æœ€ä½³è§†è§’")
    else:
        from diffsynth.trainers.base_multi_view_dataset_with_rot_grip_3cam import (
            ProjectionInterface,
            build_extrinsic_matrix,
            convert_pcd_to_base,
            _norm_rgb
        )
        print("[ProjectionInterface] Using DEFAULT projection mode (3cam)")
        print("  -> ç‚¹äº‘æ‹¼æ¥åç»Ÿä¸€æŠ•å½±åˆ°æ‰€æœ‰è§†è§’")
    return ProjectionInterface, build_extrinsic_matrix, convert_pcd_to_base, _norm_rgb


# å¯¼å…¥è®­ç»ƒä»£ç ä¸­çš„é¢„å¤„ç†æ¨¡å—ï¼ˆæ ¹æ®æŠ•å½±æ¨¡å¼é€‰æ‹©ï¼‰
try:
    ProjectionInterface, build_extrinsic_matrix, convert_pcd_to_base, _norm_rgb = \
        get_projection_interface_class(USE_DIFFERENT_PROJECTION)
    PREPROCESSING_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Failed to import projection modules: {e}")
    PREPROCESSING_AVAILABLE = False


# ====================== é…ç½®å‚æ•°ï¼ˆå…¨å±€ï¼‰ ======================
# âš ï¸ é‡è¦ï¼šåªéœ€åœ¨æ­¤å¤„ä¿®æ”¹ä¸€æ¬¡å³å¯ï¼
# åœºæ™¯è¾¹ç•Œ [x_min, y_min, z_min, x_max, y_max, z_max]
# æ³¨æ„ï¼šå¿…é¡»ä¸æœåŠ¡å™¨ç«¯çš„ scene_bounds ä¿æŒå®Œå…¨ä¸€è‡´ï¼
#
# é»˜è®¤å€¼ï¼š[0, -0.7, -0.05, 0.8, 0.7, 0.65]
# å«ä¹‰ï¼š
#   x: [0, 0.8] ç±³
#   y: [-0.7, 0.7] ç±³
#   z: [-0.05, 0.65] ç±³
# SCENE_BOUNDS = [0, -0.45, -0.05, 0.8, 0.55, 0.6]
# SCENE_BOUNDS=[0, -0.55, -0.05, 0.8, 0.45, 0.6]
SCENE_BOUNDS=[-0.1,-0.5,-0.1,0.9,0.5,0.9]
# å›¾åƒå°ºå¯¸
# æ³¨æ„ï¼šå¿…é¡»ä¸æœåŠ¡å™¨ç«¯çš„ img_size ä¿æŒå®Œå…¨ä¸€è‡´ï¼
IMG_SIZE = 256

# æ¯æ¬¡æ¨ç†åæ‰§è¡Œçš„åŠ¨ä½œæ­¥æ•°
# æ§åˆ¶æ¯æ¬¡æ¨¡å‹æ¨ç†åå®é™…æ‰§è¡Œå¤šå°‘æ­¥åŠ¨ä½œ
EXECUTION_STEP = 20

# æ¯æ¬¡é¢„æµ‹çš„å¸§æ•°ï¼ˆåŒ…æ‹¬åˆå§‹å¸§ï¼‰
# ä¾‹å¦‚ï¼š49 è¡¨ç¤ºé¢„æµ‹49å¸§ï¼Œå³åˆå§‹å¸§ + 48å¸§åŠ¨ä½œ
NUM_FRAMES = 49

# ç‚¹äº‘é…ç½®
# True: ä½¿ç”¨ä¸‰ä¸ªç›¸æœºæ‹¼æ¥çš„ç‚¹äº‘
# False: åªä½¿ç”¨ç›¸æœº1çš„ç‚¹äº‘ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
USE_MERGED_POINTCLOUD = False

# å†å²å¸§é…ç½®
# å†å²å¸§æ•°é‡é…ç½®ï¼ˆå¿…é¡»ä¸æœåŠ¡å™¨ç«¯ NUM_HISTORY_FRAMES ä¿æŒä¸€è‡´ï¼‰
# å…è®¸çš„å€¼ï¼š1ï¼ˆå•å¸§ï¼‰, 2ï¼ˆä¸¤å¸§ï¼‰, æˆ– 1+4Nï¼ˆ5, 9, 13, ...ï¼‰
# å½“è®¾ç½®ä¸º 1 æ—¶ï¼Œä½¿ç”¨å•å¸§æ¨¡å¼ï¼ˆç¦ç”¨å†å²ï¼‰
# å½“è®¾ç½®ä¸º >1 æ—¶ï¼Œä½¿ç”¨å¤šå¸§å†å²æ¨¡å¼
NUM_HISTORY_FRAMES = 1

# åŠ¨ä½œæˆªæ–­é…ç½®
# æ˜¯å¦å¯ç”¨åŠ¨ä½œæˆªæ–­ï¼ˆé˜²æ­¢è¿‡å¤§çš„åŠ¨ä½œå˜åŒ–ï¼‰
ENABLE_ACTION_CLIPPING = True
# ä½ç½®æœ€å¤§å˜åŒ–é‡ï¼ˆç±³ï¼‰- é»˜è®¤4cm
MAX_POSITION_CHANGE = 0.04
# æ—‹è½¬æœ€å¤§å˜åŒ–é‡ï¼ˆåº¦ï¼‰- é»˜è®¤10åº¦
MAX_ROTATION_CHANGE = 10.0

# Pour ä»»åŠ¡çš„åˆå§‹å…³èŠ‚é…ç½®ï¼ˆä» teleop_lpy_pour.py è·å–ï¼‰
RESET_JOINTS_POUR = np.array([5.84113346e-02, -3.28556887e-01, -5.57625597e-06, -2.61241058e+00, 5.14608518e-02, 2.26069062e+00, 7.70146633e-01])

class RoboWanClient:
    """Client for communicating with RoboWan server"""

    def __init__(self,
                 server_url: str = "http://localhost:5555",
                 img_size: int = 256,
                 scene_bounds: List[float] = None,  # é»˜è®¤ä½¿ç”¨å…¨å±€ SCENE_BOUNDS
                 sigma: float = 1.5,
                 augmentation: bool = False,
                 use_merged_pointcloud: bool = None,  # é»˜è®¤ä½¿ç”¨å…¨å±€ USE_MERGED_POINTCLOUD
                 num_history_frames: int = None):  # é»˜è®¤ä½¿ç”¨å…¨å±€ NUM_HISTORY_FRAMES
        """
        Initialize client

        Args:
            server_url: URL of the server (e.g., "http://localhost:5555")
            img_size: å›¾åƒå°ºå¯¸
            scene_bounds: åœºæ™¯è¾¹ç•Œ
            sigma: heatmapé«˜æ–¯åˆ†å¸ƒæ ‡å‡†å·®
            augmentation: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼ºï¼ˆæ¨ç†æ—¶é€šå¸¸ä¸ºFalseï¼‰
            use_merged_pointcloud: æ˜¯å¦ä½¿ç”¨æ‹¼æ¥åçš„ç‚¹äº‘ï¼ˆTrueï¼‰æˆ–åªä½¿ç”¨ç›¸æœº1çš„ç‚¹äº‘ï¼ˆFalseï¼‰
            num_history_frames: å†å²å¸§æ•°é‡ï¼ˆ1, 2, æˆ– 1+4Nï¼‰
        """
        self.server_url = server_url
        self.predict_url = f"{server_url}/predict"
        self.health_url = f"{server_url}/health"
        self.config_url = f"{server_url}/config"

        # è®°å½•å®¢æˆ·ç«¯çš„æŠ•å½±æ¨¡å¼é…ç½®
        self.use_different_projection = USE_DIFFERENT_PROJECTION

        # åˆå§‹åŒ–é¢„å¤„ç†å‚æ•°
        self.img_size = (img_size, img_size)
        self.scene_bounds = scene_bounds if scene_bounds is not None else SCENE_BOUNDS
        self.sigma = sigma
        self.augmentation = augmentation
        self.mode = "test"  # æ¨ç†æ¨¡å¼
        self.use_merged_pointcloud = use_merged_pointcloud if use_merged_pointcloud is not None else USE_MERGED_POINTCLOUD  # ç‚¹äº‘èåˆæ¨¡å¼
        self.num_history_frames = num_history_frames if num_history_frames is not None else NUM_HISTORY_FRAMES  # å†å²å¸§æ•°é‡

        # åˆå§‹åŒ–å†å²å¸§ç¼“å†²åŒº
        # å­˜å‚¨æ ¼å¼: List[Tuple[List[Image.Image], List[Image.Image]]] - æ¯ä¸ªå…ƒç´ ä¸º (heatmaps, rgbs)
        # heatmaps: List[Image.Image] - å¤šè§†è§’çƒ­åŠ›å›¾ (num_views,)
        # rgbs: List[Image.Image] - å¤šè§†è§’RGBå›¾åƒ (num_views,)
        self.history_buffer = []

        print(f"Client initialized with num_history_frames={self.num_history_frames}")

        # åˆå§‹åŒ–ä¸‰ä¸ªç›¸æœºçš„å¤–å‚çŸ©é˜µï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
        # å¤–å‚çŸ©é˜µå°†ä»get_cam_extrinsicå‡½æ•°åŠ¨æ€è·å–
        # è¿™ä¸è®­ç»ƒæ—¶datasetä»extrinsics.pklåŠ è½½çš„æ–¹å¼ä¿æŒä¸€è‡´
        self.extrinsic_matrix_1 = get_cam_extrinsic("3rd_1")
        self.extrinsic_matrix_2 = get_cam_extrinsic("3rd_2")
        self.extrinsic_matrix_3 = get_cam_extrinsic("3rd_3")

        # åˆå§‹åŒ–æŠ•å½±æ¥å£
        if PREPROCESSING_AVAILABLE:
            self.projection_interface = ProjectionInterface(
                img_size=img_size,
                rend_three_views=True,
                add_depth=False
            )
        else:
            self.projection_interface = None
            print("Warning: ProjectionInterface not available!")

    def check_health(self) -> bool:
        """Check if server is healthy"""
        try:
            response = requests.get(self.health_url, timeout=5)
            return response.status_code == 200
        except Exception as e:
            print(f"Health check failed: {e}")
            return False

    def get_server_config(self) -> dict:
        """
        è·å–æœåŠ¡å™¨é…ç½®

        Returns:
            Dictionary containing server configuration:
                - use_different_projection: bool
                - scene_bounds: List[float]
                - img_size: int
                - num_frames: int
            å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å› None
        """
        try:
            response = requests.get(self.config_url, timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"Failed to get server config: HTTP {response.status_code}")
                return None
        except Exception as e:
            print(f"Failed to get server config: {e}")
            return None

    def verify_config_consistency(self) -> bool:
        """
        éªŒè¯å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é…ç½®çš„ä¸€è‡´æ€§

        æ£€æŸ¥é¡¹ï¼š
        1. use_different_projection æŠ•å½±æ¨¡å¼
        2. scene_bounds åœºæ™¯è¾¹ç•Œ
        3. img_size å›¾åƒå°ºå¯¸

        Returns:
            bool: é…ç½®æ˜¯å¦ä¸€è‡´
        """
        print("\n" + "="*60)
        print("ğŸ” éªŒè¯å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é…ç½®ä¸€è‡´æ€§...")
        print("="*60)

        server_config = self.get_server_config()
        if server_config is None:
            print("âŒ æ— æ³•è·å–æœåŠ¡å™¨é…ç½®ï¼Œè·³è¿‡ä¸€è‡´æ€§æ£€æŸ¥")
            print("   è­¦å‘Šï¼šè¯·ç¡®ä¿æ‰‹åŠ¨æ£€æŸ¥é…ç½®ä¸€è‡´æ€§ï¼")
            return True  # è¿”å›Trueå…è®¸ç»§ç»­ï¼Œä½†ç»™å‡ºè­¦å‘Š

        all_consistent = True

        # 1. æ£€æŸ¥ use_different_projection
        server_use_different_projection = server_config.get('use_different_projection', None)
        client_use_different_projection = self.use_different_projection

        print(f"\nğŸ“Œ æŠ•å½±æ¨¡å¼ (use_different_projection):")
        print(f"   å®¢æˆ·ç«¯: {client_use_different_projection}")
        print(f"   æœåŠ¡å™¨: {server_use_different_projection}")

        if server_use_different_projection is not None:
            if client_use_different_projection != server_use_different_projection:
                print(f"   âŒ ä¸ä¸€è‡´ï¼")
                print(f"   è¯·ä¿®æ”¹å®¢æˆ·ç«¯ USE_DIFFERENT_PROJECTION = {server_use_different_projection}")
                all_consistent = False
            else:
                print(f"   âœ“ ä¸€è‡´")
        else:
            print(f"   âš ï¸  æœåŠ¡å™¨æœªè¿”å›æ­¤é…ç½®")

        # 2. æ£€æŸ¥ scene_bounds
        server_scene_bounds = server_config.get('scene_bounds', None)
        client_scene_bounds = list(self.scene_bounds)

        print(f"\nğŸ“Œ åœºæ™¯è¾¹ç•Œ (scene_bounds):")
        print(f"   å®¢æˆ·ç«¯: {client_scene_bounds}")
        print(f"   æœåŠ¡å™¨: {server_scene_bounds}")

        if server_scene_bounds is not None:
            tolerance = 1e-6
            bounds_match = all(
                abs(c - s) < tolerance
                for c, s in zip(client_scene_bounds, server_scene_bounds)
            )
            if not bounds_match:
                print(f"   âŒ ä¸ä¸€è‡´ï¼")
                print(f"   è¯·ä¿®æ”¹å®¢æˆ·ç«¯ SCENE_BOUNDS = {server_scene_bounds}")
                all_consistent = False
            else:
                print(f"   âœ“ ä¸€è‡´")
        else:
            print(f"   âš ï¸  æœåŠ¡å™¨æœªè¿”å›æ­¤é…ç½®")

        # 3. æ£€æŸ¥ img_size
        server_img_size = server_config.get('img_size', None)
        client_img_size = self.img_size[0]  # å‡è®¾æ˜¯æ­£æ–¹å½¢

        print(f"\nğŸ“Œ å›¾åƒå°ºå¯¸ (img_size):")
        print(f"   å®¢æˆ·ç«¯: {client_img_size}")
        print(f"   æœåŠ¡å™¨: {server_img_size}")

        if server_img_size is not None:
            if client_img_size != server_img_size:
                print(f"   âŒ ä¸ä¸€è‡´ï¼")
                print(f"   è¯·ä¿®æ”¹å®¢æˆ·ç«¯ IMG_SIZE = {server_img_size}")
                all_consistent = False
            else:
                print(f"   âœ“ ä¸€è‡´")
        else:
            print(f"   âš ï¸  æœåŠ¡å™¨æœªè¿”å›æ­¤é…ç½®")

        # æ€»ç»“
        print("\n" + "-"*60)
        if all_consistent:
            print("âœ“ æ‰€æœ‰é…ç½®ä¸€è‡´ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œ")
        else:
            print("âŒ å‘ç°é…ç½®ä¸ä¸€è‡´ï¼")
            print("   è¯·ä¿®æ”¹å®¢æˆ·ç«¯é…ç½®åé‡æ–°è¿è¡Œï¼Œæˆ–ä¿®æ”¹æœåŠ¡å™¨é…ç½®åé‡å¯æœåŠ¡å™¨")
        print("="*60 + "\n")

        return all_consistent

    def predict(
        self,
        heatmap_images: List[Image.Image],
        rgb_images: List[Image.Image],
        prompt: str,
        initial_rotation: List[float],
        initial_gripper: int,
        num_frames: int = 12
    ) -> dict:
        """
        Send prediction request to server

        Args:
            heatmap_images: List of PIL Images for heatmap (multi-view) - å½“å‰å¸§
            rgb_images: List of PIL Images for RGB (multi-view) - å½“å‰å¸§
            prompt: Task instruction
            initial_rotation: Initial rotation [roll, pitch, yaw] in degrees
            initial_gripper: Initial gripper state (0 or 1)
            num_frames: Number of frames to predict

        Returns:
            Dictionary containing:
                - success: bool
                - rotation: List[List[float]] - (num_frames, 3)
                - gripper: List[int] - (num_frames,)
                - error: str (if failed)
        """
        # æ›´æ–°å†å²ç¼“å†²åŒºï¼šä¿å­˜å½“å‰å¸§
        self.history_buffer.append((heatmap_images, rgb_images))

        # ä¿æŒç¼“å†²åŒºå¤§å°ä¸º num_history_frames
        if len(self.history_buffer) > self.num_history_frames:
            self.history_buffer.pop(0)

        # Prepare files
        files = []

        # åˆ¤æ–­ä½¿ç”¨å•å¸§æ¨¡å¼è¿˜æ˜¯å¤šå¸§å†å²æ¨¡å¼
        if self.num_history_frames == 1:
            # å•å¸§æ¨¡å¼ï¼šåªå‘é€å½“å‰å¸§
            print(f"  [Client] Using single-frame mode")

            # Add heatmap images
            for i, img in enumerate(heatmap_images):
                buffer = io.BytesIO()
                img.save(buffer, format='PNG')
                buffer.seek(0)
                files.append(('heatmap_images', (f'heatmap_{i}.png', buffer, 'image/png')))

            # Add RGB images
            for i, img in enumerate(rgb_images):
                buffer = io.BytesIO()
                img.save(buffer, format='PNG')
                buffer.seek(0)
                files.append(('rgb_images', (f'rgb_{i}.png', buffer, 'image/png')))

        else:
            # å¤šå¸§å†å²æ¨¡å¼ï¼šå‘é€æ‰€æœ‰å†å²å¸§
            # å¦‚æœç¼“å†²åŒºæœªæ»¡ï¼Œé‡å¤æœ€å‰é¢çš„ä¸€å¸§æ¥å¡«å……ï¼ˆä¸è®­ç»ƒæ—¶æ•°æ®é›†é€»è¾‘ä¸€è‡´ï¼‰
            history_frames = []
            if len(self.history_buffer) < self.num_history_frames:
                # ç¼“å†²åŒºæœªæ»¡ï¼Œé‡å¤ç¬¬ä¸€å¸§æ¥å¡«å……
                num_padding = self.num_history_frames - len(self.history_buffer)
                first_frame = self.history_buffer[0]
                history_frames = [first_frame] * num_padding + list(self.history_buffer)
                print(f"  [Client] Using multi-frame history mode ({len(self.history_buffer)} frames + {num_padding} padding = {self.num_history_frames} total)")
            else:
                # ç¼“å†²åŒºå·²æ»¡ï¼Œç›´æ¥ä½¿ç”¨
                history_frames = list(self.history_buffer)
                print(f"  [Client] Using multi-frame history mode ({len(history_frames)} frames)")

            img_idx = 0
            for hist_idx, (hist_heatmaps, hist_rgbs) in enumerate(history_frames):
                # Add heatmap images for this history frame
                for view_idx, img in enumerate(hist_heatmaps):
                    buffer = io.BytesIO()
                    img.save(buffer, format='PNG')
                    buffer.seek(0)
                    files.append(('heatmap_images', (f'heatmap_h{hist_idx}_v{view_idx}.png', buffer, 'image/png')))
                    img_idx += 1

            img_idx = 0
            for hist_idx, (hist_heatmaps, hist_rgbs) in enumerate(history_frames):
                # Add RGB images for this history frame
                for view_idx, img in enumerate(hist_rgbs):
                    buffer = io.BytesIO()
                    img.save(buffer, format='PNG')
                    buffer.seek(0)
                    files.append(('rgb_images', (f'rgb_h{hist_idx}_v{view_idx}.png', buffer, 'image/png')))
                    img_idx += 1

        # Prepare form data
        rotation_str = ','.join(map(str, initial_rotation))
        scene_bounds_str = ','.join(map(str, self.scene_bounds))
        data = {
            'prompt': prompt,
            'initial_rotation': rotation_str,
            'initial_gripper': initial_gripper,
            'num_frames': num_frames,
            'scene_bounds': scene_bounds_str
        }

        try:
            # Send request (timeout=600ç§’=10åˆ†é’Ÿï¼Œå› ä¸ºæ¨ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´)
            response = requests.post(self.predict_url, files=files, data=data, timeout=600)

            # Parse response
            result = response.json()
            return result

        except Exception as e:
            print(f"Request failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def update_history_buffer(
        self,
        heatmap_images: List[Image.Image],
        rgb_images: List[Image.Image]
    ):
        """
        æ›´æ–°å†å²å¸§ç¼“å†²åŒºï¼ˆä¸å‘é€predictè¯·æ±‚ï¼‰

        åœ¨æ‰§è¡ŒåŠ¨ä½œchunkæ—¶ï¼Œæ¯æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œååº”è°ƒç”¨æ­¤æ–¹æ³•æ¥æ›´æ–°ç¼“å†²åŒºï¼Œ
        ä»¥ä¿æŒå†å²å¸§çš„æ—¶åºè¿ç»­æ€§ã€‚

        Args:
            heatmap_images: List of PIL Images for heatmap (multi-view) - å½“å‰å¸§
            rgb_images: List of PIL Images for RGB (multi-view) - å½“å‰å¸§
        """
        # æ·»åŠ å½“å‰å¸§åˆ°ç¼“å†²åŒº
        self.history_buffer.append((heatmap_images, rgb_images))

        # ä¿æŒç¼“å†²åŒºå¤§å°ä¸º num_history_frames
        if len(self.history_buffer) > self.num_history_frames:
            self.history_buffer.pop(0)

        print(f"  [Client] History buffer updated: {len(self.history_buffer)}/{self.num_history_frames} frames")

    def clear_history_buffer(self):
        """
        æ¸…ç©ºå†å²å¸§ç¼“å†²åŒº

        åœ¨æ–°çš„episodeå¼€å§‹æ—¶è°ƒç”¨æ­¤æ–¹æ³•é‡ç½®ç¼“å†²åŒº
        """
        self.history_buffer = []
        print(f"  [Client] History buffer cleared")

    def preprocess(self, pcd_list, feat_list, all_poses: np.ndarray):
        """
        é¢„å¤„ç†ç‚¹äº‘åºåˆ—ã€ç‰¹å¾åºåˆ—å’Œå§¿æ€ï¼ˆä¸‰ç›¸æœºç‰ˆæœ¬ï¼Œä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰

        Args:
            pcd_list: ç‚¹äº‘åˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [pcd_cam1, pcd_cam2, pcd_cam3]
            feat_list: ç‰¹å¾åˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [feat_cam1, feat_cam2, feat_cam3]
            all_poses: å§¿æ€æ•°ç»„ [num_poses, 7] - (x,y,z,w,x,y,z) wxyzæ ¼å¼

        Returns:
            å½“ USE_DIFFERENT_PROJECTION=True æ—¶:
                pc_list: å¤„ç†åçš„ç‚¹äº‘åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [pcd_cam1, pcd_cam2, pcd_cam3]
                img_feat_list: å¤„ç†åçš„ç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [feat_cam1, feat_cam2, feat_cam3]
            å½“ USE_DIFFERENT_PROJECTION=False æ—¶:
                pc_list: å¤„ç†åçš„ç‚¹äº‘åˆ—è¡¨ï¼ˆæ‹¼æ¥åçš„å•ä¸ªç‚¹äº‘ï¼‰
                img_feat_list: å¤„ç†åçš„ç‰¹å¾åˆ—è¡¨ï¼ˆæ‹¼æ¥åçš„ï¼‰
            wpt_local: å±€éƒ¨åæ ‡ç³»ä¸‹çš„å§¿æ€ [num_poses, 3]
            rot_xyzw: æ—‹è½¬å››å…ƒæ•° [num_poses, 4] - xyzwæ ¼å¼
            rev_trans: é€†å˜æ¢å‡½æ•°
        """
        import bridgevla.mvt.utils as mvt_utils
        from bridgevla.mvt.augmentation import apply_se3_aug_con_shared

        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
        if not isinstance(pcd_list, list):
            pcd_list = [pcd_list]
        if not isinstance(feat_list, list):
            feat_list = [feat_list]

        num_frames = len(pcd_list)

        # å¤„ç†æ¯ä¸€å¸§çš„3ä¸ªç›¸æœºæ•°æ®
        merged_pcd_list = []
        merged_feat_list = []

        for frame_idx in range(num_frames):
            # è·å–è¿™ä¸€å¸§çš„3ä¸ªç›¸æœºçš„ç‚¹äº‘å’Œç‰¹å¾
            frame_pcds = pcd_list[frame_idx]  # [pcd_cam1, pcd_cam2, pcd_cam3]
            frame_feats = feat_list[frame_idx]  # [feat_cam1, feat_cam2, feat_cam3]

            # å½’ä¸€åŒ–RGBç‰¹å¾
            frame_feats_norm = [_norm_rgb(feat) for feat in frame_feats]

            # å¯¹3ä¸ªç›¸æœºçš„ç‚¹äº‘åˆ†åˆ«åº”ç”¨å¤–å‚å˜æ¢
            pcd_cam1_base = convert_pcd_to_base(extrinsic_martix=self.extrinsic_matrix_1, pcd=frame_pcds[0])
            pcd_cam2_base = convert_pcd_to_base(extrinsic_martix=self.extrinsic_matrix_2, pcd=frame_pcds[1])
            pcd_cam3_base = convert_pcd_to_base(extrinsic_martix=self.extrinsic_matrix_3, pcd=frame_pcds[2])

            # è½¬æ¢ä¸ºtorchå¼ é‡å¹¶å±•å¹³
            pcd_cam1_flat = torch.from_numpy(np.ascontiguousarray(pcd_cam1_base)).float().view(-1, 3)
            pcd_cam2_flat = torch.from_numpy(np.ascontiguousarray(pcd_cam2_base)).float().view(-1, 3)
            pcd_cam3_flat = torch.from_numpy(np.ascontiguousarray(pcd_cam3_base)).float().view(-1, 3)

            # å±•å¹³RGBç‰¹å¾
            feat_cam1_flat = ((frame_feats_norm[0].view(-1, 3) + 1) / 2).float()
            feat_cam2_flat = ((frame_feats_norm[1].view(-1, 3) + 1) / 2).float()
            feat_cam3_flat = ((frame_feats_norm[2].view(-1, 3) + 1) / 2).float()

            # æ ¹æ®æŠ•å½±æ¨¡å¼å†³å®šå¦‚ä½•ç»„ç»‡ç‚¹äº‘å’Œç‰¹å¾
            if self.use_different_projection:
                # DIFFERENT PROJECTION æ¨¡å¼ï¼šä¿ç•™3ä¸ªç‹¬ç«‹çš„ç‚¹äº‘åˆ—è¡¨
                # æ¯ä¸ªç›¸æœºçš„ç‚¹äº‘å•ç‹¬æŠ•å½±åˆ°æœ€ä½³è§†è§’
                merged_pcd_list.append([pcd_cam1_flat, pcd_cam2_flat, pcd_cam3_flat])
                merged_feat_list.append([feat_cam1_flat, feat_cam2_flat, feat_cam3_flat])
            else:
                # DEFAULT PROJECTION æ¨¡å¼ï¼šæ‹¼æ¥æˆ–é€‰æ‹©å•ä¸ªç‚¹äº‘
                if self.use_merged_pointcloud:
                    # æ‹¼æ¥3ä¸ªç›¸æœºçš„ç‚¹äº‘å’Œç‰¹å¾
                    merged_pcd = torch.cat([pcd_cam1_flat, pcd_cam2_flat, pcd_cam3_flat], dim=0)
                    merged_feat = torch.cat([feat_cam1_flat, feat_cam2_flat, feat_cam3_flat], dim=0)
                else:
                    # åªä½¿ç”¨ç›¸æœº3ï¼ˆ3rd_3ï¼‰çš„ç‚¹äº‘å’Œç‰¹å¾
                    merged_pcd = pcd_cam3_flat
                    merged_feat = feat_cam3_flat
                merged_pcd_list.append(merged_pcd)
                merged_feat_list.append(merged_feat)

        # åç»­å¤„ç†
        pc_list = merged_pcd_list
        img_feat_list = merged_feat_list

        with torch.no_grad():
            # æ•°æ®å¢å¼ºï¼ˆæ¨ç†æ—¶é€šå¸¸å…³é—­ï¼‰
            if self.augmentation and self.mode == "train":
                assert False
            else:
                # æ²¡æœ‰æ•°æ®å¢å¼ºæ—¶ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹poses
                action_trans_con = torch.from_numpy(np.array(all_poses)).float()[:, :3]
                # å°†wxyzæ ¼å¼è½¬æ¢ä¸ºxyzwæ ¼å¼
                quat_wxyz = torch.from_numpy(np.array(all_poses)).float()[:, 3:]
                action_rot_xyzw = quat_wxyz[:, [1, 2, 3, 0]]  # [w,x,y,z] -> [x,y,z,w]

            # å¯¹æ¯ä¸ªç‚¹äº‘åº”ç”¨è¾¹ç•Œçº¦æŸ
            processed_pc_list = []
            processed_feat_list = []

            if self.use_different_projection:
                # DIFFERENT PROJECTION æ¨¡å¼ï¼šå¯¹æ¯ä¸ªå¸§çš„3ä¸ªç›¸æœºç‚¹äº‘åˆ†åˆ«å¤„ç†
                for frame_pcs, frame_feats in zip(pc_list, img_feat_list):
                    processed_frame_pcs = []
                    processed_frame_feats = []
                    for pc, img_feat in zip(frame_pcs, frame_feats):
                        pc, img_feat = self.move_pc_in_bound(
                            pc.unsqueeze(0), img_feat.unsqueeze(0), self.scene_bounds
                        )
                        processed_frame_pcs.append(pc[0])
                        processed_frame_feats.append(img_feat[0])
                    processed_pc_list.append(processed_frame_pcs)
                    processed_feat_list.append(processed_frame_feats)
            else:
                # DEFAULT PROJECTION æ¨¡å¼ï¼šå¯¹æ‹¼æ¥åçš„ç‚¹äº‘å¤„ç†
                for pc, img_feat in zip(pc_list, img_feat_list):
                    pc, img_feat = self.move_pc_in_bound(
                        pc.unsqueeze(0), img_feat.unsqueeze(0), self.scene_bounds
                    )
                    processed_pc_list.append(pc[0])
                    processed_feat_list.append(img_feat[0])

            # å°†ç‚¹äº‘å’Œwptæ”¾åœ¨ä¸€ä¸ªcubeé‡Œé¢
            if self.use_different_projection:
                # DIFFERENT PROJECTION æ¨¡å¼ï¼šä½¿ç”¨ç¬¬ä¸€å¸§æ‹¼æ¥åçš„ç‚¹äº‘ä½œä¸ºå‚è€ƒ
                first_frame_merged = torch.cat(processed_pc_list[0], dim=0)
                wpt_local, rev_trans = mvt_utils.place_pc_in_cube(
                    first_frame_merged,
                    action_trans_con,
                    with_mean_or_bounds=False,
                    scene_bounds=self.scene_bounds,
                )
            else:
                # DEFAULT PROJECTION æ¨¡å¼
                wpt_local, rev_trans = mvt_utils.place_pc_in_cube(
                    processed_pc_list[0],
                    action_trans_con,
                    with_mean_or_bounds=False,
                    scene_bounds=self.scene_bounds,
                )

            # å¯¹æ¯ä¸ªç‚¹äº‘åº”ç”¨place_pc_in_cube
            final_pc_list = []

            if self.use_different_projection:
                # DIFFERENT PROJECTION æ¨¡å¼ï¼šå¯¹æ¯ä¸ªå¸§çš„3ä¸ªç›¸æœºç‚¹äº‘åˆ†åˆ«å¤„ç†
                for frame_pcs in processed_pc_list:
                    final_frame_pcs = []
                    for pc in frame_pcs:
                        pc_normalized = mvt_utils.place_pc_in_cube(
                            pc,
                            with_mean_or_bounds=False,
                            scene_bounds=self.scene_bounds,
                        )[0]
                        final_frame_pcs.append(pc_normalized)
                    final_pc_list.append(final_frame_pcs)
            else:
                # DEFAULT PROJECTION æ¨¡å¼
                for pc in processed_pc_list:
                    pc = mvt_utils.place_pc_in_cube(
                        pc,
                        with_mean_or_bounds=False,
                        scene_bounds=self.scene_bounds,
                    )[0]
                    final_pc_list.append(pc)

        return final_pc_list, processed_feat_list, wpt_local, action_rot_xyzw, rev_trans

    def get_rgb_input(self, processed_pcd, processed_rgb):
        """
        ä»å¤„ç†åçš„ç‚¹äº‘å’ŒRGBç”ŸæˆæŠ•å½±çš„RGBå›¾åƒ

        Args:
            processed_pcd: å¤„ç†åçš„ç‚¹äº‘ (torch.Tensor)
            processed_rgb: å¤„ç†åçš„RGBç‰¹å¾ (torch.Tensor)

        Returns:
            rgb_images: PIL.Imageåˆ—è¡¨ï¼Œæ¯ä¸ªè§†è§’ä¸€å¼ å›¾åƒ
        """
        # ä½¿ç”¨æŠ•å½±æ¥å£ç”ŸæˆRGBå›¾åƒ
        rgb_image = self.projection_interface.project_pointcloud_to_rgb(
            processed_pcd, processed_rgb,
            img_aug_before=0.0,  # æ¨ç†æ—¶ä¸åšå¢å¼º
            img_aug_after=0.0
        )  # (1, num_views, H, W, 6)

        rgb_image = rgb_image[0, :, :, :, 3:]  # (num_views, H, W, 3)

        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if isinstance(rgb_image, torch.Tensor):
            rgb_image = rgb_image.cpu().numpy()

        # è½¬æ¢æ¯ä¸ªè§†è§’ä¸ºPIL Image
        num_views = rgb_image.shape[0]
        rgb_images = []
        for view_idx in range(num_views):
            view_img = rgb_image[view_idx]  # (H, W, 3)

            # ä»[-1, 1]å½’ä¸€åŒ–åˆ°[0, 1]ï¼Œç„¶ååˆ°[0, 255]  # è¿™é‡Œæœ‰é—®é¢˜ï¼Œå› ä¸ºæ­¤æ—¶çš„view image åœ¨0ï¼Œ1ä¹‹é—´ï¼Œè€Œä¸æ˜¯-1ï¼Œ1
            # view_img = np.clip((view_img + 1) / 2, 0, 1)
            view_img = (view_img * 255).astype(np.uint8)

            # è½¬æ¢ä¸ºPIL Image
            pil_img = Image.fromarray(view_img)
            rgb_images.append(pil_img)

        return rgb_images

    def get_heatmap_input(self, processed_pos):
        """
        ä»å¤„ç†åçš„ä½ç½®ç”Ÿæˆçƒ­åŠ›å›¾

        Args:
            processed_pos: å¤„ç†åçš„ä½ç½® [num_poses, 3] (torch.Tensor)

        Returns:
            heatmap_images: PIL.Imageåˆ—è¡¨ï¼Œæ¯ä¸ªè§†è§’ä¸€å¼ çƒ­åŠ›å›¾
        """
        # å°†ä½ç½®æŠ•å½±åˆ°åƒç´ åæ ‡
        img_locations = self.projection_interface.project_pose_to_pixel(
            processed_pos.unsqueeze(0).to(self.projection_interface.renderer_device)
        )  # (1, num_poses, num_views, 2)

        # ç”Ÿæˆçƒ­åŠ›å›¾
        heatmap_sequence = self.projection_interface.generate_heatmap_from_img_locations(
            img_locations,
            self.img_size[0], self.img_size[1],
            self.sigma
        )  # (1, num_poses, num_views, H, W)

        # å–ç¬¬ä¸€ä¸ªbatchå’Œç¬¬ä¸€ä¸ªposeçš„çƒ­åŠ›å›¾
        heatmap = heatmap_sequence[0, 0, :, :, :]  # (num_views, H, W)

        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if isinstance(heatmap, torch.Tensor):
            heatmap = heatmap.cpu().numpy()

        # è½¬æ¢æ¯ä¸ªè§†è§’ä¸ºPIL Image
        num_views = heatmap.shape[0]
        heatmap_images = []
        for view_idx in range(num_views):
            view_hm = heatmap[view_idx]  # (H, W)

            # å½’ä¸€åŒ–åˆ°[0, 1]
            view_hm_min = view_hm.min()
            view_hm_max = view_hm.max()
            if view_hm_max > view_hm_min:
                view_hm_norm = (view_hm - view_hm_min) / (view_hm_max - view_hm_min)
            else:
                view_hm_norm = view_hm

            # åº”ç”¨colormapï¼ˆä½¿ç”¨JET colormapä¸æ·±åº¦å›¾ç±»ä¼¼ï¼‰
            view_hm_uint8 = (view_hm_norm * 255).astype(np.uint8)
            view_hm_colored = cv2.applyColorMap(view_hm_uint8, cv2.COLORMAP_JET)
            view_hm_colored = cv2.cvtColor(view_hm_colored, cv2.COLOR_BGR2RGB)

            # è½¬æ¢ä¸ºPIL Image
            pil_img = Image.fromarray(view_hm_colored)
            heatmap_images.append(pil_img)

        return heatmap_images
        
        
    @staticmethod   
    def move_pc_in_bound(pc, img_feat, bounds, no_op=False):
        """
        :param no_op: no operation
        """
        if no_op:
            return pc, img_feat

        x_min, y_min, z_min, x_max, y_max, z_max = bounds
        inv_pnt = (
            (pc[:, :, 0] < x_min)
            | (pc[:, :, 0] > x_max)
            | (pc[:, :, 1] < y_min)
            | (pc[:, :, 1] > y_max)
            | (pc[:, :, 2] < z_min)
            | (pc[:, :, 2] > z_max)
            | torch.isnan(pc[:, :, 0])
            | torch.isnan(pc[:, :, 1])
            | torch.isnan(pc[:, :, 2])
        )

        # TODO: move from a list to a better batched version
        pc = [pc[i, ~_inv_pnt] for i, _inv_pnt in enumerate(inv_pnt)]
        img_feat = [img_feat[i, ~_inv_pnt] for i, _inv_pnt in enumerate(inv_pnt)]
        return pc, img_feat


    @staticmethod    
    def convert_pcd_to_base(
            type="3rd",
            pcd=[]
        ):
        transform = get_cam_extrinsic(type)
        
        h, w = pcd.shape[:2]
        pcd = pcd.reshape(-1, 3)
        
        pcd = np.concatenate((pcd, np.ones((pcd.shape[0], 1))), axis=1)
        # pcd = (np.linalg.inv(transform) @ pcd.T).T[:, :3]
        pcd = (transform @ pcd.T).T[:, :3]
        
        pcd = pcd.reshape(h, w, 3)
        return pcd 


    @staticmethod
    def vis_pcd(pcd, rgb):

        # å°†ç‚¹äº‘å’Œé¢œè‰²è½¬æ¢ä¸ºäºŒç»´çš„å½¢çŠ¶ (N, 3)
        pcd_flat = pcd.reshape(-1, 3)  # (200 * 200, 3)
        rgb_flat = rgb.reshape(-1, 3) / 255.0  # (200 * 200, 3)

        # å°†ç‚¹äº‘å’Œé¢œè‰²ä¿¡æ¯ä¿å­˜ä¸º PLY æ–‡ä»¶
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(pcd_flat)  # è®¾ç½®ç‚¹äº‘ä½ç½®
        pcd.colors = o3d.utility.Vector3dVector(rgb_flat)  # è®¾ç½®å¯¹åº”çš„é¢œè‰²
        # o3d.io.write_point_cloud(save_path, pcd)
        o3d.visualization.draw_geometries([pcd])

    @staticmethod
    def visualize_pointcloud(processed_pcd, processed_rgb=None, title="Processed Point Cloud"):
        """
        å¯è§†åŒ–å¤„ç†åçš„ç‚¹äº‘ (line 635)

        Args:
            processed_pcd: torch.Tensor or np.ndarray, shape (num_points, 3)
            processed_rgb: torch.Tensor or np.ndarray, shape (num_points, 3), å¯é€‰çš„RGBé¢œè‰²
            title: å¯è§†åŒ–çª—å£æ ‡é¢˜
        """
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(processed_pcd, torch.Tensor):
            pcd_np = processed_pcd.cpu().numpy()
        else:
            pcd_np = processed_pcd

        # åˆ›å»ºOpen3Dç‚¹äº‘å¯¹è±¡
        pcd_o3d = o3d.geometry.PointCloud()
        pcd_o3d.points = o3d.utility.Vector3dVector(pcd_np)

        # å¦‚æœæä¾›äº†RGBé¢œè‰²
        if processed_rgb is not None:
            if isinstance(processed_rgb, torch.Tensor):
                rgb_np = processed_rgb.cpu().numpy()
            else:
                rgb_np = processed_rgb

            # ç¡®ä¿é¢œè‰²åœ¨[0, 1]èŒƒå›´å†…
            if rgb_np.max() > 1.0:
                rgb_np = rgb_np / 255.0

            pcd_o3d.colors = o3d.utility.Vector3dVector(rgb_np)

        # å¯è§†åŒ–
        print(f"\n{'='*60}")
        print(f"å¯è§†åŒ–: {title}")
        print(f"ç‚¹äº‘æ•°é‡: {len(pcd_o3d.points)}")
        if processed_rgb is not None:
            print(f"åŒ…å«RGBé¢œè‰²ä¿¡æ¯")
        print(f"{'='*60}\n")

        o3d.visualization.draw_geometries(
            [pcd_o3d],
            window_name=title,
            width=800,
            height=600
        )

    @staticmethod
    def visualize_rgb_images(rgb_images, title="Multi-view RGB Images", save_path=None):
        """
        å¯è§†åŒ–å¤šè§†è§’RGBå›¾åƒ (line 639)

        Args:
            rgb_images: List[PIL.Image], å¤šè§†è§’RGBå›¾åƒåˆ—è¡¨
            title: æ˜¾ç¤ºçª—å£æ ‡é¢˜
            save_path: å¯é€‰çš„ä¿å­˜è·¯å¾„
        """
        num_views = len(rgb_images)

        # åˆ›å»ºå­å›¾æ˜¾ç¤ºæ‰€æœ‰è§†è§’
        import matplotlib.pyplot as plt

        # è®¡ç®—å­å›¾å¸ƒå±€ (å°½é‡æ¥è¿‘æ­£æ–¹å½¢)
        cols = int(np.ceil(np.sqrt(num_views)))
        rows = int(np.ceil(num_views / cols))

        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))
        fig.suptitle(title, fontsize=16)

        # å±•å¹³axesä»¥ä¾¿ç´¢å¼•
        if num_views == 1:
            axes = [axes]
        else:
            axes = axes.flatten() if rows * cols > 1 else [axes]

        for i, (img, ax) in enumerate(zip(rgb_images, axes)):
            ax.imshow(img)
            ax.set_title(f"View {i+1}")
            ax.axis('off')

        # éšè—å¤šä½™çš„å­å›¾
        for i in range(num_views, len(axes)):
            axes[i].axis('off')

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"RGBå›¾åƒå·²ä¿å­˜åˆ°: {save_path}")

        print(f"\n{'='*60}")
        print(f"å¯è§†åŒ–: {title}")
        print(f"è§†è§’æ•°é‡: {num_views}")
        print(f"å›¾åƒå°ºå¯¸: {rgb_images[0].size}")
        print(f"{'='*60}\n")

        plt.show()

    @staticmethod
    def visualize_heatmap_images(heatmap_images, title="Multi-view Heatmaps", save_path=None):
        """
        å¯è§†åŒ–å¤šè§†è§’çƒ­åŠ›å›¾ (line 642)

        Args:
            heatmap_images: List[PIL.Image], å¤šè§†è§’çƒ­åŠ›å›¾åˆ—è¡¨
            title: æ˜¾ç¤ºçª—å£æ ‡é¢˜
            save_path: å¯é€‰çš„ä¿å­˜è·¯å¾„
        """
        num_views = len(heatmap_images)

        # åˆ›å»ºå­å›¾æ˜¾ç¤ºæ‰€æœ‰è§†è§’
        import matplotlib.pyplot as plt

        # è®¡ç®—å­å›¾å¸ƒå±€
        cols = int(np.ceil(np.sqrt(num_views)))
        rows = int(np.ceil(num_views / cols))

        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))
        fig.suptitle(title, fontsize=16)

        # å±•å¹³axesä»¥ä¾¿ç´¢å¼•
        if num_views == 1:
            axes = [axes]
        else:
            axes = axes.flatten() if rows * cols > 1 else [axes]

        for i, (img, ax) in enumerate(zip(heatmap_images, axes)):
            ax.imshow(img)
            ax.set_title(f"Heatmap View {i+1}")
            ax.axis('off')

        # éšè—å¤šä½™çš„å­å›¾
        for i in range(num_views, len(axes)):
            axes[i].axis('off')

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {save_path}")

        print(f"\n{'='*60}")
        print(f"å¯è§†åŒ–: {title}")
        print(f"è§†è§’æ•°é‡: {num_views}")
        print(f"å›¾åƒå°ºå¯¸: {heatmap_images[0].size}")
        print(f"{'='*60}\n")

        plt.show()


def normalize_angle(angle: float) -> float:
    """
    å°†è§’åº¦æ ‡å‡†åŒ–åˆ° [-Ï€, Ï€] èŒƒå›´

    Args:
        angle: è¾“å…¥è§’åº¦ï¼ˆå¼§åº¦ï¼‰

    Returns:
        æ ‡å‡†åŒ–åçš„è§’åº¦ï¼ˆå¼§åº¦ï¼‰
    """
    # ä½¿ç”¨ atan2(sin(angle), cos(angle)) æ¥æ ‡å‡†åŒ–è§’åº¦åˆ° [-Ï€, Ï€]
    return np.arctan2(np.sin(angle), np.cos(angle))


def reset_joints_pour(robot):
    """
    å°†æœºæ¢°è‡‚é‡ç½®åˆ° pour ä»»åŠ¡çš„åˆå§‹å…³èŠ‚çŠ¶æ€

    Args:
        robot: RobotController å®ä¾‹
    """
    rospy.loginfo("[Pour Reset] å°†æœºæ¢°è‡‚ç§»åŠ¨åˆ° pour åˆå§‹çŠ¶æ€...")
    rospy.loginfo(f"[Pour Reset] ç›®æ ‡å…³èŠ‚ä½ç½®: {RESET_JOINTS_POUR}")
    try:
        # å…ˆåœæ­¢å½“å‰çš„ skill
        try:
            robot.franka.stop_skill()
            rospy.sleep(0.5)
        except Exception as e:
            rospy.logwarn(f"Could not stop previous skill: {e}")

        # å°†æœºæ¢°è‡‚ç§»åŠ¨åˆ°ç›®æ ‡å…³èŠ‚ä½ç½®
        robot.franka.goto_joints(
            RESET_JOINTS_POUR.tolist(),
            duration=5.0,
            ignore_virtual_walls=True
        )
        rospy.loginfo("[Pour Reset] æœºæ¢°è‡‚å·²ç§»åŠ¨åˆ° pour åˆå§‹çŠ¶æ€")

        # ç­‰å¾…ç§»åŠ¨å®Œæˆ
        rospy.sleep(1.0)

        # éªŒè¯å½“å‰ä½ç½®
        current_joints = robot.franka.get_joints()
        rospy.loginfo(f"[Pour Reset] å½“å‰å…³èŠ‚ä½ç½®: {current_joints}")

    except Exception as e:
        rospy.logerr(f"reset_joints_pour å¤±è´¥: {e}")
        raise


def clip_action(
    target_action: np.ndarray,
    current_position: np.ndarray,
    current_rotation: np.ndarray,
    max_position_change: float = 0.04,
    max_rotation_change: float = 10.0
) -> np.ndarray:
    """
    æˆªæ–­åŠ¨ä½œï¼Œé™åˆ¶ä½ç½®å’Œæ—‹è½¬çš„æœ€å¤§å˜åŒ–é‡

    Args:
        target_action: ç›®æ ‡åŠ¨ä½œ [x, y, z, roll, pitch, yaw, gripper] (rotation in radians)
        current_position: å½“å‰ä½ç½® [x, y, z] (meters)
        current_rotation: å½“å‰æ—‹è½¬ [roll, pitch, yaw] (radians)
        max_position_change: ä½ç½®æœ€å¤§å˜åŒ–é‡ï¼ˆç±³ï¼‰
        max_rotation_change: æ—‹è½¬æœ€å¤§å˜åŒ–é‡ï¼ˆåº¦ï¼‰

    Returns:
        clipped_action: æˆªæ–­åçš„åŠ¨ä½œ [x, y, z, roll, pitch, yaw, gripper]
    """
    clipped_action = target_action.copy()

    # è®¡ç®—ä½ç½®å˜åŒ–
    target_position = target_action[:3]
    position_delta = target_position - current_position
    position_distance = np.linalg.norm(position_delta)

    # å¦‚æœä½ç½®å˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œæˆªæ–­
    if position_distance > max_position_change:
        # æ²¿ç€åŸæ–¹å‘ç¼©æ”¾åˆ°æœ€å¤§å…è®¸è·ç¦»
        scale_factor = max_position_change / position_distance
        clipped_position = current_position + position_delta * scale_factor
        clipped_action[:3] = clipped_position
        rospy.logwarn(f"  âš ï¸  Position clipped: {position_distance:.4f}m -> {max_position_change:.4f}m")

    # è®¡ç®—æ—‹è½¬å˜åŒ–ï¼ˆä½¿ç”¨è§’åº¦å½’ä¸€åŒ–é¿å…ä¸´ç•Œç‚¹é—®é¢˜ï¼‰
    target_rotation = target_action[3:6]

    # å¯¹æ¯ä¸ªæ—‹è½¬è½´åˆ†åˆ«è®¡ç®—å½’ä¸€åŒ–çš„è§’åº¦å·®
    rotation_delta_rad = np.array([
        normalize_angle(target_rotation[i] - current_rotation[i])
        for i in range(3)
    ])
    rotation_delta_deg = np.rad2deg(rotation_delta_rad)

    # å¯¹æ¯ä¸ªæ—‹è½¬è½´åˆ†åˆ«æ£€æŸ¥å’Œæˆªæ–­
    clipped_rotation = target_rotation.copy()
    rotation_clipped = False

    for i, axis_name in enumerate(['roll', 'pitch', 'yaw']):
        if abs(rotation_delta_deg[i]) > max_rotation_change:
            # æˆªæ–­åˆ°æœ€å¤§å…è®¸å˜åŒ–
            sign = np.sign(rotation_delta_deg[i])
            max_change_rad = np.deg2rad(max_rotation_change * sign)
            clipped_rotation[i] = current_rotation[i] + max_change_rad
            rotation_clipped = True
            rospy.logwarn(f"  âš ï¸  {axis_name.capitalize()} clipped: {rotation_delta_deg[i]:.2f}Â° -> {max_rotation_change * sign:.2f}Â°")

    if rotation_clipped:
        clipped_action[3:6] = clipped_rotation

    return clipped_action


def real_robot_control_loop(
    server_url: str = "http://localhost:5555",
    task_prompt: str = "put the lion on the top shelf",
    max_steps: int = 1000000,
    num_frames: int = 13,
    save_dir: Optional[str] = None,
    action_duration: float = 0.5,
    gripper_threshold: float = 0.5,
    scene_bounds: List[float] = None,
    img_size: int = None,
    enable_action_clipping: bool = None,
    max_position_change: float = None,
    max_rotation_change: float = None,
    use_merged_pointcloud: bool = None,
):
    """
    çœŸæœºæ§åˆ¶ä¸»å¾ªç¯

    Args:
        server_url: RoboWanæœåŠ¡å™¨åœ°å€
        task_prompt: ä»»åŠ¡æŒ‡ä»¤
        max_steps: æœ€å¤§æ§åˆ¶æ­¥æ•°
        num_frames: æ¯æ¬¡é¢„æµ‹çš„å¸§æ•°
        save_dir: æ•°æ®ä¿å­˜ç›®å½•ï¼ˆNoneåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
        action_duration: æ¯ä¸ªåŠ¨ä½œæ‰§è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰
        gripper_threshold: å¤¹çˆªåŠ¨ä½œé˜ˆå€¼
        scene_bounds: åœºæ™¯è¾¹ç•Œ [x_min, y_min, z_min, x_max, y_max, z_max]
        img_size: å›¾åƒå°ºå¯¸
        enable_action_clipping: æ˜¯å¦å¯ç”¨åŠ¨ä½œæˆªæ–­ï¼ˆNoneåˆ™ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        max_position_change: ä½ç½®æœ€å¤§å˜åŒ–é‡ï¼ˆç±³ï¼‰ï¼ˆNoneåˆ™ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        max_rotation_change: æ—‹è½¬æœ€å¤§å˜åŒ–é‡ï¼ˆåº¦ï¼‰ï¼ˆNoneåˆ™ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        use_merged_pointcloud: æ˜¯å¦ä½¿ç”¨æ‹¼æ¥åçš„ç‚¹äº‘ï¼ˆTrueï¼‰æˆ–åªä½¿ç”¨ç›¸æœº1çš„ç‚¹äº‘ï¼ˆFalseï¼‰
    """
    # åˆå§‹åŒ–ROSèŠ‚ç‚¹ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
    # æ³¨æ„ï¼šä½¿ç”¨ disable_signals=True ä»¥åŒ¹é… FrankaArm çš„åˆå§‹åŒ–å‚æ•°
    if not rospy.core.is_initialized():
        rospy.init_node('robowan_real_robot', anonymous=False, disable_signals=True)

    # ä½¿ç”¨å…¨å±€é…ç½®ä½œä¸ºé»˜è®¤å€¼
    if enable_action_clipping is None:
        enable_action_clipping = ENABLE_ACTION_CLIPPING
    if max_position_change is None:
        max_position_change = MAX_POSITION_CHANGE
    if max_rotation_change is None:
        max_rotation_change = MAX_ROTATION_CHANGE
    if use_merged_pointcloud is None:
        use_merged_pointcloud = USE_MERGED_POINTCLOUD

    # æ£€æŸ¥æœºå™¨äººæ¥å£æ˜¯å¦å¯ç”¨
    if not ROBOT_AVAILABLE:
        rospy.logerr("Robot interface not available! Please check imports.")
        return

    rospy.loginfo("="*60)
    rospy.loginfo("Starting Real Robot Control Loop")
    rospy.loginfo("="*60)
    if enable_action_clipping:
        rospy.loginfo(f"Action Clipping ENABLED:")
        rospy.loginfo(f"  - Max position change: {max_position_change*100:.1f} cm")
        rospy.loginfo(f"  - Max rotation change: {max_rotation_change:.1f}Â°")
    else:
        rospy.loginfo("Action Clipping DISABLED")

    # åˆ›å»ºä¿å­˜ç›®å½•
    if save_dir is None:
        save_root = Path("/media/casia/data4/lpy/RoboWan/logs")
        time_tag = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = save_root / f"run_{time_tag}"
    else:
        save_dir = Path(save_dir)

    save_dir.mkdir(parents=True, exist_ok=True)
    (save_dir / "actions").mkdir(exist_ok=True)
    rospy.loginfo(f"Saving data to: {save_dir}")

    # åˆå§‹åŒ– RoboWan å®¢æˆ·ç«¯
    rospy.loginfo(f"Connecting to RoboWan server at {server_url}")
    if scene_bounds is None:
        scene_bounds = SCENE_BOUNDS
    if img_size is None:
        img_size = IMG_SIZE
    rospy.loginfo(f"Using scene bounds: {scene_bounds}")
    rospy.loginfo(f"Using image size: {img_size}")
    rospy.loginfo(f"Using merged pointcloud: {use_merged_pointcloud}")
    client = RoboWanClient(server_url=server_url, scene_bounds=scene_bounds, img_size=img_size, use_merged_pointcloud=use_merged_pointcloud)

    # æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
    if not client.check_health():
        rospy.logerr("Server is not healthy! Please start the server first.")
        return
    rospy.loginfo("âœ“ Server is healthy")

    # éªŒè¯å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é…ç½®ä¸€è‡´æ€§
    if not client.verify_config_consistency():
        rospy.logerr("âŒ å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é…ç½®ä¸ä¸€è‡´ï¼è¯·ä¿®æ”¹é…ç½®åé‡æ–°è¿è¡Œã€‚")
        rospy.logerr("   å…³é”®é…ç½®é¡¹ï¼šUSE_DIFFERENT_PROJECTION, SCENE_BOUNDS, IMG_SIZE")
        return
    rospy.loginfo("âœ“ Configuration verified")

    # åˆå§‹åŒ–æœºå™¨äººæ§åˆ¶å™¨
    rospy.loginfo("Initializing robot controller...")
    robot = RobotController(frequency=10)
    rospy.loginfo("âœ“ Robot controller initialized")

    # æ³¨å†Œé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°ï¼Œç¡®ä¿ç¨‹åºæ„å¤–é€€å‡ºæ—¶ä¹Ÿèƒ½æ¸…ç†
    atexit.register(lambda: robot.cleanup())
    rospy.loginfo("âœ“ Exit cleanup handler registered")

    # Pour ä»»åŠ¡ï¼šå…ˆé‡ç½®åˆ°åˆå§‹å…³èŠ‚ä½ç½®
    rospy.loginfo("="*60)
    rospy.loginfo("[Pour] Resetting robot to pour initial position...")
    reset_joints_pour(robot)
    rospy.loginfo("âœ“ Robot reset to pour initial position")
    rospy.loginfo("="*60)

    # è®°å½•åˆå§‹ä½å§¿
    initial_pose = robot.get_pose()
    rospy.loginfo(f"[Pour] åˆå§‹ä½ç½®: {initial_pose.translation}")
    rospy.loginfo(f"[Pour] åˆå§‹æ—‹è½¬ (rad): {initial_pose.euler_angles}")
    rospy.loginfo(f"[Pour] åˆå§‹æ—‹è½¬ (deg): {np.rad2deg(initial_pose.euler_angles)}")

    # å¯åŠ¨åŠ¨æ€æ§åˆ¶æ¨¡å¼ï¼ˆæŒç»­è·Ÿè¸ªå‘å¸ƒçš„ç›®æ ‡ä½å§¿ï¼‰
    rospy.loginfo("Starting dynamic control mode...")

    # å…ˆåœæ­¢ä»»ä½•æ­£åœ¨è¿è¡Œçš„skill
    try:
        robot.franka.stop_skill()
        rospy.sleep(0.5)
        rospy.loginfo("âœ“ Previous skill stopped")
    except Exception as e:
        rospy.logwarn(f"Could not stop previous skill: {e}")

    # è·å–å½“å‰ä½å§¿å¹¶å¯åŠ¨åŠ¨æ€æ§åˆ¶
    current_pose = robot.get_pose()
    robot.franka.goto_pose(
        current_pose,
        duration=100000,  # å¾ˆé•¿çš„æ—¶é—´ï¼Œä¿æŒåŠ¨æ€æ¨¡å¼è¿è¡Œ
        dynamic=True,
        buffer_time=10,
        cartesian_impedances=[600.0, 600.0, 600.0, 50.0, 50.0, 50.0],
    )
    rospy.loginfo("âœ“ Dynamic control mode started")

    # æ£€æŸ¥ç›¸æœºæ¥å£æ˜¯å¦å¯ç”¨
    if not CAMERA_AVAILABLE:
        rospy.logerr("Camera interface not available! Please check imports.")
        return

    # åˆå§‹åŒ–ç›¸æœºæ§åˆ¶å™¨ï¼ˆä½¿ç”¨ä¸‰ä¸ªç¬¬ä¸‰è§†è§’ç›¸æœºï¼‰
    rospy.loginfo("Initializing three third-person cameras...")
    camera = Camera(camera_type="all")  # åˆå§‹åŒ–ä¸‰ä¸ªç›¸æœº
    rospy.loginfo("âœ“ Three third-person cameras initialized")

    # è·å–åˆå§‹çŠ¶æ€
    current_pose = robot.get_pose()

    # ä¿®å¤æ—‹è½¬å®šä¹‰ä¸åŒ¹é…é—®é¢˜ï¼šç›´æ¥ä»å››å…ƒæ•°è½¬æ¢ä¸ºæ¬§æ‹‰è§’
    # ç¡®ä¿ä¸è®­ç»ƒæ•°æ®ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è½¬æ¢æ–¹æ³•
    from scipy.spatial.transform import Rotation as R
    current_quat_init = current_pose.quaternion  # [w, x, y, z]
    quat_scipy_init = np.array([
        current_quat_init[1],
        current_quat_init[2],
        current_quat_init[3],
        current_quat_init[0]
    ])
    current_rotation_deg_init = R.from_quat(quat_scipy_init).as_euler("xyz", degrees=True)

    # æ³¨æ„ï¼šrobot.gripper_state çš„å®šä¹‰æ˜¯ False=æ‰“å¼€, True=å…³é—­
    # ä½†è®­ç»ƒæ•°æ®çš„å®šä¹‰æ˜¯ 0=å…³é—­, 1=æ‰“å¼€
    # éœ€è¦è½¬æ¢ï¼šgripper_state=False(æ‰“å¼€) -> current_gripper=1
    #         gripper_state=True(å…³é—­) -> current_gripper=0
    current_gripper = 0 if robot.gripper_state else 1  # ä¿®å¤ï¼šåè½¬æ˜ å°„

    rospy.loginfo("\n" + "="*60)
    rospy.loginfo(f"Task: {task_prompt}")
    rospy.loginfo(f"Max steps: {max_steps}")
    rospy.loginfo(f"Initial rotation (degrees): {current_rotation_deg_init}")
    rospy.loginfo(f"Initial gripper: {current_gripper}")
    rospy.loginfo("="*60 + "\n")

    # æ§åˆ¶å¾ªç¯
    step = 0

    try:
        while step < max_steps:
            rospy.loginfo(f"{'='*60}")
            rospy.loginfo(f"Step {step}/{max_steps}")
            rospy.loginfo(f"{'='*60}")

            # ========== 1. è·å–è§‚æµ‹ ==========
            rospy.loginfo("Capturing observations from three cameras...")
            try:
                obs = camera.capture()

                # è·å–ä¸‰ä¸ªç›¸æœºçš„RGBå›¾åƒå’Œç‚¹äº‘
                rgb_3rd_1 = obs['3rd_1']['rgb']
                pcd_3rd_1 = obs['3rd_1']['pcd']
                rgb_3rd_2 = obs['3rd_2']['rgb']
                pcd_3rd_2 = obs['3rd_2']['pcd']
                rgb_3rd_3 = obs['3rd_3']['rgb']
                pcd_3rd_3 = obs['3rd_3']['pcd']

                # å°†BGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼
                rgb_3rd_1 = cv2.cvtColor(rgb_3rd_1, cv2.COLOR_BGR2RGB)
                rgb_3rd_2 = cv2.cvtColor(rgb_3rd_2, cv2.COLOR_BGR2RGB)
                rgb_3rd_3 = cv2.cvtColor(rgb_3rd_3, cv2.COLOR_BGR2RGB)

                # ä¿å­˜è§‚æµ‹
                # save_observations(obs, save_dir, step)
                rospy.loginfo("âœ“ Observations captured from three cameras")

                # è·å–å½“å‰æœºå™¨äººçŠ¶æ€
                current_pose_obj = robot.get_pose()

                # ä¿®å¤æ—‹è½¬å®šä¹‰ä¸åŒ¹é…é—®é¢˜ï¼š
                # ç›´æ¥ä»å››å…ƒæ•°è½¬æ¢ä¸ºæ¬§æ‹‰è§’ï¼Œç¡®ä¿ä¸è®­ç»ƒæ•°æ®ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è½¬æ¢æ–¹æ³•
                # è®­ç»ƒæ•°æ®ä½¿ç”¨: r.as_euler("xyz", degrees=True)  (heatmap_utils.py:592)
                from scipy.spatial.transform import Rotation as R
                current_quat = current_pose_obj.quaternion  # [w, x, y, z]
                # è½¬æ¢ä¸º scipy æ ¼å¼ [x, y, z, w]
                quat_scipy = np.array([
                    current_quat[1],
                    current_quat[2],
                    current_quat[3],
                    current_quat[0]
                ])
                # ä½¿ç”¨ä¸è®­ç»ƒæ•°æ®å®Œå…¨ç›¸åŒçš„è½¬æ¢æ–¹æ³•
                current_rotation_deg = R.from_quat(quat_scipy).as_euler("xyz", degrees=True).tolist()

                # DEBUG: å¯¹æ¯” euler_angles property å’Œå››å…ƒæ•°è½¬æ¢çš„ç»“æœ
                current_rotation_rad_property = current_pose_obj.euler_angles  # [roll, pitch, yaw] in radians
                current_rotation_deg_property = np.rad2deg(current_rotation_rad_property).tolist()
                rospy.loginfo(f"[DEBUG Rotation] euler_angles property: {current_rotation_deg_property}")
                rospy.loginfo(f"[DEBUG Rotation] from quaternion (xyz): {current_rotation_deg}")

                # æ³¨æ„ï¼šrobot.gripper_state çš„å®šä¹‰æ˜¯ False=æ‰“å¼€, True=å…³é—­
                # ä½†è®­ç»ƒæ•°æ®çš„å®šä¹‰æ˜¯ 0=å…³é—­, 1=æ‰“å¼€
                # éœ€è¦è½¬æ¢ï¼šgripper_state=False(æ‰“å¼€) -> current_gripper=1
                #         gripper_state=True(å…³é—­) -> current_gripper=0
                current_gripper = 0 if robot.gripper_state else 1  # ä¿®å¤ï¼šåè½¬æ˜ å°„

                # DEBUG: æ‰“å°å¤¹çˆªçŠ¶æ€
                rospy.loginfo(f"[DEBUG Client] robot.gripper_state = {robot.gripper_state} ({'closed' if robot.gripper_state else 'open'}), current_gripper = {current_gripper} ({'closed' if current_gripper==0 else 'open'})")

                # å°†å½“å‰ä½å§¿è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼ [x, y, z, w, x, y, z] (wxyzæ ¼å¼)
                # å‡è®¾ current_pose_obj æœ‰ translation å’Œ quaternion å±æ€§
                current_position = current_pose_obj.translation  # [x, y, z]
                current_quat = current_pose_obj.quaternion  # å‡è®¾æ˜¯ [w, x, y, z] æ ¼å¼
                current_pose = np.concatenate([current_position, current_quat])  # [x, y, z, w, x, y, z]
                current_pose = current_pose.reshape(1, 7)  # [1, 7]

                rospy.loginfo(f"Current position: {current_position}")
                rospy.loginfo(f"Current rotation: {current_rotation_deg}")
                rospy.loginfo(f"Current gripper: {current_gripper}")

            except Exception as e:
                rospy.logerr(f"Error capturing observations: {e}")
                break

            # ========== 2. å‡†å¤‡è¾“å…¥æ•°æ® ==========

            # é¢„å¤„ç†æ•°æ®ï¼ˆä¸‰ç›¸æœºç‰ˆæœ¬ï¼‰
            # å°†ä¸‰ä¸ªç›¸æœºçš„æ•°æ®ç»„ç»‡æˆåˆ—è¡¨æ ¼å¼ï¼š[[pcd1, pcd2, pcd3]]
            processed_pcd_list, processed_rgb_list, processed_pos, processed_rot_xyzw, rev_trans = client.preprocess(
                [[pcd_3rd_1, pcd_3rd_2, pcd_3rd_3]],
                [[rgb_3rd_1, rgb_3rd_2, rgb_3rd_3]],
                current_pose
            )

            # å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆå› ä¸ºåªæœ‰ä¸€ä¸ªè§‚æµ‹ï¼‰
            processed_pcd = processed_pcd_list[0]  # æ‹¼æ¥åçš„ç‚¹äº‘
            processed_rgb = processed_rgb_list[0]  # æ‹¼æ¥åçš„RGB
            # å¯è§†åŒ–å¤„ç†åçš„ç‚¹äº‘ (å¯é€‰)
            # RoboWanClient.visualize_pointcloud(processed_pcd, processed_rgb, title="Processed Point Cloud (Merged 3 Cameras)")

            # å‡†å¤‡RGBå›¾åƒï¼ˆä»æ‹¼æ¥åçš„ç‚¹äº‘æŠ•å½±ï¼‰
            rgb_images = client.get_rgb_input(processed_pcd, processed_rgb)
            # å¯è§†åŒ–å¤šè§†è§’RGBå›¾åƒ (å¯é€‰)
            # RoboWanClient.visualize_rgb_images(rgb_images, title="Multi-view RGB Images")

            # å‡†å¤‡çƒ­åŠ›å›¾
            heatmap_images = client.get_heatmap_input(processed_pos)
            # å¯è§†åŒ–çƒ­åŠ›å›¾ (å¯é€‰)
            # RoboWanClient.visualize_heatmap_images(heatmap_images, title="Multi-view Heatmaps")


            # ========== 3. å‘é€è¯·æ±‚åˆ°æœåŠ¡å™¨ ==========
            rospy.loginfo("Requesting action from server...")
            try:
                result = client.predict(
                    heatmap_images=heatmap_images,
                    rgb_images=rgb_images,
                    prompt=task_prompt,
                    initial_rotation=current_rotation_deg,
                    initial_gripper=current_gripper,
                    num_frames=num_frames
                )

                if not result['success']:
                    rospy.logerr(f"Server prediction failed: {result.get('error', 'Unknown error')}")
                    break

                rospy.loginfo("âœ“ Received action from server")

            except Exception as e:
                rospy.logerr(f"Error during server request: {e}")
                break

            # ========== 4. è§£æåŠ¨ä½œ ==========
            # æå–ä½ç½®ã€æ—‹è½¬å’Œå¤¹çˆªåŠ¨ä½œ
            position_actions = result.get('position', [])
            rotation_actions = result.get('rotation', [])
            gripper_actions = result.get('gripper', [])

            # è°ƒè¯•ä¿¡æ¯
            rospy.loginfo(f"Received actions:")
            rospy.loginfo(f"  Position actions: {len(position_actions)} frames")
            rospy.loginfo(f"  Rotation actions: {len(rotation_actions)} frames")
            rospy.loginfo(f"  Gripper actions: {len(gripper_actions)} frames")

            # ========== DEBUG: æ£€æŸ¥ç¬¬ä¸€ä¸ªé¢„æµ‹ä½ç½®å’Œå½“å‰ä½ç½®çš„å·®å¼‚ ==========
            # ç†è®ºä¸Š,å¦‚æœç©ºé—´ä½ç½®->çƒ­åŠ›å›¾->ç©ºé—´ä½ç½®çš„å˜æ¢æ˜¯æ­£ç¡®çš„,
            # é‚£ä¹ˆç¬¬ä¸€ä¸ªé¢„æµ‹ä½ç½®åº”è¯¥å’Œå½“å‰ä½ç½®æ¥è¿‘
            if len(position_actions) > 0:
                predicted_first_pos = np.array(position_actions[0])  # [x, y, z]
                position_diff = predicted_first_pos - current_position
                position_distance = np.linalg.norm(position_diff)

                rospy.loginfo(f"\n{'='*60}")
                rospy.loginfo(f"[DEBUG] ä½ç½®å˜æ¢ä¸€è‡´æ€§æ£€æŸ¥:")
                rospy.loginfo(f"  å½“å‰ä½ç½® (current_position):     {current_position}")
                rospy.loginfo(f"  é¢„æµ‹ç¬¬ä¸€å¸§ä½ç½® (predicted[0]):    {predicted_first_pos}")
                rospy.loginfo(f"  ä½ç½®å·®å¼‚å‘é‡:                      {position_diff}")
                rospy.loginfo(f"  ä½ç½®å·®å¼‚è·ç¦» (L2èŒƒæ•°):             {position_distance:.6f} ç±³")

                # åˆ¤æ–­æ˜¯å¦æ¥è¿‘ (é˜ˆå€¼å¯ä»¥è°ƒæ•´,ä¾‹å¦‚1cm = 0.01m)
                threshold = 0.01  # 1cm
                if position_distance < threshold:
                    rospy.loginfo(f"  âœ“ å˜æ¢ä¸€è‡´! (è·ç¦» < {threshold}m)")
                else:
                    rospy.logwarn(f"  âœ— å˜æ¢å¯èƒ½æœ‰è¯¯! (è·ç¦» >= {threshold}m)")
                    rospy.logwarn(f"    è¿™å¯èƒ½æ„å‘³ç€ç©ºé—´ä½ç½®<->çƒ­åŠ›å›¾çš„å˜æ¢å­˜åœ¨é—®é¢˜")
                rospy.loginfo(f"{'='*60}\n")

            # ç¡®ä¿æ‰€æœ‰åŠ¨ä½œæ•°ç»„é•¿åº¦ä¸€è‡´
            num_actions = min(len(position_actions), len(rotation_actions), len(gripper_actions))
            if num_actions == 0:
                rospy.logerr("No actions received from server!")
                break

            rospy.loginfo(f"Using {num_actions} actions")

            # æ„å»ºåŠ¨ä½œçŸ©é˜µ
            action_matrix = np.zeros((num_actions, 7))
            action_matrix[:, :3] = np.array(position_actions[:num_actions])  # position
            action_matrix[:, 3:6] = np.deg2rad(rotation_actions[:num_actions])  # rotation (è½¬ä¸ºå¼§åº¦)
            action_matrix[:, 6] = np.array(gripper_actions[:num_actions])  # gripper

            # ä¿å­˜åŠ¨ä½œ
            # np.save(str(save_dir / "actions" / f"step_{step:05d}.npy"), action_matrix)
            # rospy.loginfo(f"Action matrix shape: {action_matrix.shape}")

            # ========== 5. æ‰§è¡ŒåŠ¨ä½œ ==========
            # é€šå¸¸åªæ‰§è¡Œç¬¬ä¸€ä¸ªæˆ–å‰å‡ ä¸ªåŠ¨ä½œ
            num_actions_to_execute = min(EXECUTION_STEP, len(action_matrix))

            for action_idx in range(num_actions_to_execute):
                single_action = action_matrix[action_idx]
                rospy.loginfo(f"Executing action {action_idx}/{num_actions_to_execute}")
                rospy.loginfo(f"  Original action: {single_action}")

                try:
                    # è·å–å½“å‰ä½å§¿ç”¨äºå¯¹æ¯”
                    current_pose_obj = robot.get_pose()
                    current_pos = current_pose_obj.translation
                    current_rot = current_pose_obj.euler_angles

                    rospy.loginfo(f"  Current position: {current_pos}")
                    rospy.loginfo(f"  Current rotation: {current_rot}")
                    rospy.loginfo(f"  Target position: {single_action[:3]}")
                    rospy.loginfo(f"  Target rotation: {single_action[3:6]}")

                    # è®¡ç®—ä½ç½®å·®å¼‚
                    pos_diff = np.linalg.norm(single_action[:3] - current_pos)
                    # è®¡ç®—æ—‹è½¬å·®å¼‚ï¼ˆä½¿ç”¨è§’åº¦å½’ä¸€åŒ–é¿å…ä¸´ç•Œç‚¹é—®é¢˜ï¼‰
                    rot_diff_rad = np.array([
                        normalize_angle(single_action[3+i] - current_rot[i])
                        for i in range(3)
                    ])
                    rot_diff_deg = np.rad2deg(rot_diff_rad)
                    rospy.loginfo(f"  Position difference: {pos_diff:.4f} m ({pos_diff*100:.2f} cm)")
                    rospy.loginfo(f"  Rotation difference: [{rot_diff_deg[0]:.2f}Â°, {rot_diff_deg[1]:.2f}Â°, {rot_diff_deg[2]:.2f}Â°]")

                    # åº”ç”¨åŠ¨ä½œæˆªæ–­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if enable_action_clipping:
                        single_action = clip_action(
                            target_action=single_action,
                            current_position=current_pos,
                            current_rotation=current_rot,
                            max_position_change=max_position_change,
                            max_rotation_change=max_rotation_change
                        )
                        rospy.loginfo(f"  Clipped action: {single_action}")

                    # è®¡ç®—ç›®æ ‡ä½å§¿ï¼ˆä½¿ç”¨ç»å¯¹åŠ¨ä½œï¼‰
                    target_pose = action_to_pose(
                        single_action[:6],
                        current_pose_obj,
                        is_relative=False
                    )

                    # æ‰§è¡Œä½å§¿
                    robot.execute_pose(target_pose, duration_s=action_duration)
                    rospy.loginfo(f"âœ“ Action {action_idx} executed")

                    # æ›´æ–°å½“å‰ä½å§¿
                    current_pose = robot.get_pose()

                    # æ§åˆ¶å¤¹çˆª
                    gripper_action = float(single_action[6])
                    rospy.loginfo(f"  Gripper action: {gripper_action} (current state: {1 if robot.gripper_state else 0})")
                    robot.control_gripper(gripper_action, threshold=gripper_threshold)
                    rospy.loginfo(f"  Gripper state after control: {1 if robot.gripper_state else 0}")

                    # ========== æ›´æ–°å†å²ç¼“å†²åŒº ==========
                    # åœ¨æ‰§è¡Œæ¯ä¸ªåŠ¨ä½œåé‡‡é›†æ–°è§‚æµ‹ï¼Œæ›´æ–°å†å²ç¼“å†²åŒº
                    # è¿™æ ·å¯ä»¥ä¿æŒå†å²å¸§çš„æ—¶åºè¿ç»­æ€§ï¼Œä¸è®­ç»ƒæ—¶çš„æ•°æ®æ ¼å¼ä¸€è‡´
                    if action_idx < num_actions_to_execute - 1:  # æœ€åä¸€ä¸ªåŠ¨ä½œåä¸éœ€è¦æ›´æ–°ï¼ˆä¸‹ä¸€ä¸ªstepä¼šé‡æ–°è·å–ï¼‰
                        try:
                            rospy.loginfo(f"  Updating history buffer after action {action_idx}...")

                            # é‡‡é›†æ–°è§‚æµ‹
                            obs_update = camera.capture()
                            rgb_3rd_1_update = cv2.cvtColor(obs_update['3rd_1']['rgb'], cv2.COLOR_BGR2RGB)
                            rgb_3rd_2_update = cv2.cvtColor(obs_update['3rd_2']['rgb'], cv2.COLOR_BGR2RGB)
                            rgb_3rd_3_update = cv2.cvtColor(obs_update['3rd_3']['rgb'], cv2.COLOR_BGR2RGB)
                            pcd_3rd_1_update = obs_update['3rd_1']['pcd']
                            pcd_3rd_2_update = obs_update['3rd_2']['pcd']
                            pcd_3rd_3_update = obs_update['3rd_3']['pcd']

                            # è·å–å½“å‰æœºå™¨äººä½å§¿
                            current_pose_update = robot.get_pose()
                            current_position_update = current_pose_update.translation
                            current_quat_update = current_pose_update.quaternion
                            current_pose_array_update = np.concatenate([current_position_update, current_quat_update]).reshape(1, 7)

                            # é¢„å¤„ç†æ•°æ®
                            processed_pcd_list_update, processed_rgb_list_update, processed_pos_update, _, _ = client.preprocess(
                                [[pcd_3rd_1_update, pcd_3rd_2_update, pcd_3rd_3_update]],
                                [[rgb_3rd_1_update, rgb_3rd_2_update, rgb_3rd_3_update]],
                                current_pose_array_update
                            )

                            # ç”Ÿæˆæ–°çš„è¾“å…¥å›¾åƒ
                            processed_pcd_update = processed_pcd_list_update[0]
                            processed_rgb_update = processed_rgb_list_update[0]
                            rgb_images_update = client.get_rgb_input(processed_pcd_update, processed_rgb_update)
                            heatmap_images_update = client.get_heatmap_input(processed_pos_update)

                            # æ›´æ–°å†å²ç¼“å†²åŒºï¼ˆä¸å‘é€predictè¯·æ±‚ï¼‰
                            client.update_history_buffer(heatmap_images_update, rgb_images_update)

                        except Exception as e:
                            rospy.logwarn(f"  Failed to update history buffer: {e}")

                except Exception as e:
                    rospy.logerr(f"Error executing action {action_idx}: {e}")
                    continue

            # æ­¥æ•°é€’å¢
            step += 1
            # a=input("CONTINUE?")
            rospy.loginfo(f"Step {step} completed\n")

    except KeyboardInterrupt:
        rospy.loginfo("\nControl loop interrupted by user")
    except Exception as e:
        rospy.logerr(f"Fatal error in control loop: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†æœºå™¨äººèµ„æºï¼Œåœæ­¢æ‰€æœ‰skill
        rospy.loginfo("\n" + "="*60)
        rospy.loginfo("Cleaning up robot resources...")
        try:
            robot.cleanup()
        except Exception as e:
            rospy.logwarn(f"Error during cleanup: {e}")

        rospy.loginfo("Control Loop Finished")
        rospy.loginfo(f"Total steps executed: {step}")
        rospy.loginfo(f"Data saved to: {save_dir}")
        rospy.loginfo("="*60)


def main():
    """
    RoboWançœŸæœºæ§åˆ¶ä¸»ç¨‹åºå…¥å£

    ä½¿ç”¨æ–¹æ³•ï¼š
        python RoboWan_client.py                    # ä½¿ç”¨é»˜è®¤å‚æ•°
        python RoboWan_client.py --help             # æ˜¾ç¤ºå¸®åŠ©
    """
    import argparse

    parser = argparse.ArgumentParser(
        description="RoboWançœŸæœºæ§åˆ¶å®¢æˆ·ç«¯ - è¿æ¥æœåŠ¡å™¨å¹¶æ§åˆ¶æœºå™¨äººæ‰§è¡Œä»»åŠ¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
  ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ç”¨æ³•ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œé»˜è®¤å¯ç”¨åŠ¨ä½œæˆªæ–­ï¼‰
  python RoboWan_client.py

  # æŒ‡å®šæœåŠ¡å™¨åœ°å€å’Œä»»åŠ¡
  python RoboWan_client.py --server http://10.10.1.21:5555 --task "put the lion on the top shelf"

  # æŒ‡å®šæœ€å¤§æ­¥æ•°ã€é¢„æµ‹å¸§æ•°å’Œä¿å­˜ç›®å½•
  python RoboWan_client.py --max-steps 50 --num-frames 10 --save-dir /path/to/save

  # è‡ªå®šä¹‰åœºæ™¯è¾¹ç•Œå’Œå›¾åƒå°ºå¯¸
  python RoboWan_client.py --scene-bounds "0,-0.55,-0.05,0.8,0.45,0.6" --img-size 384

  # è‡ªå®šä¹‰åŠ¨ä½œæˆªæ–­å‚æ•°ï¼ˆä½ç½®æœ€å¤§å˜åŒ–5cmï¼Œæ—‹è½¬æœ€å¤§å˜åŒ–15åº¦ ï¼‰
  python RoboWan_client.py --max-position-change 0.05 --max-rotation-change 15.0

  # ç¦ç”¨åŠ¨ä½œæˆªæ–­
  python RoboWan_client.py --disable-action-clipping
        """
    )

    # æœåŠ¡å™¨é…ç½®
    parser.add_argument(
        '--server',
        type=str,
        default="http://localhost:5555",
        help='RoboWanæœåŠ¡å™¨åœ°å€ (é»˜è®¤: http://localhost:5555ï¼Œé€šè¿‡SSHéš§é“è¿æ¥)'
    )

    # ä»»åŠ¡é…ç½®
    parser.add_argument(
        '--task',
        type=str,
        default="put the lion on the top shelf",
        help='ä»»åŠ¡æŒ‡ä»¤'
    )

    parser.add_argument(
        '--max-steps',
        type=int,
        default=100000,
        help='æœ€å¤§æ§åˆ¶æ­¥æ•° (é»˜è®¤: 100)'
    )

    parser.add_argument(
        '--num-frames',
        type=int,
        default=NUM_FRAMES,
        help=f'æ¯æ¬¡é¢„æµ‹çš„å¸§æ•° (é»˜è®¤: {NUM_FRAMES}), åŒ…æ‹¬äº†åˆå§‹å¸§'
    )

    parser.add_argument(
        '--scene-bounds',
        type=str,
        default=None,
        help='åœºæ™¯è¾¹ç•Œï¼Œæ ¼å¼ä¸ºé€—å·åˆ†éš”çš„6ä¸ªæ•°å­—: x_min,y_min,z_min,x_max,y_max,z_max (é»˜è®¤: ä½¿ç”¨SCENE_BOUNDSå…¨å±€é…ç½®)'
    )

    parser.add_argument(
        '--img-size',
        type=int,
        default=None,
        help='å›¾åƒå°ºå¯¸ (é»˜è®¤: ä½¿ç”¨IMG_SIZEå…¨å±€é…ç½®)'
    )

    # åŠ¨ä½œæˆªæ–­å‚æ•°
    parser.add_argument(
        '--enable-action-clipping',
        action='store_true',
        default=None,
        help='å¯ç”¨åŠ¨ä½œæˆªæ–­ï¼ˆé™åˆ¶ä½ç½®å’Œæ—‹è½¬çš„æœ€å¤§å˜åŒ–ï¼‰'
    )

    parser.add_argument(
        '--disable-action-clipping',
        action='store_true',
        default=False,
        help='ç¦ç”¨åŠ¨ä½œæˆªæ–­'
    )

    parser.add_argument(
        '--max-position-change',
        type=float,
        default=None,
        help='ä½ç½®æœ€å¤§å˜åŒ–é‡ï¼ˆç±³ï¼‰ (é»˜è®¤: 0.04m = 4cm)'
    )

    parser.add_argument(
        '--max-rotation-change',
        type=float,
        default=None,
        help='æ—‹è½¬æœ€å¤§å˜åŒ–é‡ï¼ˆåº¦ï¼‰ (é»˜è®¤: 10.0Â°)'
    )

    # ç‚¹äº‘é…ç½®å‚æ•°
    parser.add_argument(
        '--use-merged-pointcloud',
        action='store_true',
        default=None,
        help='ä½¿ç”¨ä¸‰ä¸ªç›¸æœºæ‹¼æ¥çš„ç‚¹äº‘ï¼ˆé»˜è®¤: ä½¿ç”¨USE_MERGED_POINTCLOUDå…¨å±€é…ç½®ï¼‰'
    )

    parser.add_argument(
        '--no-merged-pointcloud',
        action='store_true',
        default=False,
        help='åªä½¿ç”¨ç›¸æœº1çš„ç‚¹äº‘ï¼ˆè¦†ç›–USE_MERGED_POINTCLOUDå…¨å±€é…ç½®ï¼‰'
    )

    # æ§åˆ¶å‚æ•°
    parser.add_argument(
        '--action-duration',
        type=float,
        default=0.5,
        help='æ¯ä¸ªåŠ¨ä½œæ‰§è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰ (é»˜è®¤: 0.5)'
    )

    parser.add_argument(
        '--gripper-threshold',
        type=float,
        default=0.5,
        help='å¤¹çˆªåŠ¨ä½œé˜ˆå€¼ (é»˜è®¤: 0.5)' # æ„Ÿè§‰è¿™ä¸ªæ²¡æœ‰ä»€ä¹ˆç”¨
    )

    # æ•°æ®ä¿å­˜
    parser.add_argument(
        '--save-dir',
        type=str,
        default="/media/casia/data4/lpy/RoboWan/logs",
        help='æ•°æ®ä¿å­˜ç›®å½• (é»˜è®¤: è‡ªåŠ¨åˆ›å»ºæ—¶é—´æˆ³ç›®å½•)'
    )

    args = parser.parse_args()

    # è§£æscene_boundså‚æ•°
    scene_bounds = None
    if args.scene_bounds is not None:
        try:
            scene_bounds = [float(x.strip()) for x in args.scene_bounds.split(',')]
            if len(scene_bounds) != 6:
                print("é”™è¯¯: scene_boundså¿…é¡»åŒ…å«6ä¸ªæ•°å­— (x_min,y_min,z_min,x_max,y_max,z_max)")
                return
        except ValueError:
            print("é”™è¯¯: scene_boundsæ ¼å¼é”™è¯¯ï¼Œå¿…é¡»æ˜¯é€—å·åˆ†éš”çš„6ä¸ªæ•°å­—")
            return

    # å¤„ç†åŠ¨ä½œæˆªæ–­å‚æ•°
    enable_action_clipping = None
    if args.disable_action_clipping:
        enable_action_clipping = False
    elif args.enable_action_clipping:
        enable_action_clipping = True
    # å¦‚æœä¸¤ä¸ªéƒ½æ²¡æŒ‡å®šï¼Œåˆ™ä¸ºNoneï¼Œä½¿ç”¨å…¨å±€é…ç½®

    # å¤„ç†ç‚¹äº‘é…ç½®å‚æ•°
    use_merged_pointcloud = None
    if args.no_merged_pointcloud:
        use_merged_pointcloud = False
    elif args.use_merged_pointcloud:
        use_merged_pointcloud = True
    # å¦‚æœä¸¤ä¸ªéƒ½æ²¡æŒ‡å®šï¼Œåˆ™ä¸ºNoneï¼Œä½¿ç”¨å…¨å±€é…ç½®

    # æ£€æŸ¥æœºå™¨äººæ¥å£æ˜¯å¦å¯ç”¨
    if not ROBOT_AVAILABLE:
        print("="*60)
        print("é”™è¯¯ï¼šæœºå™¨äººæ¥å£ä¸å¯ç”¨ï¼")
        print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š")
        print("  - frankapy")
        print("  - diffusion_policy")
        print("  - autolab_core")
        print("="*60)
        return

    # åˆå§‹åŒ–ROS - RobotController å†…éƒ¨ä¼šè‡ªåŠ¨åˆå§‹åŒ–ROSèŠ‚ç‚¹
    # å¦‚æœéœ€è¦æ‰‹åŠ¨åˆå§‹åŒ–ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # try:
    #     if not rospy.core.is_initialized():
    #         rospy.init_node('robowan_client', anonymous=True)
    # except Exception as e:
    #     print(f"é”™è¯¯: ROSåˆå§‹åŒ–å¤±è´¥ - {e}")
    #     print("è¯·ç¡®ä¿å·²å¯åŠ¨ roscore")
    #     return

    # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„é…ç½®
    final_enable_clipping = enable_action_clipping if enable_action_clipping is not None else ENABLE_ACTION_CLIPPING
    final_max_pos_change = args.max_position_change if args.max_position_change is not None else MAX_POSITION_CHANGE
    final_max_rot_change = args.max_rotation_change if args.max_rotation_change is not None else MAX_ROTATION_CHANGE
    final_use_merged_pointcloud = use_merged_pointcloud if use_merged_pointcloud is not None else USE_MERGED_POINTCLOUD

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\n" + "="*60)
    print("RoboWançœŸæœºæ§åˆ¶å®¢æˆ·ç«¯")
    print("="*60)
    print(f"æœåŠ¡å™¨åœ°å€:        {args.server}")
    print(f"ä»»åŠ¡æŒ‡ä»¤:          {args.task}")
    print(f"æœ€å¤§æ­¥æ•°:          {args.max_steps}")
    print(f"é¢„æµ‹å¸§æ•°:          {args.num_frames}    (initial frame included)")
    print(f"åœºæ™¯è¾¹ç•Œ:          {scene_bounds if scene_bounds else SCENE_BOUNDS} {'(è‡ªå®šä¹‰)' if scene_bounds else '(é»˜è®¤)'}")
    print(f"å›¾åƒå°ºå¯¸:          {args.img_size if args.img_size else IMG_SIZE} {'(è‡ªå®šä¹‰)' if args.img_size else '(é»˜è®¤)'}")
    print(f"ç‚¹äº‘èåˆæ¨¡å¼:      {'ä½¿ç”¨ä¸‰ç›¸æœºæ‹¼æ¥' if final_use_merged_pointcloud else 'åªä½¿ç”¨ç›¸æœº1'} {'(è‡ªå®šä¹‰)' if use_merged_pointcloud is not None else '(é»˜è®¤)'}")
    print(f"åŠ¨ä½œæ‰§è¡Œæ—¶é•¿:      {args.action_duration}ç§’")
    print(f"å¤¹çˆªé˜ˆå€¼:          {args.gripper_threshold}")
    print("")
    print("åŠ¨ä½œæˆªæ–­é…ç½®:")
    if final_enable_clipping:
        print(f"  çŠ¶æ€:            å¯ç”¨ âœ“")
        print(f"  æœ€å¤§ä½ç½®å˜åŒ–:    {final_max_pos_change*100:.1f} cm")
        print(f"  æœ€å¤§æ—‹è½¬å˜åŒ–:    {final_max_rot_change:.1f}Â°")
    else:
        print(f"  çŠ¶æ€:            ç¦ç”¨ âœ—")
    print("")
    print(f"ä¿å­˜ç›®å½•:          {args.save_dir if args.save_dir else 'è‡ªåŠ¨åˆ›å»º'}")
    print("="*60)

    # ç¡®è®¤å¯åŠ¨
    try:
        user_input = input("\næ˜¯å¦å¼€å§‹æ§åˆ¶æœºå™¨äºº? (yes/no): ").strip().lower()
        if user_input not in ['yes', 'y']:
            print("å·²å–æ¶ˆ")
            return
    except KeyboardInterrupt:
        print("\nå·²å–æ¶ˆ")
        return

    print("\nå¼€å§‹çœŸæœºæ§åˆ¶...")
    print("æŒ‰ Ctrl+C å¯éšæ—¶åœæ­¢\n")

    # å¯åŠ¨çœŸæœºæ§åˆ¶å¾ªç¯
    try:
        real_robot_control_loop(
            server_url=args.server,
            task_prompt=args.task,
            max_steps=args.max_steps,
            num_frames=args.num_frames,
            save_dir=args.save_dir,
            action_duration=args.action_duration,
            gripper_threshold=args.gripper_threshold,
            scene_bounds=scene_bounds,
            img_size=args.img_size,
            enable_action_clipping=enable_action_clipping,
            max_position_change=args.max_position_change,
            max_rotation_change=args.max_rotation_change,
            use_merged_pointcloud=use_merged_pointcloud,
        )
    except KeyboardInterrupt:
        rospy.loginfo("\nç”¨æˆ·ä¸­æ–­æ§åˆ¶")
    except Exception as e:
        rospy.logerr(f"\næ§åˆ¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
