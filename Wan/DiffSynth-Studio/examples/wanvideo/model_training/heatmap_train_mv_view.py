"""
Heatmap Training Script for Wan2.2
åŸºäºŽåŽŸå§‹train.pyï¼Œä¸“é—¨ç”¨äºŽçƒ­åŠ›å›¾åºåˆ—ç”Ÿæˆè®­ç»ƒ ä¸”æ”¯æŒå¤šè§†è§’
"""

import torch
import os
import json
import sys
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import torch.nn as nn

# å¼ºåˆ¶åˆ·æ–°è¾“å‡ºï¼Œç¡®ä¿é”™è¯¯ä¿¡æ¯èƒ½è¢«æ˜¾ç¤º
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering=1)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', buffering=1)

# SwanLabå¯¼å…¥ï¼ˆå¯é€‰ï¼‰
try:
    import swanlab
    SWANLAB_AVAILABLE = True
    print(f"âœ“ SwanLab imported successfully (version: {swanlab.__version__})")
except ImportError as e:
    SWANLAB_AVAILABLE = False
    print(f"Warning: SwanLab not available. Install with: pip install swanlab")
    print(f"ImportError details: {e}")
    assert False
except Exception as e:
    SWANLAB_AVAILABLE = False
    print(f"Warning: SwanLab import failed with error: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()
from diffsynth.trainers.utils import DiffusionTrainingModule, ModelLogger, launch_training_task, wan_parser
from accelerate import Accelerator
from accelerate.utils import DistributedDataParallelKwargs
from tqdm import tqdm
import torch.utils.data
from torch.utils.data import ConcatDataset

# æ·»åŠ trainersè·¯å¾„ä»¥å¯¼å…¥æˆ‘ä»¬çš„è‡ªå®šä¹‰æ•°æ®é›†
sys.path.append(os.path.join(os.path.dirname(__file__), "../../../"))
# from diffsynth.trainers.heatmap_dataset_mv_with_rot_grip import HeatmapDatasetFactory
from diffsynth.trainers.heatmap_dataset_mv_with_rot_grip_3cam import HeatmapDatasetFactory
# å¤šå¸§åŽ†å²æ”¯æŒçš„æ•°æ®é›†å’Œå·¥åŽ‚
from diffsynth.trainers.heatmap_dataset_mv_with_rot_grip_3cam_history import HeatmapDatasetFactoryWithHistory

os.environ["TOKENIZERS_PARALLELISM"] = "false"


class HeatmapWanTrainingModule(DiffusionTrainingModule):
    """
    çƒ­åŠ›å›¾ä¸“ç”¨çš„Wanè®­ç»ƒæ¨¡å—
    ç»§æ‰¿åŽŸå§‹WanTrainingModuleï¼Œé’ˆå¯¹çƒ­åŠ›å›¾åºåˆ—ç”Ÿæˆè¿›è¡Œä¼˜åŒ–
    """

    def __init__(
        self,
        wan_type,
        model_paths=None, model_id_with_origin_paths=None,
        trainable_models=None,
        lora_base_model=None, lora_target_modules="q,k,v,o,ffn.0,ffn.2", lora_rank=32, lora_checkpoint=None,
        use_gradient_checkpointing=True,
        extra_inputs=None,
        max_timestep_boundary=1.0,
        min_timestep_boundary=0.0,
        use_dual_head=False,
        unfreeze_modulation_and_norms=False,  # æ–°å¢žï¼šæŽ§åˆ¶æ˜¯å¦è§£å†» modulation å’Œ norm å‚æ•°
        num_history_frames=1,  # åŽ†å²å¸§æ•°é‡ï¼Œç”¨äºŽå¤šå¸§æ¡ä»¶æ¨¡å¼
        rgb_loss_weight=0.5,  # RGB lossæƒé‡ï¼Œæ€»loss = rgb_loss_weight * loss_rgb + (1 - rgb_loss_weight) * loss_heatmap
    ):
        super().__init__()

        # ä¿å­˜é…ç½®å‚æ•°
        self.unfreeze_modulation_and_norms = unfreeze_modulation_and_norms
        self.num_history_frames = num_history_frames
        self.rgb_loss_weight = rgb_loss_weight

        # Debug: æ˜¾ç¤ºé¢„è®­ç»ƒcheckpointé…ç½®
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if local_rank == 0:
            print("\n" + "="*80)
            print("ðŸš€ INITIALIZING TRAINING MODULE")
            print("="*80)
            print(f"  WAN Type: {wan_type}")
            print(f"  LoRA Base Model: {lora_base_model}")
            print(f"  LoRA Target Modules: {lora_target_modules}")
            print(f"  LoRA Rank: {lora_rank}")
            print(f"  Use Dual Head: {use_dual_head}")
            print(f"  RGB Loss Weight: {rgb_loss_weight}")
            print(f"  Unfreeze Modulation & Norms: {unfreeze_modulation_and_norms}")
            if lora_checkpoint is not None:
                print(f"\n  ðŸ“¦ PRETRAINED CHECKPOINT SPECIFIED:")
                print(f"     Path: {lora_checkpoint}")
                if os.path.exists(lora_checkpoint):
                    file_size_mb = os.path.getsize(lora_checkpoint) / (1024 * 1024)
                    print(f"     âœ“ File exists, size: {file_size_mb:.2f} MB")
                else:
                    print(f"     âœ— WARNING: File does not exist!")
            else:
                print(f"\n  â„¹ï¸  No pretrained checkpoint specified (training from scratch)")
            print("="*80 + "\n")

        self.wan_type = wan_type
        if self.wan_type == "WAN_2_1_14B_I2V":
            from diffsynth.pipelines.wan_video_14BI2V_condition_rgb_heatmap_first import WanVideoPipeline, ModelConfig
        elif self.wan_type == "5B_TI2V":
            from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig
        elif self.wan_type == "5B_TI2V_RGB_HEATMAP":
            from diffsynth.pipelines.wan_video_5B_TI2V_heatmap_and_rgb import WanVideoPipeline, ModelConfig
        elif self.wan_type == "5B_TI2V_RGB_HEATMAP_MV":
            # View concatenation mode: uses view-concat pipeline
            from diffsynth.pipelines.wan_video_5B_TI2V_heatmap_and_rgb_mv_view import WanVideoPipeline, ModelConfig
            from diffsynth.models.wan_video_dit_mv import SelfAttention
        elif self.wan_type == "5B_TI2V_RGB_HEATMAP_MV_HISTORY":
            # å¤šå¸§åŽ†å²æ¡ä»¶çš„pipelineï¼Œä½¿ç”¨ä¿®æ”¹åŽçš„VAE
            from diffsynth.pipelines.wan_video_5B_TI2V_heatmap_and_rgb_mv_history import WanVideoPipeline, ModelConfig
            from diffsynth.models.wan_video_dit_mv import SelfAttention
        else:
            assert False, f"Unsupported wan_type: {self.wan_type}"
        # Load models (ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼)
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))

        model_configs = self.parse_model_configs(model_paths, model_id_with_origin_paths, enable_fp8_training=False)
        if local_rank == 0:
            print("\n" + "="*80)
            print(f"[DEBUG] Loading pipeline with use_dual_head={use_dual_head}")
            print("="*80)
        self.pipe = WanVideoPipeline.from_pretrained(torch_dtype=torch.bfloat16, device="cpu", model_configs=model_configs, wan_type=self.wan_type, use_dual_head=use_dual_head)
        self.use_dual_head = use_dual_head
        if local_rank == 0:
            print("\n" + "="*80)
            print(f"[DEBUG] Pipeline loaded successfully with use_dual_head={use_dual_head}")
            print("="*80 + "\n")

        if self.wan_type in ["5B_TI2V_RGB_HEATMAP_MV", "5B_TI2V_RGB_HEATMAP_MV_HISTORY"]:
            # add mv attention module - ç¡®ä¿dtypeä¸€è‡´
            dim=self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
            # èŽ·å–æ¨¡åž‹çš„dtypeå’Œdeviceï¼Œç¡®ä¿æ–°æ·»åŠ çš„æ¨¡å—ä¸ŽçŽ°æœ‰æ¨¡åž‹ä¸€è‡´
            model_dtype = self.pipe.dit.blocks[0].self_attn.q.weight.dtype
            model_device = self.pipe.dit.blocks[0].self_attn.q.weight.device

            if local_rank == 0:
                print(f"[DEBUG] Adding multi-view attention modules with dtype={model_dtype}, device={model_device}")

            for block in self.pipe.dit.blocks:
                # åˆ›å»ºprojectorå¹¶è½¬æ¢åˆ°æ­£ç¡®çš„dtypeå’Œdevice
                block.projector = nn.Linear(dim, dim).to(dtype=model_dtype, device=model_device)
                block.projector.weight.data.zero_()
                block.projector.bias.data.zero_()

                # åˆ›å»ºnorm_mvså¹¶è½¬æ¢dtype
                block.norm_mvs = nn.LayerNorm(dim, eps=block.norm1.eps, elementwise_affine=False).to(dtype=model_dtype, device=model_device)

                # åˆ›å»ºmodulation_mvså‚æ•°ï¼Œä½¿ç”¨æ­£ç¡®çš„dtype
                block.modulation_mvs = nn.Parameter(torch.randn(1, 3, dim, dtype=model_dtype, device=model_device) / dim**0.5)
                block.modulation_mvs.data = block.modulation.data[:, :3, :].clone()

                # åˆ›å»ºmvs_attnï¼Œå…ˆåŠ è½½æƒé‡ï¼Œå†è½¬æ¢dtype
                block.mvs_attn = SelfAttention(dim, block.self_attn.num_heads, block.self_attn.norm_q.eps)
                block.mvs_attn.load_state_dict(block.self_attn.state_dict(), strict=True)
                block.mvs_attn = block.mvs_attn.to(dtype=model_dtype, device=model_device)
        

        # Training mode (ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼)
        self.switch_pipe_to_training_mode(
            self.pipe, trainable_models,
            lora_base_model, lora_target_modules, lora_rank, lora_checkpoint=lora_checkpoint,
            enable_fp8_training=False,
        )

        # ============================================================
        # è§£å†» patch_embedding å’Œ head.head çš„æ‰€æœ‰å‚æ•°
        # è¿™äº›å±‚ä½¿ç”¨å…¨é‡è®­ç»ƒè€ŒéžLoRA
        # ============================================================
        self._unfreeze_patch_embedding_and_head()

        # ============================================================
        # è§£å†»å¤šè§†è§’æ¨¡å—çš„æ‰€æœ‰å‚æ•°ï¼ˆä»…åœ¨MVæ¨¡å¼ä¸‹ï¼‰
        # è¿™äº›æ¨¡å—åŒ…æ‹¬ï¼šprojector, norm_mvs, modulation_mvs, mvs_attn
        # ============================================================
        if self.wan_type in ["5B_TI2V_RGB_HEATMAP_MV", "5B_TI2V_RGB_HEATMAP_MV_HISTORY"]:
            self._unfreeze_mv_modules()

        # ============================================================
        # è§£å†» modulation å‚æ•°ï¼ˆå¯é€‰ï¼Œç”± unfreeze_modulation_and_norms æŽ§åˆ¶ï¼‰
        # AdaLN è°ƒåˆ¶å‚æ•°ï¼Œç”¨äºŽæ ¹æ®æ—¶é—´æ­¥ t è°ƒæ•´å½’ä¸€åŒ–
        # ============================================================
        self._unfreeze_modulation()

        # ============================================================
        # å‚æ•°ç›‘æŽ§ï¼šæ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        # ============================================================
        self._print_trainable_parameters_info()


        # Store other configs (ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼)
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.use_gradient_checkpointing_offload = False  # æ·»åŠ æ­¤å‚æ•°ä»¥ä¿æŒä¸Žtrain.pyçš„å…¼å®¹æ€§
        self.extra_inputs = extra_inputs.split(",") if extra_inputs is not None else []
        self.max_timestep_boundary = max_timestep_boundary
        self.min_timestep_boundary = min_timestep_boundary

        # Debug settings
        self.debug_counter = 0
        self.debug_save_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../debug_log"))

    def _unfreeze_patch_embedding_and_head(self):
        """
        è§£å†» patch_embedding å’Œ head çš„æ‰€æœ‰å‚æ•°ï¼ˆæƒé‡å’Œbiasï¼‰

        è¿™äº›å±‚ä½¿ç”¨å…¨é‡è®­ç»ƒè€ŒéžLoRAå¾®è°ƒã€‚
        ç”±äºŽå®ƒä»¬è¢«é‡æ–°åˆå§‹åŒ–ä»¥é€‚åº”æ–°çš„è¾“å…¥/è¾“å‡ºç»´åº¦ï¼Œ
        æ‰€æœ‰å‚æ•°éƒ½éœ€è¦ä»Žå¤´å­¦ä¹ ã€‚

        åœ¨åŒheadæ¨¡å¼ä¸‹ï¼š
        - è§£å†» head_rgb å’Œ head_heatmap (è€Œä¸æ˜¯ head)
        - è§£å†» patch_embedding_rgb å’Œ patch_embedding_heatmap (è€Œä¸æ˜¯ patch_embedding)
        """
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))

        unfrozen_params = []
        all_head_params = []  # ç”¨äºŽè°ƒè¯•ï¼šæ‰€æœ‰åŒ…å«headçš„å‚æ•°
        all_patch_embedding_params = []  # ç”¨äºŽè°ƒè¯•ï¼šæ‰€æœ‰patch_embeddingå‚æ•°

        for name, param in self.pipe.dit.named_parameters():
            # è°ƒè¯•ï¼šæ”¶é›†æ‰€æœ‰åŒ…å«headçš„å‚æ•°å
            if 'head' in name:
                all_head_params.append((name, param.requires_grad))
            # è°ƒè¯•ï¼šæ”¶é›†æ‰€æœ‰patch_embeddingçš„å‚æ•°å
            if 'patch_embedding' in name:
                all_patch_embedding_params.append((name, param.requires_grad))

            # æ£€æŸ¥æ˜¯å¦æ˜¯ patch_embedding æˆ– head çš„å‚æ•°
            # åœ¨åŒheadæ¨¡å¼ä¸‹ï¼š
            #   - åŒ¹é… head_rgb å’Œ head_heatmap
            #   - åŒ¹é… patch_embedding_rgb å’Œ patch_embedding_heatmap
            # åœ¨å•headæ¨¡å¼ä¸‹ï¼š
            #   - åŒ¹é… head
            #   - åŒ¹é… patch_embedding
            # æ³¨æ„ï¼šè¿™é‡Œä¸åŒ…æ‹¬LoRAå‚æ•°ï¼ˆlora_A/lora_Bï¼‰ï¼ŒåªåŒ…æ‹¬åŽŸå§‹å±‚çš„weightå’Œbias
            is_patch_embedding = 'patch_embedding' in name
            is_head = ('head' in name) if not self.use_dual_head else ('head_rgb' in name or 'head_heatmap' in name)
            is_not_lora = ('lora_A' not in name and 'lora_B' not in name)

            if (is_patch_embedding or is_head) and is_not_lora:
                # è§£å†»è¿™ä¸ªå‚æ•°
                param.requires_grad = True
                unfrozen_params.append(name)

        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        if local_rank == 0:
            print("\n" + "="*80)
            mode_str = "DUAL HEAD + DUAL PATCH_EMBEDDING" if self.use_dual_head else "SINGLE HEAD + SINGLE PATCH_EMBEDDING"
            print(f"FULL PARAMETER UNFREEZING FOR PATCH_EMBEDDING AND HEAD ({mode_str} MODE)")
            print("="*80)

            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ‰€æœ‰patch_embeddingçš„å‚æ•°
            print(f"\n[DEBUG] All parameters with 'patch_embedding' in name ({len(all_patch_embedding_params)} total):")
            for name, requires_grad in all_patch_embedding_params:
                print(f"  - {name}: requires_grad={requires_grad}")

            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ‰€æœ‰åŒ…å«headçš„å‚æ•°
            print(f"\n[DEBUG] All parameters with 'head' in name ({len(all_head_params)} total):")
            for name, requires_grad in all_head_params:
                print(f"  - {name}: requires_grad={requires_grad}")

            if len(unfrozen_params) > 0:
                print(f"\nUnfroze {len(unfrozen_params)} parameter(s) for full training:")
                for name in unfrozen_params:
                    param = dict(self.pipe.dit.named_parameters())[name]
                    print(f"  âœ“ {name}")
                    print(f"    Shape: {param.shape}, Trainable: {param.requires_grad}")
            else:
                print("\nâš ï¸  WARNING: No parameters were unfrozen!")
            print("="*80 + "\n")

    def _unfreeze_modulation(self):
        """
        è§£å†» modulation å‚æ•°ï¼ˆAdaLN è°ƒåˆ¶å‚æ•°ï¼‰

        Modulation å‚æ•°åœ¨ DiT ä¸­ç”¨äºŽ Adaptive Layer Normalization (AdaLN)ï¼š
        - æ ¹æ®æ—¶é—´æ­¥ t åŠ¨æ€è°ƒæ•´æ¯å±‚çš„ scale å’Œ shift
        - å¯¹äºŽä¸åŒä»»åŠ¡ï¼ˆå¦‚æœºæ¢°è‡‚è½¨è¿¹é¢„æµ‹ vs é€šç”¨è§†é¢‘ç”Ÿæˆï¼‰ï¼Œæ—¶é—´æ­¥çš„è¯­ä¹‰å¯èƒ½ä¸åŒ
        - å‚æ•°é‡å¾ˆå°ï¼ˆæ¯ä¸ª block çº¦ 18K å‚æ•°ï¼‰ï¼Œä½†å½±å“å¾ˆå¤§

        åªæœ‰å½“ self.unfreeze_modulation_and_norms=True æ—¶æ‰è§£å†»
        """
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))

        if not self.unfreeze_modulation_and_norms:
            if local_rank == 0:
                print("\n" + "="*80)
                print("â„¹ï¸  MODULATION PARAMETERS: KEEPING FROZEN (Backward Compatible)")
                print("="*80)
                print("  Modulation parameters will stay frozen (using pretrained values).")
                print("  To unfreeze, add --unfreeze_modulation_and_norms flag.")
                print("="*80 + "\n")
            return

        unfrozen_params = []

        for name, param in self.pipe.dit.named_parameters():
            # åªè§£å†» modulation å‚æ•°ï¼ˆä¸åŒ…æ‹¬ modulation_mvsï¼Œå®ƒå·²åœ¨ _unfreeze_mv_modules ä¸­å¤„ç†ï¼‰
            if 'modulation' in name and 'modulation_mvs' not in name and 'blocks.' in name:
                param.requires_grad = True
                unfrozen_params.append(name)

        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        if local_rank == 0:
            print("\n" + "="*80)
            print("âš¡ MODULATION PARAMETER UNFREEZING")
            print("="*80)

            if len(unfrozen_params) > 0:
                print(f"\nâœ… Unfroze {len(unfrozen_params)} modulation parameters:")
                print(f"  ðŸ“¦ AdaLN modulation: {len(unfrozen_params)} parameters")

                # è®¡ç®—æ€»å‚æ•°é‡
                total_modulation_params = sum(
                    self.pipe.dit.state_dict()[name].numel()
                    for name in unfrozen_params
                )
                print(f"  ðŸ“Š Total: {total_modulation_params:,} parameters (~{total_modulation_params/1e6:.2f}M)")

                print("\nðŸ’¡ è®­ç»ƒç­–ç•¥:")
                print("  - Modulation: å…¨é‡è®­ç»ƒï¼ˆé€‚åº”æ–°ä»»åŠ¡çš„æ—¶é—´æ­¥è°ƒåˆ¶æ¨¡å¼ï¼‰")
            else:
                print("\nâš ï¸  WARNING: No modulation parameters were unfrozen!")
            print("="*80 + "\n")

    def _unfreeze_mv_modules(self):
        """
        è§£å†»å¤šè§†è§’ï¼ˆMulti-Viewï¼‰æ¨¡å—çš„å‚æ•°

        è¿™äº›æ¨¡å—åœ¨ç¬¬86-97è¡Œè¢«æ·»åŠ åˆ°æ¯ä¸ªDiT blockä¸­ï¼š
        - projector: Linearå±‚ï¼Œç”¨äºŽå¤šè§†è§’ç‰¹å¾æŠ•å½± (å…¨é‡è®­ç»ƒ)
        - norm_mvs: LayerNormå±‚ï¼Œç”¨äºŽå¤šè§†è§’ç‰¹å¾å½’ä¸€åŒ– (æ— å¯è®­ç»ƒå‚æ•°)
        - modulation_mvs: è°ƒåˆ¶å‚æ•° (å…¨é‡è®­ç»ƒ)
        - mvs_attn: SelfAttentionå±‚ï¼Œç”¨äºŽå¤šè§†è§’æ³¨æ„åŠ›è®¡ç®— (LoRAè®­ç»ƒ)

        è®­ç»ƒç­–ç•¥ï¼š
        - projector, modulation_mvs: å…¨é‡è®­ç»ƒï¼ˆè§£å†»æ‰€æœ‰æƒé‡å’Œbiasï¼‰
        - mvs_attn.norm_q/norm_k: å¯é€‰å…¨é‡è®­ç»ƒï¼ˆå¦‚æžœ unfreeze_modulation_and_norms=Trueï¼‰
        - mvs_attn: åªä¿æŒ LoRA å‚æ•°å¯è®­ç»ƒï¼ŒåŽŸå§‹æƒé‡ä¿æŒå†»ç»“
          è¿™æ ·å¯ä»¥å‡å°‘æ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†
        """
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))

        unfrozen_params = []
        all_mv_params = []  # ç”¨äºŽè°ƒè¯•ï¼šæ‰€æœ‰ MV ç›¸å…³çš„å‚æ•°

        for name, param in self.pipe.dit.named_parameters():
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šè§†è§’ç›¸å…³çš„å‚æ•°
            is_projector = 'projector' in name and 'blocks.' in name
            is_modulation_mvs = 'modulation_mvs' in name
            is_mvs_attn = 'mvs_attn' in name

            # æ”¶é›†æ‰€æœ‰ MV ç›¸å…³å‚æ•°ç”¨äºŽè°ƒè¯•
            if is_projector or is_modulation_mvs or is_mvs_attn:
                all_mv_params.append((name, param.requires_grad))

            # æŽ’é™¤ LoRA å‚æ•°ï¼ˆlora_A/lora_B å·²ç»åœ¨æ·»åŠ æ—¶è®¾ä¸ºå¯è®­ç»ƒï¼‰
            is_not_lora = ('lora_A' not in name and 'lora_B' not in name)

            # è§£å†»ç­–ç•¥ï¼š
            # 1. projector å’Œ modulation_mvs: å…¨é‡è®­ç»ƒï¼ˆæ‰€æœ‰å‚æ•°ï¼‰
            # 2. mvs_attn: LoRA å‚æ•° + å¯é€‰çš„ norm_q/norm_k
            if (is_projector or is_modulation_mvs) and is_not_lora:
                # projector å’Œ modulation_mvs å…¨é‡è®­ç»ƒ
                param.requires_grad = True
                unfrozen_params.append(name)
            elif is_mvs_attn:
                # mvs_attn çš„ norm_q/norm_kï¼ˆå¦‚æžœå¯ç”¨ï¼‰
                if self.unfreeze_modulation_and_norms and is_not_lora:
                    if 'norm_q' in name or 'norm_k' in name:
                        param.requires_grad = True
                        unfrozen_params.append(name)
                # mvs_attn çš„ LoRA å‚æ•°ä¿æŒå¯è®­ç»ƒï¼ˆå·²ç»åœ¨æ·»åŠ  LoRA æ—¶è®¾ç½®ï¼‰
                elif not is_not_lora:
                    unfrozen_params.append(name)

        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        if local_rank == 0:
            print("\n" + "="*80)
            print("MULTI-VIEW MODULE PARAMETER UNFREEZING")
            print("="*80)

            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ‰€æœ‰ MV ç›¸å…³å‚æ•°
            print(f"\n[DEBUG] All MV-related parameters ({len(all_mv_params)} total):")
            for name, requires_grad in all_mv_params[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {name}: requires_grad={requires_grad}")
            if len(all_mv_params) > 10:
                print(f"  ... and {len(all_mv_params) - 10} more")

            if len(unfrozen_params) > 0:
                print(f"\nâœ… Unfroze {len(unfrozen_params)} parameter(s):")

                # æŒ‰æ¨¡å—ç±»åž‹åˆ†ç»„ç»Ÿè®¡
                projector_params = [n for n in unfrozen_params if 'projector' in n]
                modulation_mvs_params = [n for n in unfrozen_params if 'modulation_mvs' in n]
                mvs_attn_lora_params = [n for n in unfrozen_params if 'mvs_attn' in n and 'lora' in n]
                mvs_attn_norm_params = [n for n in unfrozen_params if 'mvs_attn' in n and ('norm_q' in n or 'norm_k' in n)]

                print(f"\n  ðŸ“¦ Projector (å…¨é‡è®­ç»ƒ): {len(projector_params)} parameters")
                print(f"  ðŸ“¦ Modulation_mvs (å…¨é‡è®­ç»ƒ): {len(modulation_mvs_params)} parameters")
                print(f"  ðŸ“¦ MVS_Attn LoRA (LoRAè®­ç»ƒ): {len(mvs_attn_lora_params)} parameters")
                if mvs_attn_norm_params:
                    print(f"  ðŸ“¦ MVS_Attn Norms (å…¨é‡è®­ç»ƒ): {len(mvs_attn_norm_params)} parameters")

                print("\nðŸ’¡ è®­ç»ƒç­–ç•¥:")
                print("  - Projector & Modulation_mvs: å…¨é‡è®­ç»ƒï¼ˆä»Žé›¶å¼€å§‹å­¦ä¹ ï¼‰")
                print("  - MVS_Attn: LoRAå¾®è°ƒï¼ˆåˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰")
                if mvs_attn_norm_params:
                    print("  - MVS_Attn norm_q/norm_k: å…¨é‡è®­ç»ƒï¼ˆé€‚åº”å¤šè§†è§’ç‰¹å¾åˆ†å¸ƒï¼‰")
            else:
                print("\nâš ï¸  WARNING: No MV module parameters were unfrozen!")
                print("    This might indicate that the MV modules were not properly initialized.")
            print("="*80 + "\n")

    def _print_trainable_parameters_info(self):
        """
        æ‰“å°å¯è®­ç»ƒå‚æ•°çš„è¯¦ç»†ä¿¡æ¯  
        ç‰¹åˆ«å…³æ³¨ patch_embedding å’Œ head çš„å‚æ•°çŠ¶æ€
        åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        """
        # debug æ‰“å°å¤šè§†è§’æ¨¡å—åˆ°åº•æ˜¯loraåœ¨è°ƒè¿˜æ˜¯å…¨é‡è®­ç»ƒã€‚å¦‚æžœæ˜¯loraè°ƒçš„è¯ï¼Œé‚£ä¹ˆå¯è®­ç»ƒå‚æ•°åº”è¯¥å æ€»é‡çš„æ¯”ä¾‹éžå¸¸å°‘ï¼Ÿ
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if local_rank != 0:
            return

        print("\n" + "="*80)
        print("TRAINABLE PARAMETERS MONITORING")
        print("="*80)

        # ç»Ÿè®¡ä¿¡æ¯
        total_params = 0
        trainable_params = 0
        frozen_params = 0

        # ç‰¹åˆ«å…³æ³¨çš„æ¨¡å—
        patch_embedding_params = {}
        head_params = {}
        mv_module_params = {}  # æ–°å¢žï¼šå¤šè§†è§’æ¨¡å—å‚æ•°
        lora_params = {}
        other_trainable_params = {}

        # éåŽ†æ‰€æœ‰å‚æ•°
        for name, param in self.pipe.dit.named_parameters():
            total_params += param.numel()

            if param.requires_grad:
                trainable_params += param.numel()

                # åˆ†ç±»å‚æ•°
                if 'patch_embedding' in name:
                    patch_embedding_params[name] = {
                        'shape': tuple(param.shape),
                        'numel': param.numel(),
                        'dtype': param.dtype
                    }
                elif 'head' in name:
                    head_params[name] = {
                        'shape': tuple(param.shape),
                        'numel': param.numel(),
                        'dtype': param.dtype
                    }
                # æ–°å¢žï¼šæ£€æŸ¥æ˜¯å¦æ˜¯MVæ¨¡å—å‚æ•°
                elif any(['projector' in name and 'blocks.' in name, 'norm_mvs' in name,
                         'modulation_mvs' in name, 'mvs_attn' in name]):
                    mv_module_params[name] = {
                        'shape': tuple(param.shape),
                        'numel': param.numel(),
                        'dtype': param.dtype
                    }
                elif 'lora' in name.lower():
                    lora_params[name] = {
                        'shape': tuple(param.shape),
                        'numel': param.numel(),
                        'dtype': param.dtype
                    }
                else:
                    other_trainable_params[name] = {
                        'shape': tuple(param.shape),
                        'numel': param.numel(),
                        'dtype': param.dtype
                    }
            else:
                frozen_params += param.numel()

        # æ‰“å°æ€»è§ˆ
        print(f"\nðŸ“Š Parameter Overview:")
        print(f"  Total parameters:     {total_params:,} ({total_params/1e6:.2f}M)")
        print(f"  Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M, {trainable_params/total_params*100:.2f}%)")
        print(f"  Frozen parameters:    {frozen_params:,} ({frozen_params/1e6:.2f}M, {frozen_params/total_params*100:.2f}%)")

        # æ‰“å° patch_embedding å‚æ•°
        print(f"\nðŸ” PATCH_EMBEDDING Parameters:")
        if patch_embedding_params:
            print(f"  âœ… {len(patch_embedding_params)} trainable parameter(s) found:")
            for name, info in patch_embedding_params.items():
                print(f"    - {name}")
                print(f"      Shape: {info['shape']}, Count: {info['numel']:,}, Dtype: {info['dtype']}")
        else:
            print(f"  âŒ NO trainable parameters found in patch_embedding!")
            print(f"     â†’ patch_embedding parameters are FROZEN")

        # æ‰“å° head å‚æ•°
        print(f"\nðŸ” HEAD Parameters:")
        if head_params:
            print(f"  âœ… {len(head_params)} trainable parameter(s) found:")
            for name, info in head_params.items():
                print(f"    - {name}")
                print(f"      Shape: {info['shape']}, Count: {info['numel']:,}, Dtype: {info['dtype']}")
        else:
            print(f"  âŒ NO trainable parameters found in head!")
            print(f"     â†’ head parameters are FROZEN")

        # æ‰“å° MV æ¨¡å—å‚æ•°ï¼ˆä»…åœ¨MVæ¨¡å¼ä¸‹ï¼‰
        if self.wan_type in ["5B_TI2V_RGB_HEATMAP_MV", "5B_TI2V_RGB_HEATMAP_MV_HISTORY"]:
            print(f"\nðŸ” MULTI-VIEW MODULE Parameters:")
            if mv_module_params:
                mv_param_count = sum(info['numel'] for info in mv_module_params.values())
                print(f"  âœ… {len(mv_module_params)} trainable parameter(s) found ({mv_param_count:,} total, {mv_param_count/1e6:.2f}M):")

                # æŒ‰æ¨¡å—ç±»åž‹ç»Ÿè®¡
                projector_count = sum(info['numel'] for name, info in mv_module_params.items() if 'projector' in name)
                norm_mvs_count = sum(info['numel'] for name, info in mv_module_params.items() if 'norm_mvs' in name)
                modulation_mvs_count = sum(info['numel'] for name, info in mv_module_params.items() if 'modulation_mvs' in name)
                mvs_attn_count = sum(info['numel'] for name, info in mv_module_params.items() if 'mvs_attn' in name)

                print(f"    ðŸ“¦ Projector: {projector_count:,} params ({projector_count/1e6:.2f}M)")
                print(f"    ðŸ“¦ Norm_mvs: {norm_mvs_count:,} params ({norm_mvs_count/1e6:.2f}M)")
                print(f"    ðŸ“¦ Modulation_mvs: {modulation_mvs_count:,} params ({modulation_mvs_count/1e6:.2f}M)")
                print(f"    ðŸ“¦ MVS_Attn: {mvs_attn_count:,} params ({mvs_attn_count/1e6:.2f}M)")

                # æ˜¾ç¤ºå‰3ä¸ªå‚æ•°ç¤ºä¾‹
                print(f"\n  Sample parameters:")
                for i, (name, info) in enumerate(list(mv_module_params.items())[:3]):
                    print(f"    - {name}")
                    print(f"      Shape: {info['shape']}, Count: {info['numel']:,}")
                if len(mv_module_params) > 3:
                    print(f"    ... and {len(mv_module_params) - 3} more MV parameters")
            else:
                print(f"  âŒ NO trainable parameters found in MV modules!")
                print(f"     â†’ MV module parameters are FROZEN or not initialized")

        # æ‰“å° LoRA å‚æ•°
        print(f"\nðŸ” LoRA Parameters:")
        if lora_params:
            lora_param_count = sum(info['numel'] for info in lora_params.values())
            print(f"  âœ… {len(lora_params)} LoRA parameter(s) found ({lora_param_count:,} total, {lora_param_count/1e6:.2f}M):")
            # åªæ˜¾ç¤ºå‰5ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡é•¿
            for i, (name, info) in enumerate(list(lora_params.items())[:5]):
                print(f"    - {name}")
                print(f"      Shape: {info['shape']}, Count: {info['numel']:,}")
            if len(lora_params) > 5:
                print(f"    ... and {len(lora_params) - 5} more LoRA parameters")
        else:
            print(f"  âš ï¸  NO LoRA parameters found!")

        # æ‰“å°å…¶ä»–å¯è®­ç»ƒå‚æ•°
        if other_trainable_params:
            other_param_count = sum(info['numel'] for info in other_trainable_params.values())
            print(f"\nðŸ“‹ Other Trainable Parameters:")
            print(f"  {len(other_trainable_params)} parameter(s) ({other_param_count:,} total, {other_param_count/1e6:.2f}M)")
            # åªæ˜¾ç¤ºå‰3ä¸ª
            for i, (name, info) in enumerate(list(other_trainable_params.items())[:3]):
                print(f"    - {name}: {info['shape']}")
            if len(other_trainable_params) > 3:
                print(f"    ... and {len(other_trainable_params) - 3} more")

        # é‡è¦æç¤º
        print(f"\nðŸ’¡ Important Notes:")
        if not patch_embedding_params:
            print(f"  âš ï¸  WARNING: patch_embedding is NOT being trained!")
            print(f"     If you modified in_dim, you should add 'patch_embedding' to lora_target_modules")
        if not head_params:
            print(f"  âš ï¸  WARNING: head is NOT being trained!")
            print(f"     If you modified out_dim, you should add 'head.head' to lora_target_modules")

        print("="*80 + "\n")

    def _save_initial_parameters(self):
        """
        ä¿å­˜ patch_embedding å’Œ head.head çš„åˆå§‹å‚æ•°å€¼
        ç”¨äºŽåŽç»­æ£€æµ‹è¿™äº›å‚æ•°æ˜¯å¦çœŸçš„åœ¨è®­ç»ƒä¸­æ›´æ–°
        """
        self.initial_params = {}

        for name, param in self.pipe.dit.named_parameters():
            if ('patch_embedding' in name or 'head' in name) and param.requires_grad:
                # ä¿å­˜å‚æ•°çš„å…‹éš†å‰¯æœ¬ï¼ˆdetachä»¥é¿å…å½±å“æ¢¯åº¦è®¡ç®—ï¼‰
                self.initial_params[name] = param.data.clone().detach()

        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if local_rank == 0 and len(self.initial_params) > 0:
            weight_count = sum(1 for name in self.initial_params.keys() if 'weight' in name or 'lora' in name)
            bias_count = sum(1 for name in self.initial_params.keys() if 'bias' in name)
            print(f"ðŸ“¸ Saved initial parameters for {len(self.initial_params)} parameters")
            print(f"   - Weight/LoRA parameters: {weight_count}")
            print(f"   - Bias parameters: {bias_count}")
            print(f"   (patch_embedding and head parameters will be monitored for updates)")

    def check_parameter_updates(self, step):
        """
        æ£€æŸ¥ patch_embedding å’Œ head.head çš„å‚æ•°æ˜¯å¦å·²æ›´æ–°
        åªåœ¨ä¸»è¿›ç¨‹æ‰“å°

        Args:
            step: å½“å‰è®­ç»ƒæ­¥æ•°
        """
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if local_rank != 0:
            return

        if not hasattr(self, 'initial_params') or len(self.initial_params) == 0:
            return

        print(f"\nðŸ” Checking parameter updates at step {step}:")
        print("="*80)

        updated_count = 0
        unchanged_count = 0
        bias_updated_count = 0
        bias_unchanged_count = 0

        # èŽ·å–å½“å‰å‚æ•°å­—å…¸ï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼Œæé«˜æ•ˆçŽ‡ï¼‰
        current_params_dict = dict(self.pipe.dit.named_parameters())

        # åˆ†åˆ«å¤„ç† weight/LoRA å‚æ•°å’Œ bias å‚æ•°
        weight_params = {name: val for name, val in self.initial_params.items() if 'bias' not in name}
        bias_params = {name: val for name, val in self.initial_params.items() if 'bias' in name}

        # æ£€æŸ¥ weight/LoRA å‚æ•°
        if weight_params:
            print("\nðŸ“Š Weight/LoRA Parameters:")
            for name, initial_value in weight_params.items():
                # èŽ·å–å½“å‰å‚æ•°å€¼
                if name not in current_params_dict:
                    print(f"  âš ï¸  WARNING: {name} not found in current model parameters!")
                    continue

                current_value = current_params_dict[name].data

                # ç¡®ä¿åˆå§‹å€¼å’Œå½“å‰å€¼åœ¨åŒä¸€è®¾å¤‡ä¸Šè¿›è¡Œæ¯”è¾ƒ
                if initial_value.device != current_value.device:
                    initial_value = initial_value.to(current_value.device)

                # è®¡ç®—å‚æ•°å˜åŒ–çš„ç¨‹åº¦
                diff = (current_value - initial_value).abs()
                max_diff = diff.max().item()
                mean_diff = diff.mean().item()

                # åˆ¤æ–­å‚æ•°æ˜¯å¦æ›´æ–°ï¼ˆä½¿ç”¨è¾ƒå°çš„é˜ˆå€¼ï¼Œå› ä¸ºLoRAæ›´æ–°å¯èƒ½å¾ˆå°ï¼‰
                is_updated = max_diff > 1e-8

                if is_updated:
                    updated_count += 1
                    status = "âœ… UPDATED"
                    print(f"  {status}: {name}")
                    print(f"    Max diff: {max_diff:.2e}, Mean diff: {mean_diff:.2e}")
                else:
                    unchanged_count += 1
                    status = "âŒ UNCHANGED"
                    print(f"  {status}: {name}")
                    print(f"    Max diff: {max_diff:.2e}, Mean diff: {mean_diff:.2e}")

        # æ£€æŸ¥ bias å‚æ•°
        if bias_params:
            print("\nðŸ“Š Bias Parameters:")
            for name, initial_value in bias_params.items():
                # èŽ·å–å½“å‰å‚æ•°å€¼
                if name not in current_params_dict:
                    print(f"  âš ï¸  WARNING: {name} not found in current model parameters!")
                    continue

                current_value = current_params_dict[name].data

                # ç¡®ä¿åˆå§‹å€¼å’Œå½“å‰å€¼åœ¨åŒä¸€è®¾å¤‡ä¸Šè¿›è¡Œæ¯”è¾ƒ
                if initial_value.device != current_value.device:
                    initial_value = initial_value.to(current_value.device)

                # è®¡ç®—å‚æ•°å˜åŒ–çš„ç¨‹åº¦
                diff = (current_value - initial_value).abs()
                max_diff = diff.max().item()
                mean_diff = diff.mean().item()

                # åˆ¤æ–­å‚æ•°æ˜¯å¦æ›´æ–°ï¼ˆä½¿ç”¨è¾ƒå°çš„é˜ˆå€¼ï¼‰
                is_updated = max_diff > 1e-8

                if is_updated:
                    bias_updated_count += 1
                    status = "âœ… UPDATED"
                    print(f"  {status}: {name}")
                    print(f"    Max diff: {max_diff:.2e}, Mean diff: {mean_diff:.2e}")
                else:
                    bias_unchanged_count += 1
                    status = "âŒ UNCHANGED"
                    print(f"  {status}: {name}")
                    print(f"    Max diff: {max_diff:.2e}, Mean diff: {mean_diff:.2e}")

        print("="*80)
        print("Summary:")
        print(f"  Weight/LoRA: {updated_count} updated, {unchanged_count} unchanged")
        print(f"  Bias: {bias_updated_count} updated, {bias_unchanged_count} unchanged")
        print(f"  Total: {updated_count + bias_updated_count} updated, {unchanged_count + bias_unchanged_count} unchanged")

        if unchanged_count > 0:
            print(f"âš ï¸  WARNING: {unchanged_count} parameter(s) have NOT been updated!")
            print(f"   This may indicate that these parameters are not in the optimizer")

        print("="*80 + "\n")

    def forward_preprocess(self, data):
        """
        é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œä¸“é—¨é’ˆå¯¹çƒ­åŠ›å›¾æ•°æ®æ ¼å¼
        æ³¨æ„ï¼šWanæ¨¡åž‹åªæ”¯æŒbatch_size=1
        """
        # CFG-sensitive parameters
        inputs_posi = {"prompt": data["prompt"]}
        inputs_nega = {}

        # CFG-unsensitive parameters
        inputs_shared = {
            # çƒ­åŠ›å›¾åºåˆ—ä½œä¸ºè§†é¢‘è¾“å…¥
            "input_video": data["video"], # ( T,num_view) List of PIL list
            "height": data["video"][0][0].size[1], # æ³¨æ„åŒºåˆ†é«˜å’Œå®½
            "width": data["video"][0][0].size[0],
            "num_frames": len(data["video"]),
            # è®­ç»ƒç›¸å…³å‚æ•°
            "cfg_scale": 1,
            "tiled": False,
            "rand_device": self.pipe.device,
            "use_gradient_checkpointing": self.use_gradient_checkpointing,
            "use_gradient_checkpointing_offload": self.use_gradient_checkpointing_offload,
            "cfg_merge": False,
            "vace_scale": 1,
            "max_timestep_boundary": self.max_timestep_boundary,
            "min_timestep_boundary": self.min_timestep_boundary,
            "use_dual_head":self.use_dual_head,
            # View concatenation mode: num_view = num_heatmap_views + num_rgb_views
            # When we have both RGB and Heatmap: num_view = 3 + 3 = 6
            # When we only have Heatmap: num_view = 3
            "num_view": len(data["video"][0]) + (len(data.get("input_video_rgb", [[]])[0]) if "input_video_rgb" in self.extra_inputs else 0),
            "num_history_frames": self.num_history_frames,  # åŽ†å²å¸§æ•°é‡
        }

        # Extra inputs - å¯¹äºŽçƒ­åŠ›å›¾ä»»åŠ¡ï¼Œä¸»è¦æ˜¯input_image
        '''
        # check only the follow conditions for self.extra_inputs:
        (input_image)
        (input_image,condition_rgb)
        (input_image,input_image_rgb,input_video_rgb)
        '''
        # check self.extra_inputs satisfy the above three conditions
        assert self.extra_inputs == ["input_image"] or \
               self.extra_inputs == ["input_image", "condition_rgb"] or \
               self.extra_inputs == ["input_image", "input_image_rgb", "input_video_rgb"]

        for extra_input in self.extra_inputs:
            if extra_input == "input_image":
                # ä½¿ç”¨é¦–å¸§RGBå›¾åƒ/çƒ­åŠ›å›¾ä½œä¸ºæ¡ä»¶è¾“å…¥
                inputs_shared["input_image"] = data["input_image"]
            elif extra_input == "end_image":
                inputs_shared["end_image"] = data["video"][-1]
            elif extra_input == "reference_image" or extra_input == "vace_reference_image":
                inputs_shared[extra_input] = data[extra_input][0]
            elif extra_input == "condition_rgb":
                # å°†RGBä½œä¸ºé¢å¤–çš„æ¡ä»¶è¾“å…¥
                inputs_shared["condition_rgb"] = data["condition_rgb"]
            elif extra_input == "input_image_rgb":
                inputs_shared["input_image_rgb"] = data["input_image_rgb"]
            elif extra_input == "input_video_rgb":
                inputs_shared["input_video_rgb"] = data["input_video_rgb"]
            else:
                if extra_input in data:
                    inputs_shared[extra_input] = data[extra_input]

        # å¤šå¸§åŽ†å²æ”¯æŒï¼šæ·»åŠ  input_images å’Œ input_images_rgb
        # è¿™äº›å­—æ®µåœ¨å¤šåŽ†å²å¸§æ¨¡å¼ä¸‹ç”±æ•°æ®å‡†å¤‡å‡½æ•°ç”Ÿæˆ
        if "input_images" in data:
            inputs_shared["input_images"] = data["input_images"]
        if "input_images_rgb" in data:
            inputs_shared["input_images_rgb"] = data["input_images_rgb"]

        # Pipeline units will automatically process the input parameters.
        for unit in self.pipe.units:
            inputs_shared, inputs_posi, inputs_nega = self.pipe.unit_runner(unit, self.pipe, inputs_shared, inputs_posi, inputs_nega)
        return {**inputs_shared, **inputs_posi}


    def visualize_forward_inputs(self, inputs):
        """
        å¯è§†åŒ– forward å‰çš„è¾“å…¥æ•°æ®ï¼ˆæ”¯æŒå¤šè§†è§’ï¼‰
        åŒ…æ‹¬: input_image, input_image_rgb, input_video, input_video_rgb

        æ•°æ®ç»“æž„ï¼š
        - input_image: åˆ—è¡¨ï¼ŒåŒ…å«Nä¸ªè§†è§’çš„PILå›¾åƒ [view1, view2, ...]
        - input_image_rgb: åˆ—è¡¨ï¼ŒåŒ…å«Nä¸ªè§†è§’çš„PILå›¾åƒ [view1, view2, ...]
        - input_video: åŒå±‚åˆ—è¡¨ [time][view]ï¼Œæ¯ä¸ªæ—¶é—´æ­¥åŒ…å«å¤šä¸ªè§†è§’
        - input_video_rgb: åŒå±‚åˆ—è¡¨ [time][view]ï¼Œæ¯ä¸ªæ—¶é—´æ­¥åŒ…å«å¤šä¸ªè§†è§’

        åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ
        """
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if local_rank != 0:
            return

        try:
            # ç¡®ä¿debugç›®å½•å­˜åœ¨
            os.makedirs(self.debug_save_dir, exist_ok=True)

            print(f"\n{'='*80}")
            print(f"FORWARD INPUT VISUALIZATION - MULTI-VIEW (Step: {self.debug_counter})")
            print(f"{'='*80}")
            print(f"Available keys: {list(inputs.keys())}")

            # æ”¶é›†æ‰€æœ‰è¦å¯è§†åŒ–çš„æ•°æ®
            input_image = inputs.get('input_image')
            input_image_rgb = inputs.get('input_image_rgb')
            input_video = inputs.get('input_video')
            input_video_rgb = inputs.get('input_video_rgb')

            # æ£€æµ‹æ•°æ®ç»“æž„
            num_views = 0
            num_frames = 0

            # æ£€æµ‹è§†è§’æ•°é‡
            if input_image and isinstance(input_image, list):
                num_views = len(input_image)
                print(f"âœ“ input_image: {num_views} views")
            elif input_image_rgb and isinstance(input_image_rgb, list):
                num_views = len(input_image_rgb)
                print(f"âœ“ input_image_rgb: {num_views} views")

            # æ£€æµ‹å¸§æ•°
            if input_video and isinstance(input_video, list) and len(input_video) > 0:
                num_frames = len(input_video)
                if isinstance(input_video[0], list):
                    num_views = max(num_views, len(input_video[0]))
                    print(f"âœ“ input_video: {num_frames} frames Ã— {len(input_video[0])} views")
                else:
                    print(f"âœ“ input_video: {num_frames} frames (single view)")

            if input_video_rgb and isinstance(input_video_rgb, list) and len(input_video_rgb) > 0:
                if isinstance(input_video_rgb[0], list):
                    print(f"âœ“ input_video_rgb: {len(input_video_rgb)} frames Ã— {len(input_video_rgb[0])} views")
                else:
                    print(f"âœ“ input_video_rgb: {len(input_video_rgb)} frames (single view)")

            # è®¡ç®—å¸ƒå±€
            # æ¯ä¸ªsectionçš„åˆ—æ•° = max(num_views, 3)
            cols = max(num_views, 3)

            # åŠ¨æ€è®¡ç®—è¡Œæ•°
            rows_list = []

            # ç¬¬1è¡Œï¼šæ ‡é¢˜
            rows_list.append(('title_img', 0.3))

            # ç¬¬2-3è¡Œï¼šinput_image å’Œ input_image_rgb
            if input_image or input_image_rgb:
                rows_list.append(('input_image', 1.5))
                rows_list.append(('input_image_rgb', 1.5))

            # ç¬¬4è¡Œï¼šè§†é¢‘æ ‡é¢˜
            if input_video or input_video_rgb:
                rows_list.append(('title_video', 0.3))

            # ç¬¬5-Nè¡Œï¼šinput_video (æ¯å¸§ä¸€è¡Œ)
            if input_video and isinstance(input_video, list) and len(input_video) > 0:
                for _ in range(min(num_frames, 15)):  # æœ€å¤šæ˜¾ç¤º4å¸§
                    rows_list.append(('video_frame', 1.2))

            # ç¬¬Mè¡Œï¼šRGBè§†é¢‘æ ‡é¢˜
            if input_video_rgb:
                rows_list.append(('title_video_rgb', 0.3))

            # ç¬¬M+1-Kè¡Œï¼šinput_video_rgb (æ¯å¸§ä¸€è¡Œ)
            if input_video_rgb and isinstance(input_video_rgb, list) and len(input_video_rgb) > 0:
                for _ in range(min(len(input_video_rgb), 15)):  # æœ€å¤šæ˜¾ç¤º4å¸§
                    rows_list.append(('video_rgb_frame', 1.2))

            # åˆ›å»ºfigure
            total_rows = len(rows_list)
            height_ratios = [r[1] for r in rows_list]
            fig_height = sum(height_ratios) * 2.5

            fig = plt.figure(figsize=(2.5*cols, fig_height))
            gs = fig.add_gridspec(total_rows, cols, hspace=0.3, wspace=0.1, height_ratios=height_ratios)

            current_row = 0

            # ============ æ ‡é¢˜ï¼šInput Images ============
            if rows_list[current_row][0] == 'title_img':
                ax_title = fig.add_subplot(gs[current_row, :])
                ax_title.text(0.5, 0.5, 'Input Images (Multi-View)', ha='center', va='center',
                             fontsize=14, fontweight='bold', transform=ax_title.transAxes,
                             bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.7))
                ax_title.axis('off')
                current_row += 1

            # ============ input_image (å¤šè§†è§’) ============
            if current_row < len(rows_list) and rows_list[current_row][0] == 'input_image':
                if input_image and isinstance(input_image, list):
                    for i, img in enumerate(input_image):
                        if i < cols:
                            ax = fig.add_subplot(gs[current_row, i])
                            if hasattr(img, 'save'):
                                img_array = np.array(img)
                                ax.imshow(img_array)
                                ax.set_title(f'Heatmap View {i}', fontsize=10, fontweight='bold')
                                if i == 0:
                                    print(f"  View {i}: Size {img.size}, Mode {img.mode}")
                            else:
                                ax.text(0.5, 0.5, f'View {i}\nInvalid', ha='center', va='center',
                                       fontsize=9, transform=ax.transAxes)
                            ax.axis('off')
                    # å¡«å……ç©ºç™½
                    for i in range(len(input_image), cols):
                        ax = fig.add_subplot(gs[current_row, i])
                        ax.axis('off')
                else:
                    ax = fig.add_subplot(gs[current_row, :])
                    ax.text(0.5, 0.5, 'No input_image', ha='center', va='center',
                           fontsize=10, transform=ax.transAxes)
                    ax.axis('off')
                current_row += 1

            # ============ input_image_rgb (å¤šè§†è§’) ============
            if current_row < len(rows_list) and rows_list[current_row][0] == 'input_image_rgb':
                if input_image_rgb and isinstance(input_image_rgb, list):
                    for i, img in enumerate(input_image_rgb):
                        if i < cols:
                            ax = fig.add_subplot(gs[current_row, i])
                            if hasattr(img, 'save'):
                                img_array = np.array(img)
                                ax.imshow(img_array)
                                ax.set_title(f'RGB View {i}', fontsize=10, fontweight='bold')
                                if i == 0:
                                    print(f"  View {i}: Size {img.size}, Mode {img.mode}")
                            else:
                                ax.text(0.5, 0.5, f'View {i}\nInvalid', ha='center', va='center',
                                       fontsize=9, transform=ax.transAxes)
                            ax.axis('off')
                    # å¡«å……ç©ºç™½
                    for i in range(len(input_image_rgb), cols):
                        ax = fig.add_subplot(gs[current_row, i])
                        ax.axis('off')
                else:
                    ax = fig.add_subplot(gs[current_row, :])
                    ax.text(0.5, 0.5, 'No input_image_rgb', ha='center', va='center',
                           fontsize=10, transform=ax.transAxes)
                    ax.axis('off')
                current_row += 1

            # ============ è§†é¢‘æ ‡é¢˜ ============
            if current_row < len(rows_list) and rows_list[current_row][0] == 'title_video':
                ax_title = fig.add_subplot(gs[current_row, :])
                ax_title.text(0.5, 0.5, 'Input Video (Heatmap Sequence, Multi-View)', ha='center', va='center',
                             fontsize=14, fontweight='bold', transform=ax_title.transAxes,
                             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
                ax_title.axis('off')
                current_row += 1

            # ============ input_video (å¤šè§†è§’Ã—å¤šå¸§) ============
            if input_video and isinstance(input_video, list) and len(input_video) > 0:
                frames_to_show = min(len(input_video), 15)
                for frame_idx in range(frames_to_show):
                    if current_row < len(rows_list) and rows_list[current_row][0] == 'video_frame':
                        frame_data = input_video[frame_idx]

                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè§†è§’
                        if isinstance(frame_data, list):
                            # å¤šè§†è§’æ¨¡å¼
                            for view_idx, img in enumerate(frame_data):
                                if view_idx < cols:
                                    ax = fig.add_subplot(gs[current_row, view_idx])
                                    if hasattr(img, 'save'):
                                        img_array = np.array(img)
                                        ax.imshow(img_array)
                                        ax.set_title(f'T{frame_idx}V{view_idx}', fontsize=8)
                                    ax.axis('off')
                            # å¡«å……ç©ºç™½
                            for view_idx in range(len(frame_data), cols):
                                ax = fig.add_subplot(gs[current_row, view_idx])
                                ax.axis('off')
                        else:
                            # å•è§†è§’æ¨¡å¼
                            ax = fig.add_subplot(gs[current_row, 0])
                            if hasattr(frame_data, 'save'):
                                img_array = np.array(frame_data)
                                ax.imshow(img_array)
                                ax.set_title(f'Frame {frame_idx}', fontsize=9)
                            ax.axis('off')
                            # å¡«å……å‰©ä½™åˆ—
                            for i in range(1, cols):
                                ax = fig.add_subplot(gs[current_row, i])
                                ax.axis('off')

                        current_row += 1

            # ============ RGBè§†é¢‘æ ‡é¢˜ ============
            if current_row < len(rows_list) and rows_list[current_row][0] == 'title_video_rgb':
                ax_title = fig.add_subplot(gs[current_row, :])
                ax_title.text(0.5, 0.5, 'Input Video RGB (RGB Sequence, Multi-View)', ha='center', va='center',
                             fontsize=14, fontweight='bold', transform=ax_title.transAxes,
                             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))
                ax_title.axis('off')
                current_row += 1

            # ============ input_video_rgb (å¤šè§†è§’Ã—å¤šå¸§) ============
            if input_video_rgb and isinstance(input_video_rgb, list) and len(input_video_rgb) > 0:
                frames_to_show = min(len(input_video_rgb), 15)
                for frame_idx in range(frames_to_show):
                    if current_row < len(rows_list) and rows_list[current_row][0] == 'video_rgb_frame':
                        frame_data = input_video_rgb[frame_idx]

                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè§†è§’
                        if isinstance(frame_data, list):
                            # å¤šè§†è§’æ¨¡å¼
                            for view_idx, img in enumerate(frame_data):
                                if view_idx < cols:
                                    ax = fig.add_subplot(gs[current_row, view_idx])
                                    if hasattr(img, 'save'):
                                        img_array = np.array(img)
                                        ax.imshow(img_array)
                                        ax.set_title(f'T{frame_idx}V{view_idx}', fontsize=8)
                                    ax.axis('off')
                            # å¡«å……ç©ºç™½
                            for view_idx in range(len(frame_data), cols):
                                ax = fig.add_subplot(gs[current_row, view_idx])
                                ax.axis('off')
                        else:
                            # å•è§†è§’æ¨¡å¼
                            ax = fig.add_subplot(gs[current_row, 0])
                            if hasattr(frame_data, 'save'):
                                img_array = np.array(frame_data)
                                ax.imshow(img_array)
                                ax.set_title(f'Frame {frame_idx}', fontsize=9)
                            ax.axis('off')
                            # å¡«å……å‰©ä½™åˆ—
                            for i in range(1, cols):
                                ax = fig.add_subplot(gs[current_row, i])
                                ax.axis('off')

                        current_row += 1

            # ä¿å­˜ç»„åˆå›¾
            save_path = os.path.join(self.debug_save_dir, f"step_{self.debug_counter:04d}_all_inputs_multiview.png")
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()

            print("\nâœ… Multi-view inputs visualization saved to:")
            print(f"   {save_path}")

            # æ‰“å°å…¶ä»–å…³é”®ä¿¡æ¯
            print("\nOther inputs:")
            for key in ['prompt', 'height', 'width', 'num_frames', 'num_view']:
                if key in inputs:
                    print(f"  {key}: {inputs[key]}")

            print(f"{'='*80}\n")

        except Exception as e:
            print(f"âŒ Error in forward input visualization: {e}")
            import traceback
            traceback.print_exc()

    def visualize_processed_inputs(self, inputs, data):
        """
        å¯è§†åŒ–ç»è¿‡forward_preprocesså¤„ç†åŽçš„input_videoå’Œinput_image
        """
        try:
            # ç¡®ä¿debugç›®å½•å­˜åœ¨
            os.makedirs(self.debug_save_dir, exist_ok=True)

            print(f"\n=== DEBUG VISUALIZATION (Counter: {self.debug_counter}) ===")
            print(f"Original prompt: {data.get('prompt', 'N/A')}")
            print(f"Processed inputs keys: {list(inputs.keys())}")

            # å¯è§†åŒ–processed input_image
            if 'input_image' in inputs and inputs['input_image'] is not None:
                input_img = inputs['input_image']
                if hasattr(input_img, 'save'):  # PIL Image
                    input_img_path = os.path.join(self.debug_save_dir, f"debug_{self.debug_counter:04d}_processed_input_image.png")
                    input_img.save(input_img_path)
                    print(f"Processed input image saved: {input_img_path}")
                    print(f"Processed input image size: {input_img.size}")
                else:
                    print(f"Input image type: {type(input_img)}, shape: {getattr(input_img, 'shape', 'N/A')}")

            # å¯è§†åŒ–processed input_video
            if 'input_video' in inputs and inputs['input_video'] is not None:
                input_video = inputs['input_video']

                if isinstance(input_video, list) and len(input_video) > 0:
                    # å¦‚æžœæ˜¯PILå›¾åƒåˆ—è¡¨
                    video_frames = input_video
                    num_frames = len(video_frames)

                    print(f"Processed video frames count: {num_frames}")

                    # åˆ›å»ºç½‘æ ¼æ˜¾ç¤ºæ‰€æœ‰å¸§
                    cols = min(5, num_frames)  # æœ€å¤š5åˆ—
                    rows = (num_frames + cols - 1) // cols

                    fig, axes = plt.subplots(rows, cols, figsize=(15, 3*rows))
                    if rows == 1 and cols == 1:
                        axes = [axes]
                    elif rows == 1 or cols == 1:
                        axes = axes.flatten()
                    else:
                        axes = axes.flatten()

                    for i, frame in enumerate(video_frames):
                        if i < len(axes):
                            # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
                            if hasattr(frame, 'save'):  # PIL Image
                                frame_array = np.array(frame)
                                axes[i].imshow(frame_array)
                                axes[i].set_title(f"Processed Frame {i}")
                                axes[i].axis('off')

                                # åŒæ—¶ä¿å­˜å•ç‹¬çš„å¸§
                                frame_path = os.path.join(self.debug_save_dir, f"debug_{self.debug_counter:04d}_processed_frame_{i:02d}.png")
                                frame.save(frame_path)
                            else:
                                axes[i].text(0.5, 0.5, f"Frame {i}\n{type(frame)}",
                                           ha='center', va='center', transform=axes[i].transAxes)
                                axes[i].axis('off')

                    # éšè—å¤šä½™çš„å­å›¾
                    for i in range(num_frames, len(axes)):
                        axes[i].axis('off')

                    # ä¿å­˜ç»„åˆå›¾
                    combined_path = os.path.join(self.debug_save_dir, f"debug_{self.debug_counter:04d}_processed_video_sequence.png")
                    plt.tight_layout()
                    plt.savefig(combined_path, dpi=150, bbox_inches='tight')
                    plt.close()

                    print(f"Processed video sequence saved: {combined_path}")
                    print(f"Individual processed frames saved with pattern: debug_{self.debug_counter:04d}_processed_frame_XX.png")

                    if len(video_frames) > 0 and hasattr(video_frames[0], 'size'):
                        print(f"Processed video info: {num_frames} frames, size: {video_frames[0].size}")
                else:
                    print(f"Input video type: {type(input_video)}, shape: {getattr(input_video, 'shape', 'N/A')}")

            # æ‰“å°å…¶ä»–é‡è¦çš„inputsä¿¡æ¯
            for key, value in inputs.items():
                if key not in ['input_image', 'input_video']:
                    if isinstance(value, (int, float, str, bool)):
                        print(f"  {key}: {value}")
                    elif hasattr(value, 'shape'):
                        print(f"  {key}: shape {value.shape}, dtype {getattr(value, 'dtype', 'N/A')}")
                    elif isinstance(value, (list, tuple)):
                        print(f"  {key}: {type(value)} of length {len(value)}")
                    else:
                        print(f"  {key}: {type(value)}")

            print(f"=== END DEBUG VISUALIZATION ===")

        except Exception as e:
            print(f"Error in debug visualization: {e}")
            import traceback
            traceback.print_exc()

    def forward(self, data, inputs=None):
        """
        å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æŸå¤±
        """
        # é¢„å¤„ç†
        if inputs is None:
            inputs = self.forward_preprocess(data)

        # å¼ºåˆ¶è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®å¤
        import os
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        expected_device = f"cuda:{local_rank}"

        # ä¿®å¤ rand_device è®¾ç½®
        if "rand_device" in inputs:
            inputs["rand_device"] = expected_device

        # æ£€æŸ¥å¹¶ä¿®å¤æ‰€æœ‰è¾“å…¥å¼ é‡çš„è®¾å¤‡
        def move_to_device(obj, device):
            if hasattr(obj, 'to') and hasattr(obj, 'device'):
                return obj.to(device)
            elif isinstance(obj, (list, tuple)):
                return type(obj)(move_to_device(item, device) for item in obj)
            elif isinstance(obj, dict):
                return {key: move_to_device(value, device) for key, value in obj.items()}
            else:
                return obj

        try:
            # å°†æ‰€æœ‰è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            inputs = move_to_device(inputs, expected_device)

            # åŒæ—¶ç¡®ä¿æ•°æ®ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            data = move_to_device(data, expected_device)
        except Exception as device_move_error:
            print(f"Warning: Could not move inputs to device {expected_device}: {device_move_error}")

        # HARDCODED DEBUGåˆ†æ”¯ - æ‰‹åŠ¨è®¾ç½®ä¸ºTrueæ—¶å¯ç”¨å¯è§†åŒ–
        DEBUG_VISUALIZATION = False # æ‰‹åŠ¨ä¿®æ”¹æ­¤å¤„ä¸ºTrueæ¥å¯ç”¨debugå¯è§†åŒ–
        # import pdb;pdb.set_trace()
        if DEBUG_VISUALIZATION:
            print(f"\nðŸ” DEBUG MODE ACTIVATED (Step {self.debug_counter})")
            # å¯è§†åŒ– forward å‰çš„è¾“å…¥
            self.visualize_forward_inputs(inputs)
            import pdb;pdb.set_trace()
            # å¯è§†åŒ–ç»è¿‡å¤„ç†åŽçš„è¾“å…¥ï¼ˆä¿ç•™åŽŸæœ‰åŠŸèƒ½ï¼‰
            # self.visualize_processed_inputs(inputs, data)
            self.debug_counter += 1

        # å‰å‘ä¼ æ’­è®¡ç®—

        # æ­£å¸¸å‰å‘ä¼ æ’­
        models = {name: getattr(self.pipe, name) for name in self.pipe.in_iteration_models}
        loss = self.pipe.training_loss(**models, **inputs, rgb_loss_weight=self.rgb_loss_weight)
        return loss

    def log_to_swanlab(self, loss_value, step, swanlab_run=None):
        """
        è®°å½•æŸå¤±åˆ°SwanLab
        """
        if swanlab_run is not None and SWANLAB_AVAILABLE:
            try:
                # swanlab å·²ç»åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œä¸éœ€è¦é‡å¤å¯¼å…¥
                # åªè®°å½•æœ‰æ•ˆçš„æ•°å€¼ï¼Œé¿å…Noneå€¼å¯¼è‡´çš„é”™è¯¯
                log_data = {
                    "train_loss": loss_value,
                    "step": step
                }

                # å¦‚æžœæœ‰å­¦ä¹ çŽ‡æ–¹æ³•ä¸”è¿”å›žæœ‰æ•ˆå€¼ï¼Œæ‰è®°å½•å­¦ä¹ çŽ‡
                if hasattr(self, 'get_current_lr'):
                    lr = self.get_current_lr()
                    if lr is not None:
                        log_data["learning_rate"] = lr

                swanlab.log(log_data, step=step)
                print(f"SwanLab logged: step={step}, loss={loss_value:.4f}")
            except Exception as e:
                print(f"Warning: Failed to log to SwanLab: {e}")
                import traceback
                traceback.print_exc()


def launch_optimized_training_task(
    dataset: torch.utils.data.Dataset,
    model: DiffusionTrainingModule,
    model_logger: ModelLogger,
    args=None,
):
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„è®­ç»ƒä»»åŠ¡å¯åŠ¨å™¨ï¼Œä¸“é—¨é’ˆå¯¹40GB A100è¿›è¡Œå†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
    """
    if args is None:
        raise ValueError("args is required for optimized training")

    # å‚æ•°æå–
    learning_rate = args.learning_rate
    weight_decay = args.weight_decay
    num_workers = args.dataset_num_workers
    save_steps = args.save_steps
    save_epochs_interval = getattr(args, 'save_epochs_interval', 0)  # é»˜è®¤æ¯ä¸ªepochéƒ½ä¿å­˜
    num_epochs = args.num_epochs
    gradient_accumulation_steps = args.gradient_accumulation_steps
    find_unused_parameters = args.find_unused_parameters
    train_batch_size = args.train_batch_size

    # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°é…ç½®ä¿¡æ¯ï¼ˆé¿å…å¤šè¿›ç¨‹é‡å¤æ‰“å°ï¼‰
    # ä½¿ç”¨å…¨å±€RANKçŽ¯å¢ƒå˜é‡æ£€æŸ¥ä¸»è¿›ç¨‹ï¼ˆå¤šæœºè®­ç»ƒæ—¶æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰LOCAL_RANK=0ï¼‰
    import os
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    global_rank = int(os.environ.get("RANK", 0))
    is_main_process_for_print = global_rank == 0  # åªæœ‰å…¨å±€RANK=0æ‰æ‰“å°

    # åœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°é…ç½®ä¿¡æ¯
    if is_main_process_for_print:
        print(f"Training configuration:")
        print(f"  - Batch size per GPU: 1 (Wan model limitation)")
        print(f"  - Gradient accumulation steps: {gradient_accumulation_steps}")
        print(f"  - Effective batch size per GPU: {gradient_accumulation_steps}")
        print(f"  - Total effective batch size (all GPUs): {gradient_accumulation_steps} Ã— {os.environ.get('WORLD_SIZE', 1)} = {gradient_accumulation_steps * int(os.environ.get('WORLD_SIZE', 1))}")
        print(f"  - Data workers: {num_workers}")
        print(f"  - Gradient checkpointing: {args.use_gradient_checkpointing}")
        print(f"  - Save epochs interval: {save_epochs_interval} (0=every epoch)")

    # ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é…ç½®
    # æ³¨æ„ï¼šWanæ¨¡åž‹åªæ”¯æŒbatch_size=1ï¼Œå› ä¸ºï¼š
    # 1. æ¯ä¸ªæ ·æœ¬åŒ…å«å®Œæ•´çš„è§†é¢‘åºåˆ—ï¼ˆå¤šå¸§ï¼‰
    # 2. pipelineçš„è®¾è®¡ä¸æ”¯æŒbatchå¤„ç†
    # 3. ä½¿ç”¨gradient_accumulationæ¥æ¨¡æ‹Ÿæ›´å¤§çš„batch size

    def collate_single_sample(batch):
        """
        Collate function: åªè¿”å›žç¬¬ä¸€ä¸ªæ ·æœ¬
        Wanæ¨¡åž‹ä¸æ”¯æŒbatch_size>1
        """
        return batch[0]

    dataloader_kwargs = {
        'batch_size': 1,  # å›ºå®šä¸º1ï¼ŒWanæ¨¡åž‹é™åˆ¶
        'shuffle': True,
        'num_workers': num_workers,
        'pin_memory': args.dataloader_pin_memory,
        'persistent_workers': num_workers > 0,
        'prefetch_factor': 1 if num_workers > 0 else None,
        'drop_last': True,
        'collate_fn': collate_single_sample,
    }

    if is_main_process_for_print and train_batch_size > 1:
        print(f"â„¹ï¸  Note: train_batch_size={train_batch_size} is ignored.")
        print(f"   Wan model only supports batch_size=1")
        print(f"   Use gradient_accumulation_steps={gradient_accumulation_steps} to simulate larger batches")

    if is_main_process_for_print:
        print(f"DataLoader configuration: {dataloader_kwargs}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)

    # Acceleratoré…ç½® - ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„ç®€å•æ–¹å¼
    accelerator = Accelerator(
        kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=find_unused_parameters)],
    )

    if is_main_process_for_print:
        print(f"Accelerator initialized with distributed_type: {accelerator.state.distributed_type}")

    # æ˜¾ç¤ºå¤šæœºåˆ†å¸ƒå¼è®­ç»ƒçš„è¯¦ç»†ä¿¡æ¯
    if accelerator.is_main_process:
        print("\n" + "="*80)
        print("ðŸ“Š DISTRIBUTED TRAINING CONFIGURATION")
        print("="*80)
        print(f"  Distributed type: {accelerator.state.distributed_type}")
        print(f"  Total GPUs (num_processes): {accelerator.num_processes}")
        print(f"  Current process index: {accelerator.process_index}")
        print(f"  Local process index: {accelerator.local_process_index}")
        print(f"  Device: {accelerator.device}")

        # ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å–é¢„æœŸçš„é…ç½®
        expected_num_machines = int(os.environ.get('NUM_MACHINES', os.environ.get('WORLD_SIZE', '1')))
        num_gpus_per_node = int(os.environ.get('NUM_GPUS_PER_NODE', '8'))
        expected_total_gpus = expected_num_machines * num_gpus_per_node

        print(f"\n  Expected configuration:")
        print(f"    - Machines: {expected_num_machines}")
        print(f"    - GPUs per machine: {num_gpus_per_node}")
        print(f"    - Expected total GPUs: {expected_total_gpus}")

        print(f"\n  ðŸŽ¯ Actual total GPUs being used: {accelerator.num_processes}")

        if accelerator.num_processes < expected_total_gpus:
            print(f"  âš ï¸  WARNING: Using {accelerator.num_processes} GPUs, but expected {expected_total_gpus}!")
            print(f"     Ensure you've started the script on ALL {expected_num_machines} machines")
            print(f"     with NODE_RANK values from 0 to {expected_num_machines-1}.")
        elif accelerator.num_processes == expected_total_gpus:
            print(f"  âœ… All expected GPUs are active!")
        print("="*80 + "\n")

    # åˆå§‹åŒ–SwanLabï¼ˆåªåœ¨ä¸»è¿›ç¨‹è¿›è¡Œï¼ŒAcceleratoråˆå§‹åŒ–åŽï¼‰
    # ä½¿ç”¨ accelerator.is_main_process ç¡®ä¿åªæœ‰å…¨å±€ä¸»è¿›ç¨‹åˆå§‹åŒ–SwanLab
    if accelerator.is_main_process and args.enable_swanlab and not args.debug_mode and SWANLAB_AVAILABLE:
        try:
            print("\n" + "="*80)
            print("ðŸ¦¢ INITIALIZING SWANLAB")
            print("="*80)
            print(f"API Key: {args.swanlab_api_key[:8]}***")
            print(f"Project: {args.swanlab_project}")
            print(f"Experiment: {args.swanlab_experiment}")

            swanlab.login(api_key=args.swanlab_api_key)
            swanlab_run = swanlab.init(
                project=args.swanlab_project,
                experiment_name=args.swanlab_experiment,
                config={
                    "learning_rate": args.learning_rate,
                    "num_epochs": args.num_epochs,
                    "sequence_length": args.sequence_length,
                    "train_batch_size": args.train_batch_size,
                    "lora_rank": args.lora_rank,
                    "heatmap_sigma": args.heatmap_sigma,
                    "colormap_name": args.colormap_name,
                    "height": args.height,
                    "width": args.width,
                    "gradient_accumulation_steps": args.gradient_accumulation_steps,
                    "logging_steps": args.logging_steps,
                    "num_machines": accelerator.num_processes // accelerator.state.num_processes,
                    "num_gpus": accelerator.num_processes,
                }
            )
            print(f"âœ… SwanLab initialized successfully!")
            print(f"   Project: {args.swanlab_project}")
            print(f"   Experiment: {args.swanlab_experiment}")
            print(f"   Logging frequency: every {args.logging_steps} steps")

            # æµ‹è¯•ä¸€æ¬¡æ—¥å¿—è®°å½•
            swanlab.log({"test": 1.0}, step=0)
            print("âœ… Test log sent to SwanLab")
            print("="*80 + "\n")

        except Exception as e:
            print(f"âŒ Failed to initialize SwanLab: {e}")
            import traceback
            traceback.print_exc()
            swanlab_run = None
    elif accelerator.is_main_process and args.enable_swanlab and args.debug_mode:
        print("\nâš ï¸  SwanLab disabled in debug mode\n")
    elif accelerator.is_main_process and args.enable_swanlab and not SWANLAB_AVAILABLE:
        print("\nâš ï¸  Warning: SwanLab requested but not available. Install with: pip install swanlab\n")

    # å°†SwanLab runå¯¹è±¡è®¾ç½®åˆ°argsä¸­ï¼Œè¿™æ ·è®­ç»ƒå¾ªçŽ¯å¯ä»¥è®¿é—®å®ƒ
    # æ³¨æ„ï¼šswanlab_runåœ¨ä¸Šé¢çš„if-elifå—ä¸­åˆå§‹åŒ–ï¼Œå¦‚æžœæ²¡æœ‰åˆå§‹åŒ–åˆ™ä¸ºNone
    if 'swanlab_run' not in locals():
        swanlab_run = None
    args.swanlab_run = swanlab_run

    # æ›´æ–° is_main_process ä¸º Accelerator æä¾›çš„å‡†ç¡®å€¼
    # è¿™æ ·åŽç»­ä»£ç å¯ä»¥ç»§ç»­ä½¿ç”¨ is_main_process å˜é‡
    is_main_process = accelerator.is_main_process

    # ä¼˜åŒ–å™¨é…ç½® - ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼
    optimizer = torch.optim.AdamW(
        model.trainable_modules(),
        lr=learning_rate,
        weight_decay=weight_decay,
        eps=1e-6,
    )

    # å­¦ä¹ çŽ‡è°ƒåº¦å™¨ - ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼
    scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)

    # å‡†å¤‡è®­ç»ƒç»„ä»¶ - ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„ç®€å•æ–¹å¼
    try:
        model, optimizer, dataloader, scheduler = accelerator.prepare(model, optimizer, dataloader, scheduler)

        if accelerator.is_main_process:
            print("âœ… Model preparation completed successfully")

    except Exception as e:
        if accelerator.is_main_process:
            print(f"âŒ Error during model preparation: {e}")
        raise e

    # å¼€å§‹è®­ç»ƒå¾ªçŽ¯
    try:
        if accelerator.is_main_process:
            print("ðŸš€ Starting training loop...")
            print(f"ðŸ“Š Training for {num_epochs} epochs")
            print(f"ðŸ“Š Dataset size: {len(dataloader)} batches")

        # è®­ç»ƒå¾ªçŽ¯
        for epoch_id in range(num_epochs):
            if accelerator.is_main_process:
                print(f"\nðŸ”„ Starting epoch {epoch_id+1}/{num_epochs}")

            model.train()
            epoch_loss = 0
            step_count = 0

            # æ·»åŠ è¿›åº¦æ¡
            pbar = tqdm(dataloader, desc=f"Epoch {epoch_id+1}/{num_epochs}")

            for step, data in enumerate(pbar):
                with accelerator.accumulate(model):
                    # æ¢¯åº¦æ¸…é›¶ - ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼
                    optimizer.zero_grad(set_to_none=True)

                    # å¼ºåˆ¶CUDAåŒæ­¥ï¼Œç¡®ä¿è®¾å¤‡çŠ¶æ€ä¸€è‡´
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()

                    # å‰å‘ä¼ æ’­
                    try:
                        # æ£€æŸ¥ load_from_cache å±žæ€§
                        # å¯¹äºŽ ConcatDatasetï¼Œæ£€æŸ¥ç¬¬ä¸€ä¸ªå­æ•°æ®é›†çš„å±žæ€§
                        if isinstance(dataset, ConcatDataset):
                            load_from_cache = getattr(dataset.datasets[0], 'load_from_cache', False)
                        else:
                            load_from_cache = getattr(dataset, 'load_from_cache', False)

                        if load_from_cache:
                            loss_dict = model({}, inputs=data)
                        else:
                            loss_dict = model(data)
                    except RuntimeError as e:
                        if "device" in str(e).lower():
                            if accelerator.is_main_process:
                                print(f"Device error at step {step}: {e}")
                                print(f"Current device: {accelerator.device}")
                                print(f"Model device: {next(model.parameters()).device}")
                            raise e
                        else:
                            raise e

                    # æå–lossï¼ˆæ”¯æŒæ—§ç‰ˆæœ¬è¿”å›žå•ä¸ªå€¼æˆ–æ–°ç‰ˆæœ¬è¿”å›ždictï¼‰
                    if isinstance(loss_dict, dict):
                        loss = loss_dict["loss"]
                        loss_rgb = loss_dict.get("loss_rgb", None)
                        loss_heatmap = loss_dict.get("loss_heatmap", None)
                    else:
                        # å‘åŽå…¼å®¹ï¼šå¦‚æžœè¿”å›žçš„æ˜¯å•ä¸ªlosså€¼
                        loss = loss_dict
                        loss_rgb = None
                        loss_heatmap = None

                    # åå‘ä¼ æ’­
                    accelerator.backward(loss)

                    # æ¢¯åº¦è£å‰ª
                    if hasattr(args, 'max_grad_norm') and args.max_grad_norm > 0:
                        accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    # ä¼˜åŒ–å™¨æ­¥éª¤ - ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼
                    optimizer.step()
                    if scheduler is not None:
                        scheduler.step()

                    # è®°å½•æŸå¤±
                    epoch_loss += loss.item()
                    step_count += 1

                    # SwanLabæ—¥å¿—è®°å½• - åªåœ¨ä¸»è¿›ç¨‹ä¸”æ»¡è¶³logging_stepsé¢‘çŽ‡æ—¶è®°å½•
                    global_step = step + epoch_id * len(dataloader)
                    should_log = (accelerator.is_main_process and
                                 hasattr(args, 'swanlab_run') and args.swanlab_run is not None and
                                 hasattr(args, 'logging_steps') and global_step % args.logging_steps == 0)

                    if should_log:
                        print(f"Logging to SwanLab at step {global_step}")
                        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°SwanLab
                        try:
                            # swanlab å·²ç»åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œä¸éœ€è¦é‡å¤å¯¼å…¥
                            # èŽ·å–å½“å‰å­¦ä¹ çŽ‡ - ä½¿ç”¨ä¸Žtrain.pyç›¸åŒçš„æ–¹å¼
                            current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']

                            log_data = {
                                "train_loss": loss.item(),
                                "learning_rate": current_lr,
                                "epoch": epoch_id,
                                "step": global_step
                            }

                            # æ·»åŠ RGBå’ŒHeatmap lossçš„è®°å½•
                            if loss_rgb is not None:
                                log_data["train_loss_rgb"] = loss_rgb.item()
                            if loss_heatmap is not None:
                                log_data["train_loss_heatmap"] = loss_heatmap.item()

                            swanlab.log(log_data, step=global_step)

                            # æ‰“å°æ—¥å¿—ä¿¡æ¯
                            log_msg = f"SwanLab logged: step={global_step}, loss={loss.item():.4f}"
                            if loss_rgb is not None and loss_heatmap is not None:
                                log_msg += f", loss_rgb={loss_rgb.item():.4f}, loss_heatmap={loss_heatmap.item():.4f}"
                            log_msg += f", lr={current_lr:.2e}"
                            print(log_msg)
                        except Exception as e:
                            print(f"Warning: Failed to log to SwanLab: {e}")
                    elif global_step % args.logging_steps == 0:
                        print(f"Step {global_step}: main_process={accelerator.is_main_process}, swanlab_run={args.swanlab_run is not None}")

                    # æ›´æ–°è¿›åº¦æ¡
                    postfix_dict = {
                        'loss': f"{loss.item():.4f}",
                        'avg_loss': f"{epoch_loss/step_count:.4f}"
                    }
                    # æ·»åŠ RGBå’ŒHeatmap lossåˆ°è¿›åº¦æ¡
                    if loss_rgb is not None:
                        postfix_dict['loss_rgb'] = f"{loss_rgb.item():.4f}"
                    if loss_heatmap is not None:
                        postfix_dict['loss_hm'] = f"{loss_heatmap.item():.4f}"
                    pbar.set_postfix(postfix_dict)

                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªåœ¨save_steps > 0æ—¶ä¿å­˜ï¼‰
                    if save_steps > 0:
                        model_logger.on_step_end(accelerator, model, save_steps)

                    # å‚æ•°æ›´æ–°æ£€æµ‹ - åœ¨è®­ç»ƒæ—©æœŸå’Œå…³é”®æ­¥éª¤æ£€æµ‹
                    # åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œï¼Œæ£€æµ‹æ­¥æ•°ï¼š1, 10, 50, 100
                    # if global_step in [1, 10, 50, 100]:
                    #     # èŽ·å–unwrappedæ¨¡åž‹ä»¥è®¿é—®check_parameter_updatesæ–¹æ³•
                    #     unwrapped_model = accelerator.unwrap_model(model)
                    #     if hasattr(unwrapped_model, 'check_parameter_updates'):
                    #         unwrapped_model.check_parameter_updates(global_step)

            # æ¯ä¸ªepochç»“æŸæ—¶çš„å¤„ç† - æ ¹æ®save_epochs_intervalå†³å®šæ˜¯å¦ä¿å­˜
            # save_epochs_interval=0 è¡¨ç¤ºæ¯ä¸ªepochéƒ½ä¿å­˜
            # save_epochs_interval=N è¡¨ç¤ºæ¯Nä¸ªepochä¿å­˜ä¸€æ¬¡
            should_save_epoch = (save_epochs_interval == 0) or ((epoch_id + 1) % save_epochs_interval == 0) or (epoch_id == num_epochs - 1)
            if should_save_epoch:
                model_logger.on_epoch_end(accelerator, model, epoch_id)
                if accelerator.is_main_process:
                    print(f"âœ… Saved checkpoint at epoch {epoch_id + 1}")

            # æ¯ä¸ªepochç»“æŸæ—¶æ£€æµ‹å‚æ•°æ›´æ–°
            # å·²éªŒè¯å‚æ•°ä¼šæ›´æ–°ï¼Œæš‚æ—¶æ³¨é‡ŠæŽ‰
            # if accelerator.is_main_process:
            #     unwrapped_model = accelerator.unwrap_model(model)
            #     if hasattr(unwrapped_model, 'check_parameter_updates'):
            #         unwrapped_model.check_parameter_updates(f"Epoch {epoch_id+1} end")

            accelerator.print(f"Epoch {epoch_id+1} completed. Average loss: {epoch_loss/step_count:.4f}")

        # è®­ç»ƒç»“æŸå¤„ç†ï¼ˆåªåœ¨save_steps > 0æ—¶ä¿å­˜æœ€åŽçš„stepæ£€æŸ¥ç‚¹ï¼‰
        if save_steps > 0:
            model_logger.on_training_end(accelerator, model, save_steps)

        if accelerator.is_main_process:
            print("ðŸŽ‰ Training completed successfully!")

    except Exception as training_error:
        if accelerator.is_main_process:
            print(f"âŒ Training failed with error: {training_error}")
            import traceback
            print(f"ðŸ“Š Full traceback:")
            traceback.print_exc()
        raise training_error

    # è®­ç»ƒå®Œæˆ


def create_heatmap_parser():
    """
    åˆ›å»ºçƒ­åŠ›å›¾è®­ç»ƒä¸“ç”¨çš„å‚æ•°è§£æžå™¨
    """
    import argparse
    parser = argparse.ArgumentParser(description="Heatmap sequence training for Wan2.2")

    # ä»Žwan_parserå¤åˆ¶å¿…è¦çš„å‚æ•°
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="Learning rate.")
    parser.add_argument("--num_epochs", type=int, default=5, help="Number of training epochs.")
    parser.add_argument("--output_path", type=str, required=True, help="Output path.")
    parser.add_argument("--remove_prefix_in_ckpt", type=str, default="", help="Remove prefix in checkpoint.")
    parser.add_argument("--trainable_models", type=str, default="", help="Trainable models.")
    parser.add_argument("--lora_base_model", type=str, default="", help="LoRA base model.")
    parser.add_argument("--lora_target_modules", type=str, default="q,k,v,o,ffn.0,ffn.2", help="LoRA target modules.")
    parser.add_argument("--lora_rank", type=int, default=32, help="LoRA rank.")
    parser.add_argument("--lora_checkpoint", type=str, default="", help="LoRA checkpoint.")
    parser.add_argument("--extra_inputs", type=str, default="", help="Extra inputs.")
    parser.add_argument("--use_gradient_checkpointing", action="store_true", help="Use gradient checkpointing.")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="Gradient accumulation steps.")
    parser.add_argument("--dataloader_pin_memory", type=lambda x: x.lower() == 'true', default=True, help="Enable pin memory for dataloader.")
    parser.add_argument("--max_timestep_boundary", type=float, default=1.0, help="Max timestep boundary.")
    parser.add_argument("--min_timestep_boundary", type=float, default=0.0, help="Min timestep boundary.")
    parser.add_argument("--find_unused_parameters", action="store_true", help="Find unused parameters.")
    parser.add_argument("--save_steps", type=int, default=500, help="Save steps.")
    parser.add_argument("--save_epochs_interval", type=int, default=0, help="Save model every N epochs. 0 means save every epoch.")
    parser.add_argument("--dataset_num_workers", type=int, default=0, help="Dataset num workers.")
    parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay.")
    parser.add_argument("--model_paths", type=str, default="", help="Model paths.")
    parser.add_argument("--model_id_with_origin_paths", type=str, default="", help="Model ID with origin paths.")
    parser.add_argument("--height", type=int, default=256, help="Image height.")
    parser.add_argument("--width", type=int, default=256, help="Image width.")
    parser.add_argument("--num_frames", type=int, default=5, help="Number of frames.")
    parser.add_argument("--dataset_repeat", type=int, default=1, help="Dataset repeat.")
    parser.add_argument("--train_batch_size", type=int, default=1, help="Training batch size.")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="Max gradient norm.")
    parser.add_argument("--warmup_steps", type=int, default=0, help="Warmup steps.")
    parser.add_argument("--logging_steps", type=int, default=10, help="Logging steps.")
    parser.add_argument("--wan_type", type=str, default="14B_I2V", help="Type of Wan model (e.g., '14B_I2V', '5B_TI2V', or 'WAN_2_1_14B_I2V')")
    parser.add_argument("--use_dual_head", action="store_true", help="Use dual head mode (separate heads for RGB and Heatmap)")
    parser.add_argument("--rgb_loss_weight", type=float, default=0.5,
                       help="Weight for RGB loss in total loss. Total loss = rgb_loss_weight * loss_rgb + (1 - rgb_loss_weight) * loss_heatmap. Range: 0.0 ~ 1.0 (default: 0.5)")
    parser.add_argument("--use_merged_pointcloud", action="store_true", help="Use merged pointcloud from 3 cameras (default: False, only use camera 1)")
    parser.add_argument("--use_different_projection", action="store_true", help="Use different projection mode (base_multi_view_dataset_with_rot_grip_3cam_different_projection.py)")

    # é«˜çº§è®­ç»ƒå‚æ•°ï¼šæŽ§åˆ¶æ˜¯å¦è§£å†» modulation å’Œ norm å‚æ•°
    parser.add_argument("--unfreeze_modulation_and_norms", action="store_true",
                       help="Unfreeze modulation and RMSNorm parameters (modulation, mvs_attn.norm_q/norm_k). "
                            "Default: False (keep frozen for backward compatibility with existing checkpoints). "
                            "Set to True when training new models for better adaptation.")

    # æ·»åŠ çƒ­åŠ›å›¾ä¸“ç”¨å‚æ•°
    parser.add_argument("--heatmap_data_root", type=str, nargs='+', required=True,
                       help="Root directory containing robot trajectory data (single path or list of task paths for multi-task training)")
    parser.add_argument("--trail_start", type=int, default=None,
                       help="Starting trail number (e.g., 1 for trail_1). If None, use all trails.")
    parser.add_argument("--trail_end", type=int, default=None,
                       help="Ending trail number (e.g., 50 for trail_50). If None, use all trails.")
    parser.add_argument("--sequence_length", type=int, default=10,
                       help="Length of heatmap sequence to predict")
    parser.add_argument("--step_interval", type=int, default=1,
                       help="Interval between trajectory steps")
    parser.add_argument("--min_trail_length", type=int, default=15,
                       help="Minimum trajectory length requirement")
    parser.add_argument("--heatmap_sigma", type=float, default=1.5,
                       help="Standard deviation for Gaussian heatmap generation")
    parser.add_argument("--colormap_name", type=str, default="jet",
                       help="Colormap name for heatmap conversion (ç»Ÿä¸€ä½¿ç”¨cv2 JET)")
    parser.add_argument("--scene_bounds", type=str,
                       default="0,-0.45,-0.05,0.8,0.55,0.6",
                       help="Scene bounds as comma-separated values")
    parser.add_argument("--transform_augmentation_xyz", type=str,
                       default="0.1,0.1,0.1",
                       help="XYZ augmentation range as comma-separated values")
    parser.add_argument("--transform_augmentation_rpy", type=str,
                       default="0.0,0.0,20.0",
                       help="RPY augmentation range as comma-separated values")
    parser.add_argument("--disable_augmentation", action="store_true",
                       help="Disable data augmentation")
    parser.add_argument("--debug_mode", action="store_true",
                       help="Enable debug mode (use fewer data)")

    # åŽ†å²å¸§é…ç½®
    parser.add_argument("--num_history_frames", type=int, default=1,
                       help="Number of history frames as condition. "
                            "Allowed values: 1, 2, or 1+4N (5, 9, 13, ...). "
                            "1: single-frame condition (backward compatible), 1 condition latent. "
                            "2: two frames separately encoded, 2 condition latents. "
                            "1+4N: first frame alone + groups of 4, (1+N) condition latents.")

    # SwanLabç›¸å…³å‚æ•°
    parser.add_argument("--enable_swanlab", action="store_true",
                       help="Enable SwanLab logging")
    parser.add_argument("--swanlab_api_key", type=str, default="h1x6LOLp5qGLTfsPuB7Qw",
                       help="SwanLab API key")
    parser.add_argument("--swanlab_project", type=str, default="wan2.2-heatmap-training",
                       help="SwanLab project name")
    parser.add_argument("--swanlab_experiment", type=str, default="heatmap-lora",
                       help="SwanLab experiment name")

    return parser


def parse_float_list(s: str, name: str):
    """
    è§£æžé€—å·åˆ†éš”çš„æµ®ç‚¹æ•°åˆ—è¡¨
    """
    try:
        return [float(x.strip()) for x in s.split(',')]
    except ValueError as e:
        raise ValueError(f"Invalid format for {name}: {s}. Expected comma-separated numbers.") from e


if __name__ == "__main__":
    parser = create_heatmap_parser()
    args = parser.parse_args()

    print("="*60)
    print("HEATMAP SEQUENCE TRAINING FOR WAN2.2")
    print("="*60)
    # å¤„ç†data_root - æ”¯æŒå•ä»»åŠ¡æˆ–å¤šä»»åŠ¡
    data_roots = args.heatmap_data_root if isinstance(args.heatmap_data_root, list) else [args.heatmap_data_root]
    if len(data_roots) > 1:
        print(f"Multi-task training mode: {len(data_roots)} tasks")
        for i, root in enumerate(data_roots):
            print(f"  Task {i+1}: {root}")
    else:
        print(f"Data root: {data_roots[0]}")
    if args.trail_start is not None or args.trail_end is not None:
        print(f"Trail range: [{args.trail_start or 'start'}, {args.trail_end or 'end'}]")
    print(f"Sequence length: {args.sequence_length}")
    print(f"Colormap: {args.colormap_name}")
    print(f"Output path: {args.output_path}")
    print(f"Debug mode: {args.debug_mode}")
    print(f"SwanLab enabled: {args.enable_swanlab and not args.debug_mode}")
    print("="*60)

    # SwanLabåˆå§‹åŒ–å°†åœ¨Acceleratoråˆå§‹åŒ–åŽè¿›è¡Œ
    # è¿™æ ·å¯ä»¥ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è¿›ç¨‹ä¿¡æ¯
    swanlab_run = None

    # æ‰“å°åˆ†å¸ƒå¼çŽ¯å¢ƒä¿¡æ¯ï¼ˆä»…ç”¨äºŽè°ƒè¯•ï¼‰
    import os
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    global_rank = int(os.environ.get("RANK", 0))

    # å®šä¹‰ is_main_process ç”¨äºŽ Accelerator åˆå§‹åŒ–ä¹‹å‰çš„ä»£ç 
    # æ³¨æ„ï¼šè¿™æ˜¯åŸºäºŽçŽ¯å¢ƒå˜é‡çš„åˆæ­¥åˆ¤æ–­ï¼Œå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹ä¸å‡†ç¡®
    # Accelerator åˆå§‹åŒ–åŽåº”ä½¿ç”¨ accelerator.is_main_process
    is_main_process = global_rank == 0

    print(f"ðŸš€ Process initialization...")
    print(f"   - LOCAL_RANK from env: {local_rank}")
    print(f"   - RANK from env: {global_rank}")
    print(f"   - WORLD_SIZE from env: {os.environ.get('WORLD_SIZE', 'Not set')}")
    print(f"   - Is main process (preliminary): {is_main_process}")
    print(f"   âš ï¸  Note: These values might not be final until Accelerator is initialized")

    # è§£æžå‚æ•°
    scene_bounds = parse_float_list(args.scene_bounds, "scene_bounds")
    transform_augmentation_xyz = parse_float_list(args.transform_augmentation_xyz, "transform_augmentation_xyz")
    transform_augmentation_rpy = parse_float_list(args.transform_augmentation_rpy, "transform_augmentation_rpy")

    # éªŒè¯ num_history_frames çš„åˆæ³•æ€§ï¼šå¿…é¡»æ˜¯ 1, 2, æˆ– 1+4N
    def is_valid_history_frames(n):
        if n == 1 or n == 2:
            return True
        if n > 2 and (n - 1) % 4 == 0:
            return True
        return False

    if not is_valid_history_frames(args.num_history_frames):
        raise ValueError(
            f"Invalid num_history_frames={args.num_history_frames}.\n"
            f"Allowed values: 1, 2, or 1+4N (5, 9, 13, ...).\n"
            f"This ensures proper VAE encoding: first frame alone, then groups of 4."
        )

    # éªŒè¯ wan_type å’Œ num_history_frames çš„ä¸€è‡´æ€§ï¼ˆåŒå‘æ£€æµ‹ï¼‰
    # è§„åˆ™1: num_history_frames > 1 å¿…é¡»ä½¿ç”¨ 5B_TI2V_RGB_HEATMAP_MV_HISTORY
    if args.num_history_frames > 1 and args.wan_type != "5B_TI2V_RGB_HEATMAP_MV_HISTORY":
        raise ValueError(
            f"Invalid configuration: num_history_frames={args.num_history_frames} > 1, "
            f"but wan_type={args.wan_type}.\n"
            f"When using multi-frame history (num_history_frames > 1), you MUST set "
            f"wan_type='5B_TI2V_RGB_HEATMAP_MV_HISTORY'."
        )
    # è§„åˆ™2: ä½¿ç”¨ 5B_TI2V_RGB_HEATMAP_MV_HISTORY å¿…é¡»è®¾ç½® num_history_frames > 1
    if args.num_history_frames == 1 and args.wan_type == "5B_TI2V_RGB_HEATMAP_MV_HISTORY":
        raise ValueError(
            f"Invalid configuration: wan_type=5B_TI2V_RGB_HEATMAP_MV_HISTORY, "
            f"but num_history_frames={args.num_history_frames}.\n"
            f"When using 5B_TI2V_RGB_HEATMAP_MV_HISTORY, you MUST set num_history_frames > 1.\n"
            f"If you want single-frame mode, use wan_type='5B_TI2V_RGB_HEATMAP_MV'."
        )

    # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®é›†
    if is_main_process:
        print("Creating heatmap dataset...")
        print(f"  num_history_frames: {args.num_history_frames}")

    # é€‰æ‹©æ•°æ®é›†å·¥åŽ‚ï¼šå¤šå¸§åŽ†å²ä½¿ç”¨ HeatmapDatasetFactoryWithHistory
    use_history_dataset = args.num_history_frames > 1
    DatasetFactory = HeatmapDatasetFactoryWithHistory if use_history_dataset else HeatmapDatasetFactory

    try:
        # å¤šä»»åŠ¡è®­ç»ƒ: ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºæ•°æ®é›†
        if len(data_roots) > 1:
            if is_main_process:
                print(f"Multi-task training mode: {len(data_roots)} tasks")
                if use_history_dataset:
                    print(f"  Using HeatmapDatasetFactoryWithHistory (num_history_frames={args.num_history_frames})")
            datasets = []
            for task_idx, task_root in enumerate(data_roots):
                if is_main_process:
                    print(f"  Loading task {task_idx+1}/{len(data_roots)}: {task_root}")

                # æž„å»ºåŸºç¡€å‚æ•°
                dataset_kwargs = dict(
                    data_root=task_root,
                    sequence_length=args.sequence_length,
                    step_interval=args.step_interval,
                    min_trail_length=args.min_trail_length,
                    image_size=(args.height, args.width),
                    sigma=args.heatmap_sigma,
                    augmentation=not args.disable_augmentation,
                    mode="train",
                    scene_bounds=scene_bounds,
                    transform_augmentation_xyz=transform_augmentation_xyz,
                    transform_augmentation_rpy=transform_augmentation_rpy,
                    debug=args.debug_mode,
                    colormap_name=args.colormap_name,
                    repeat=args.dataset_repeat,
                    wan_type=args.wan_type,
                    trail_start=args.trail_start,
                    trail_end=args.trail_end,
                    use_merged_pointcloud=args.use_merged_pointcloud,
                    use_different_projection=args.use_different_projection,
                )
                # å¤šå¸§åŽ†å²éœ€è¦é¢å¤–å‚æ•°
                if use_history_dataset:
                    dataset_kwargs['num_history_frames'] = args.num_history_frames

                task_dataset = DatasetFactory.create_robot_trajectory_dataset(**dataset_kwargs)
                if is_main_process:
                    print(f"    âœ“ Task {task_idx+1} loaded: {len(task_dataset)} samples")
                datasets.append(task_dataset)

            # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
            dataset = ConcatDataset(datasets)
            if is_main_process:
                print(f"âœ“ Multi-task dataset created: {len(dataset)} samples (from {len(data_roots)} tasks)")
        else:
            # å•ä»»åŠ¡è®­ç»ƒ
            if is_main_process:
                print(f"Single-task training mode: {data_roots[0]}")
                if use_history_dataset:
                    print(f"  Using HeatmapDatasetFactoryWithHistory (num_history_frames={args.num_history_frames})")

            # æž„å»ºåŸºç¡€å‚æ•°
            dataset_kwargs = dict(
                data_root=data_roots[0],
                sequence_length=args.sequence_length,
                step_interval=args.step_interval,
                min_trail_length=args.min_trail_length,
                image_size=(args.height, args.width),
                sigma=args.heatmap_sigma,
                augmentation=not args.disable_augmentation,
                mode="train",
                scene_bounds=scene_bounds,
                transform_augmentation_xyz=transform_augmentation_xyz,
                transform_augmentation_rpy=transform_augmentation_rpy,
                debug=args.debug_mode,
                colormap_name=args.colormap_name,
                repeat=args.dataset_repeat,
                wan_type=args.wan_type,
                trail_start=args.trail_start,
                trail_end=args.trail_end,
                use_merged_pointcloud=args.use_merged_pointcloud,
                use_different_projection=args.use_different_projection,
            )
            # å¤šå¸§åŽ†å²éœ€è¦é¢å¤–å‚æ•°
            if use_history_dataset:
                dataset_kwargs['num_history_frames'] = args.num_history_frames

            dataset = DatasetFactory.create_robot_trajectory_dataset(**dataset_kwargs)
            if is_main_process:
                print(f"âœ“ Dataset created: {len(dataset)} samples")

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆæ•°æ®é›†åˆ›å»ºï¼ˆåœ¨åˆ†å¸ƒå¼çŽ¯å¢ƒä¸‹ç”±barrierè‡ªåŠ¨å¤„ç†ï¼‰
        pass

        if is_main_process:
            print(f"Dataset created successfully with {len(dataset)} samples")

            # æµ‹è¯•æ•°æ®åŠ è½½
            print("Testing data loading...")
            test_sample = dataset[10]
            print(f"Sample keys: {list(test_sample.keys())}")
            print(f"Video frames: {len(test_sample['video'])}")
            print(f"First frame size: {test_sample['video'][0][0].size}")
            print(f"Prompt: {test_sample['prompt']}")

    except Exception as e:
        if is_main_process:
            print(f"Error creating dataset: {e}")
            import traceback
            traceback.print_exc()
        # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½é€€å‡º
        exit(1)

    # åˆ›å»ºè®­ç»ƒæ¨¡å—
    if is_main_process:
        print("Creating training module...")
    try:
        # å¤„ç†ç©ºå­—ç¬¦ä¸²å‚æ•°ï¼Œé¿å…å¯¼è‡´é”™è¯¯
        model_id_with_origin_paths = args.model_id_with_origin_paths if args.model_id_with_origin_paths else None
        lora_checkpoint = args.lora_checkpoint if args.lora_checkpoint else None
        trainable_models = args.trainable_models if args.trainable_models else None
        lora_base_model = args.lora_base_model if args.lora_base_model else None

        print(f"Initializing training module with wan_type: {args.wan_type}")
        print(f"Model paths: {args.model_paths}")
        print(f"LoRA base model: {args.lora_base_model}")

        # åˆå§‹åŒ–è®­ç»ƒæ¨¡åž‹
        print(f"Process {local_rank}: Starting model creation...")
        print(f"Use dual head mode: {args.use_dual_head}")
        model = HeatmapWanTrainingModule(
            wan_type=args.wan_type,
            model_paths=args.model_paths,
            model_id_with_origin_paths=model_id_with_origin_paths,
            trainable_models=trainable_models,
            lora_base_model=lora_base_model,
            lora_target_modules=args.lora_target_modules,
            lora_rank=args.lora_rank,
            lora_checkpoint=lora_checkpoint,
            use_gradient_checkpointing=args.use_gradient_checkpointing,
            extra_inputs=args.extra_inputs,
            max_timestep_boundary=args.max_timestep_boundary,
            min_timestep_boundary=args.min_timestep_boundary,
            use_dual_head=args.use_dual_head,
            unfreeze_modulation_and_norms=args.unfreeze_modulation_and_norms,
            num_history_frames=args.num_history_frames,
            rgb_loss_weight=args.rgb_loss_weight,
        )

        print(f"Process {local_rank}: Model creation completed successfully!")

        if is_main_process:
            print("Training module created successfully")

    except Exception as e:
        if is_main_process:
            print(f"Error creating training module: {e}")
            import traceback
            traceback.print_exc()
        # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½é€€å‡º
        exit(1)

    # åˆ›å»ºæ¨¡åž‹æ—¥å¿—å™¨
    if is_main_process:
        print("Setting up model logger...")
    model_logger = ModelLogger(
        args.output_path,
        remove_prefix_in_ckpt=args.remove_prefix_in_ckpt
    )

    # å¯åŠ¨è®­ç»ƒ
    if is_main_process:
        print("Starting training...")
        print(f"Optimizing data loading with {args.dataset_num_workers} workers...")
    try:
        # å¦‚æžœSwanLabå¯ç”¨ï¼Œæ·»åŠ åˆ°argsä¸­
        if swanlab_run is not None:
            args.swanlab_run = swanlab_run
        else:
            args.swanlab_run = None

        # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„è®­ç»ƒå‡½æ•°ï¼Œå®ƒæœ‰æ›´å¥½çš„é”™è¯¯å¤„ç†
        launch_optimized_training_task(dataset, model, model_logger, args=args)

        if is_main_process:
            print("Training completed successfully!")

        # ç»“æŸSwanLabå®žéªŒ
        if swanlab_run is not None:
            swanlab_run.finish()
            if is_main_process:
                print("SwanLab experiment finished")

    except Exception as e:
        if is_main_process:
            print(f"Training failed: {e}")
            import traceback
            traceback.print_exc()

        # ç¡®ä¿SwanLabå®žéªŒæ­£ç¡®ç»“æŸ
        if swanlab_run is not None:
            swanlab_run.finish()

        exit(1)