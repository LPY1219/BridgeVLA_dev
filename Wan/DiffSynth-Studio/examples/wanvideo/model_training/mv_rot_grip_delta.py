"""
Multi-View Rotation and Gripper Prediction Training Script

This script trains a model to predict rotation (roll, pitch, yaw) and gripper state
from VAE-encoded RGB and heatmap features.

Key features:
- Uses VAE-compressed video features (RGB + Heatmap)
- Temporal compression: T frames -> 1 + (T-1)//4 frames
- Predicts rotation and gripper for future frames
- Uses SwanLab for logging
- Tracks accuracy metrics during training
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, ConcatDataset
from accelerate import Accelerator
from accelerate.utils import DistributedDataParallelKwargs
from tqdm import tqdm
import os
import sys
from pathlib import Path
import argparse
import numpy as np
from typing import Dict, Optional

# Add DiffSynth-Studio to path
diffsynth_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
sys.path.insert(0, diffsynth_path)

from diffsynth.trainers.heatmap_dataset_mv_with_rot_grip import HeatmapDatasetFactory
from diffsynth import ModelManager, WanVideoPipeline


class MultiViewRotationGripperPredictor(nn.Module):
    """
    é¢„æµ‹æ—‹è½¬å’Œå¤¹çˆªå˜åŒ–é‡çš„æ¨¡å‹

    åŸºäºå¸§é—´ç‰¹å¾å·®å¼‚é¢„æµ‹ç›¸å¯¹äºç¬¬ä¸€å¸§çš„å˜åŒ–é‡

    è¾“å…¥ï¼š
        - rgb_features: VAEç¼–ç çš„RGBç‰¹å¾ (b, v, c_rgb, t_compressed, h, w)
        - heatmap_features: VAEç¼–ç çš„Heatmapç‰¹å¾ (b, v, c_hm, t_compressed, h, w)
        - num_future_frames: éœ€è¦é¢„æµ‹çš„æœªæ¥å¸§æ•° (T-1ï¼Œä¸åŒ…æ‹¬åˆå§‹å¸§)

    è¾“å‡ºï¼š
        - rotation_logits: (b, num_future_frames, num_rotation_bins*3) - æœªæ¥å¸§ç›¸å¯¹äºç¬¬ä¸€å¸§çš„rotationå˜åŒ–é‡é¢„æµ‹
        - gripper_logits: (b, num_future_frames, 2) - æœªæ¥å¸§çš„gripperçŠ¶æ€å˜åŒ–é¢„æµ‹ (0=ä¸å˜, 1=æ”¹å˜)

    æ ¸å¿ƒè®¾è®¡ï¼š
        - ä¸éœ€è¦åˆå§‹æ—‹è½¬å’Œå¤¹çˆªçŠ¶æ€ä½œä¸ºè¾“å…¥
        - è®¡ç®—æ¯ä¸ªæœªæ¥å¸§ä¸ç¬¬ä¸€å¸§çš„ç‰¹å¾å·®å¼‚ï¼ˆé€šè¿‡cross-attentionï¼‰
        - åŸºäºç‰¹å¾å·®å¼‚é¢„æµ‹æ—‹è½¬å˜åŒ–é‡å’Œå¤¹çˆªçŠ¶æ€å˜åŒ–

    å…¶ä¸­ t_compressed = 1 + (T-1)//4, t_future = T-1 (å› ä¸ºä¸é¢„æµ‹åˆå§‹å¸§)
    """

    def __init__(
        self,
        rgb_channels: int = 48,  # VAE latent channels
        heatmap_channels: int = 48,  # VAE latent channels
        hidden_dim: int = 512,
        num_views: int = 3,
        num_rotation_bins: int = 72,
        temporal_upsample_factor: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.rgb_channels = rgb_channels
        self.heatmap_channels = heatmap_channels
        self.hidden_dim = hidden_dim
        self.num_views = num_views
        self.num_rotation_bins = num_rotation_bins
        self.temporal_upsample_factor = temporal_upsample_factor

        # ç‰¹å¾æå–å™¨ - ä¸ºæ¯ä¸ªè§†è§’å’Œæ¯ç§æ¨¡æ€æå–ç‰¹å¾
        input_channels = rgb_channels + heatmap_channels
        self.feature_extractor = nn.Sequential(
            nn.Conv3d(input_channels, hidden_dim // 2, kernel_size=3, padding=1),
            nn.GroupNorm(8, hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Conv3d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),
            nn.GroupNorm(8, hidden_dim),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool3d((None, 1, 1)),  # ç©ºé—´æ± åŒ–: (b*v, hidden_dim, t, 1, 1)
        ) # ä¸€å®šè¦æ‹¼æ¥heatmapå’Œ colorå˜›ï¼Ÿ

        # å¤šè§†è§’èåˆ
        self.view_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )

        # æ—¶é—´ä¸Šé‡‡æ ·æ¨¡å— - åŸºäºVAEå‹ç¼©ç‰¹æ€§çš„è®¾è®¡
        # VAEå‹ç¼©ç­–ç•¥: ä¿ç•™ç¬¬1å¸§ + åç»­å¸§4xå‹ç¼©
        # compressed: 1 + (T-1)//4, target: T-1 (future frames only)

        # ç¬¬0å¸§ç‰¹å¾å¤„ç† - å•ç‹¬å¤„ç†å®Œæ•´ä¿ç•™çš„ç¬¬ä¸€å¸§
        self.first_frame_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
        )

        # åç»­å¸§ä¸Šé‡‡æ · - å¯¹å‹ç¼©çš„å¸§è¿›è¡Œ4xä¸Šé‡‡æ ·
        self.compressed_frames_upsampler = nn.Sequential(
            # ç‰¹å¾å¢å¼º
            nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            # 4xè½¬ç½®å·ç§¯ä¸Šé‡‡æ ·
            nn.ConvTranspose1d(hidden_dim * 2, hidden_dim, kernel_size=4, stride=4, padding=0),
            nn.ReLU(inplace=True),
            # ç‰¹å¾æç‚¼
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

        # Cross-Attention: æœªæ¥å¸§ attend to ç¬¬ä¸€å¸§
        # ç”¨äºå»ºæ¨¡æœªæ¥å¸§ä¸ç¬¬ä¸€å¸§ä¹‹é—´çš„å…³ç³»
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        self.cross_attn_norm = nn.LayerNorm(hidden_dim)

        # Transformerç¼–ç å™¨ - æ—¶é—´å»ºæ¨¡ï¼ˆå¤„ç†cross-attentionåçš„ç‰¹å¾ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=8,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)

        # é¢„æµ‹å¤´
        self.rotation_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_rotation_bins * 3)  # roll, pitch, yaw
        )

        self.gripper_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 2)  # open, close
        )

    def forward(
        self,
        rgb_features: torch.Tensor,  # (b, v, c_rgb, t_compressed, h, w)
        heatmap_features: torch.Tensor,  # (b, v, c_hm, t_compressed, h, w)
        num_future_frames: int,  # T-1
    ):
        b, v, _, t_compressed, h, w = rgb_features.shape

        # 1. åˆå¹¶RGBå’ŒHeatmapç‰¹å¾
        combined_features = torch.cat([rgb_features, heatmap_features], dim=2)  # (b, v, c_rgb+c_hm, t, h, w)

        # 2. ä¸ºæ¯ä¸ªè§†è§’æå–ç‰¹å¾
        # Reshape: (b, v, c, t, h, w) -> (b*v, c, t, h, w)
        c_total = self.rgb_channels + self.heatmap_channels
        combined_features = combined_features.view(b * v, c_total, t_compressed, h, w)
        features = self.feature_extractor(combined_features)  # (b*v, hidden_dim, t, 1, 1)
        features = features.squeeze(-1).squeeze(-1)  # (b*v, hidden_dim, t)
        features = features.permute(0, 2, 1)  # (b*v, t, hidden_dim)

        # Reshape back: (b*v, t, hidden_dim) -> (b, v, t, hidden_dim)
        features = features.view(b, v, t_compressed, self.hidden_dim)

        # 3. è·¨è§†è§’èåˆï¼ˆåœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼‰
        fused_features = []
        for t_idx in range(t_compressed):
            # å–å‡ºæ‰€æœ‰è§†è§’åœ¨æ—¶é—´æ­¥tçš„ç‰¹å¾
            view_features = features[:, :, t_idx, :]  # (b, v, hidden_dim)
            # Multi-head attentionè·¨è§†è§’èåˆ
            fused, _ = self.view_attention(
                view_features, view_features, view_features
            )  # (b, v, hidden_dim)
            # å¹³å‡æ± åŒ–æ‰€æœ‰è§†è§’
            fused = fused.mean(dim=1)  # (b, hidden_dim)
            fused_features.append(fused)

        fused_features = torch.stack(fused_features, dim=1)  # (b, t_compressed, hidden_dim)

        # 4. åŸºäºVAEå‹ç¼©ç‰¹æ€§çš„æ—¶é—´ä¸Šé‡‡æ ·
        # VAEå‹ç¼©: [frame_0(å®Œæ•´), compressed_frames(1+(T-1)//4)]
        # ç›®æ ‡: é¢„æµ‹future frames (T-1å¸§)

        # åˆ†ç¦»ç¬¬0å¸§å’Œå‹ç¼©å¸§
        first_frame_features = fused_features[:, 0, :]  # (b, hidden_dim) - å®Œæ•´ä¿ç•™çš„ç¬¬ä¸€å¸§
        compressed_features = fused_features[:, 1:, :]  # (b, t_compressed-1, hidden_dim) - å‹ç¼©çš„åç»­å¸§

        # å¤„ç†ç¬¬0å¸§ç‰¹å¾
        first_frame_features = self.first_frame_proj(first_frame_features)  # (b, hidden_dim)

        # å¯¹å‹ç¼©å¸§è¿›è¡Œ4xä¸Šé‡‡æ ·
        if compressed_features.shape[1] > 0:  # å¦‚æœæœ‰å‹ç¼©å¸§
            # (b, t_compressed-1, hidden_dim) -> (b, hidden_dim, t_compressed-1)
            compressed_features = compressed_features.permute(0, 2, 1)
            # 4xä¸Šé‡‡æ ·: (b, hidden_dim, t_compressed-1) -> (b, hidden_dim, (t_compressed-1)*4)
            upsampled_features = self.compressed_frames_upsampler(compressed_features)
        else:
            # å¦‚æœåªæœ‰ç¬¬0å¸§ï¼Œç›´æ¥ä½¿ç”¨
            upsampled_features = first_frame_features.unsqueeze(-1)  # (b, hidden_dim, 1)

        # å¦‚æœä¸Šé‡‡æ ·åçš„é•¿åº¦ä¸ç›®æ ‡é•¿åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œå¾®è°ƒ
        if upsampled_features.size(2) != num_future_frames:
            upsampled_features = F.interpolate(
                upsampled_features,
                size=num_future_frames,
                mode='linear',
                align_corners=True if num_future_frames > 1 else False
            )  # (b, hidden_dim, num_future_frames)

        # (b, hidden_dim, num_future_frames) -> (b, num_future_frames, hidden_dim)
        future_frame_features = upsampled_features.permute(0, 2, 1)

        # 5. Cross-Attention: æœªæ¥å¸§ attend to ç¬¬ä¸€å¸§
        # Query: æœªæ¥å¸§ç‰¹å¾, Key/Value: ç¬¬ä¸€å¸§ç‰¹å¾
        # è¿™æ ·æ¯ä¸ªæœªæ¥å¸§éƒ½å¯ä»¥ä»ç¬¬ä¸€å¸§ä¸­æå–ç›¸å…³ä¿¡æ¯æ¥è®¡ç®—å˜åŒ–é‡
        # éœ€è¦å°†ç¬¬ä¸€å¸§ç‰¹å¾è½¬æ¢ä¸ºæ­£ç¡®çš„shape
        first_frame_features_for_attn = first_frame_features.unsqueeze(1)  # (b, 1, hidden_dim)

        cross_attn_output, _ = self.cross_attention(
            query=future_frame_features,       # (b, num_future_frames, hidden_dim)
            key=first_frame_features_for_attn,  # (b, 1, hidden_dim)
            value=first_frame_features_for_attn # (b, 1, hidden_dim)
        )  # (b, num_future_frames, hidden_dim)

        # æ®‹å·®è¿æ¥ + LayerNorm
        future_frame_features = self.cross_attn_norm(future_frame_features + cross_attn_output)

        # 6. Transformeræ—¶é—´å»ºæ¨¡ï¼ˆå¤„ç†æœªæ¥å¸§ä¹‹é—´çš„æ—¶åºä¾èµ–ï¼‰
        temporal_features = self.transformer(future_frame_features)  # (b, num_future_frames, hidden_dim)

        # 7. é¢„æµ‹æœªæ¥å¸§ç›¸å¯¹äºç¬¬ä¸€å¸§çš„rotationå˜åŒ–é‡å’ŒgripperçŠ¶æ€å˜åŒ–
        rotation_logits = self.rotation_head(temporal_features)  # (b, num_future_frames, num_bins*3)
        gripper_logits = self.gripper_head(temporal_features)  # (b, num_future_frames, 2)

        return rotation_logits, gripper_logits


class VAEFeatureExtractor:
    """
    VAEç‰¹å¾æå–å™¨ - ç”¨äºæå–RGBå’ŒHeatmapçš„VAEç‰¹å¾
    å‚è€ƒ WanVideoUnit_InputVideoEmbedder çš„å®ç°æ–¹å¼
    """

    def __init__(self, vae, device, torch_dtype=torch.bfloat16):
        self.vae = vae
        self.device = device
        self.torch_dtype = torch_dtype

    def preprocess_image(self, image, min_value=-1, max_value=1):
        """å°† PIL.Image è½¬æ¢ä¸º torch.Tensor"""
        # Transform a PIL.Image to torch.Tensor
        image = torch.Tensor(np.array(image, dtype=np.float32))
        image = image.to(dtype=self.torch_dtype, device=self.device)
        image = image * ((max_value - min_value) / 255) + min_value
        # pattern: "B C H W"
        image = image.permute(2, 0, 1).unsqueeze(0)  # H W C -> 1 C H W
        return image

    def preprocess_video(self, video, min_value=-1, max_value=1):
        """
        å°† list of PIL.Image è½¬æ¢ä¸º torch.Tensor
        å‚è€ƒ ModelManager.preprocess_video çš„å®ç°

        Args:
            video: List[PIL.Image] - è§†é¢‘å¸§åˆ—è¡¨

        Returns:
            torch.Tensor - shape (1, C, T, H, W)
        """
        # Transform a list of PIL.Image to torch.Tensor
        video_tensors = [self.preprocess_image(image, min_value=min_value, max_value=max_value) for image in video]
        # Stack along time dimension: [(1, C, H, W)] -> (1, C, H, W) * T
        # We need to concatenate along a new time dimension
        # First stack to get (T, 1, C, H, W), then squeeze and permute
        video = torch.stack(video_tensors, dim=0)  # (T, 1, C, H, W)
        video = video.squeeze(1)  # (T, C, H, W)
        video = video.permute(1, 0, 2, 3)  # (C, T, H, W)
        video = video.unsqueeze(0)  # (1, C, T, H, W)
        return video

    @torch.no_grad()
    def encode_videos(self, rgb_videos, heatmap_videos, tiled=False, tile_size=(34, 34), tile_stride=(18, 16)):
        """
        ç¼–ç RGBå’ŒHeatmapè§†é¢‘
        å‚è€ƒ WanVideoUnit_InputVideoEmbedder.process çš„å®ç°

        Multi-view input: List[List[PIL.Image]] with shape [time][view]
        éœ€è¦è½¬æ¢ä¸º [view][time] ç„¶ååˆ†åˆ«å¤„ç†æ¯ä¸ªè§†è§’

        Args:
            rgb_videos: List[List[PIL.Image]] - [time][view] RGBè§†é¢‘
            heatmap_videos: List[List[PIL.Image]] - [time][view] Heatmapè§†é¢‘

        Returns:
            rgb_features: (num_views, c, t_compressed, h, w)
            heatmap_features: (num_views, c, t_compressed, h, w)
        """
        # Multi-view input: List[List[PIL.Image]] with shape (T, num_views)
        num_frames = len(rgb_videos)
        num_views = len(rgb_videos[0])

        # æŒ‰è§†è§’åˆ†ç»„å¤„ç† - RGB
        all_rgb_view_latents = []
        for view_idx in range(num_views):
            # æå–å½“å‰è§†è§’çš„æ‰€æœ‰RGBå¸§: [time] -> List[PIL.Image]
            view_rgb_frames = [rgb_videos[t][view_idx] for t in range(num_frames)]
            # é¢„å¤„ç†ä¸ºtensor: (1, C, T, H, W)
            view_rgb_video = self.preprocess_video(view_rgb_frames)
            # Remove batch dimension: (1, C, T, H, W) -> (C, T, H, W)
            # VAE.encode expects (C, T, H, W) and will add batch dim internally
            view_rgb_video = view_rgb_video.squeeze(0)
            # VAEç¼–ç : (C, T, H, W) -> (c_latent, t_compressed, h_latent, w_latent)
            view_rgb_latents = self.vae.encode(
                [view_rgb_video],
                device=self.device,
                tiled=tiled,
                tile_size=tile_size,
                tile_stride=tile_stride
            )
            # å–ç¬¬ä¸€ä¸ªå…ƒç´ å¹¶è½¬æ¢ç±»å‹
            view_rgb_latents = view_rgb_latents[0].to(dtype=self.torch_dtype, device=self.device)
            all_rgb_view_latents.append(view_rgb_latents)

        # åˆå¹¶æ‰€æœ‰è§†è§’çš„RGB latents: List[(c, t, h, w)] -> (num_views, c, t, h, w)
        rgb_features = torch.stack(all_rgb_view_latents, dim=0)

        # æŒ‰è§†è§’åˆ†ç»„å¤„ç† - Heatmap
        all_heatmap_view_latents = []
        for view_idx in range(num_views):
            # æå–å½“å‰è§†è§’çš„æ‰€æœ‰Heatmapå¸§
            view_heatmap_frames = [heatmap_videos[t][view_idx] for t in range(num_frames)]
            # é¢„å¤„ç†ä¸ºtensor: (1, C, T, H, W)
            view_heatmap_video = self.preprocess_video(view_heatmap_frames)
            # Remove batch dimension: (1, C, T, H, W) -> (C, T, H, W)
            view_heatmap_video = view_heatmap_video.squeeze(0)
            # VAEç¼–ç 
            view_heatmap_latents = self.vae.encode(
                [view_heatmap_video],
                device=self.device,
                tiled=tiled,
                tile_size=tile_size,
                tile_stride=tile_stride
            )
            view_heatmap_latents = view_heatmap_latents[0].to(dtype=self.torch_dtype, device=self.device)
            all_heatmap_view_latents.append(view_heatmap_latents)

        # åˆå¹¶æ‰€æœ‰è§†è§’çš„Heatmap latents: List[(c, t, h, w)] -> (num_views, c, t, h, w)
        heatmap_features = torch.stack(all_heatmap_view_latents, dim=0)

        return rgb_features, heatmap_features


def collate_fn_with_vae(batch, vae_extractor, heatmap_latent_scale=1.0, latent_noise_std=0.0):
    """
    è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºVAEç‰¹å¾

    Args:
        latent_noise_std: æ·»åŠ åˆ°latentçš„é«˜æ–¯å™ªå£°æ ‡å‡†å·®ï¼ˆç”¨äºè®­ç»ƒæ—¶æ•°æ®å¢å¼ºï¼‰
    """
    # batchæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯dataset[i]
    # ç”±äºWanæ¨¡å‹é™åˆ¶ï¼Œæˆ‘ä»¬ä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªæ ·æœ¬
    sample = batch[0]

    # æå–æ•°æ®
    input_video_rgb = sample['input_video_rgb']  # [time][view] - PIL Images
    input_video_heatmap = sample['video']  # [time][view] - PIL Images

    # ä½¿ç”¨ delta å­—æ®µï¼ˆç›¸å¯¹äºç¬¬ä¸€å¸§çš„å˜åŒ–é‡ï¼‰ï¼Œè€Œä¸æ˜¯ç»å¯¹å€¼
    rotation_targets = sample['rotation_delta_targets']  # (t-1, 3) - ç›¸å¯¹äºç¬¬ä¸€å¸§çš„æ—‹è½¬å˜åŒ–é‡
    gripper_targets = sample['gripper_change_targets']  # (t-1,) - ç›¸å¯¹äºç¬¬ä¸€å¸§çš„å¤¹çˆªçŠ¶æ€å˜åŒ–

    # å¤„ç†ç»´åº¦
    if rotation_targets.ndim == 2:
        rotation_targets = rotation_targets.unsqueeze(0)  # (1, t-1, 3)
    if gripper_targets.ndim == 1:
        gripper_targets = gripper_targets.unsqueeze(0)  # (1, t-1)

    # ç¼–ç ä¸ºVAEç‰¹å¾
    rgb_features, heatmap_features = vae_extractor.encode_videos(
        input_video_rgb, input_video_heatmap
    )

    # åº”ç”¨heatmapç¼©æ”¾
    if heatmap_latent_scale != 1.0:
        heatmap_features = heatmap_features * heatmap_latent_scale

    # æ·»åŠ å™ªå£°å¢å¼º - æ¨¡æ‹Ÿæ‰©æ•£æ¨¡å‹é™å™ªåçš„latent
    # è¿™å¯ä»¥æå‡æ¨¡å‹å¯¹æ¨ç†æ—¶noisy latentsçš„é²æ£’æ€§
    if latent_noise_std > 0:
        # ä¸ºRGB latentæ·»åŠ é«˜æ–¯å™ªå£°
        rgb_noise = torch.randn_like(rgb_features) * latent_noise_std
        rgb_features = rgb_features + rgb_noise

        # ä¸ºHeatmap latentæ·»åŠ é«˜æ–¯å™ªå£°
        heatmap_noise = torch.randn_like(heatmap_features) * latent_noise_std
        heatmap_features = heatmap_features + heatmap_noise

    # æ·»åŠ batchç»´åº¦
    rgb_features = rgb_features.unsqueeze(0)  # (1, v, c, t_compressed, h, w)
    heatmap_features = heatmap_features.unsqueeze(0)

    return {
        'rgb_features': rgb_features,
        'heatmap_features': heatmap_features,
        'rotation_targets': rotation_targets,  # (1, t-1, 3)
        'gripper_targets': gripper_targets,  # (1, t-1)
        'num_future_frames': rotation_targets.shape[1],  # t-1
    }


def train_epoch(
    model: MultiViewRotationGripperPredictor,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler,
    accelerator: Accelerator,
    epoch_id: int,
    args,
):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()

    epoch_loss = 0
    epoch_loss_rotation = 0
    epoch_loss_gripper = 0
    step_count = 0

    # å‡†ç¡®ç‡ç´¯ç§¯
    rotation_acc_roll_sum = 0.0
    rotation_acc_pitch_sum = 0.0
    rotation_acc_yaw_sum = 0.0
    gripper_acc_sum = 0.0
    acc_step_count = 0

    pbar = tqdm(dataloader, desc=f"Epoch {epoch_id+1}/{args.num_epochs}")

    for step, batch in enumerate(pbar):
        with accelerator.accumulate(model):
            optimizer.zero_grad(set_to_none=True)

            # å‰å‘ä¼ æ’­
            rotation_logits, gripper_logits = model(
                rgb_features=batch['rgb_features'],
                heatmap_features=batch['heatmap_features'],
                num_future_frames=batch['num_future_frames'],
            )

            # è®¡ç®—loss
            rotation_targets = batch['rotation_targets']  # (b, t-1, 3)
            gripper_targets = batch['gripper_targets']  # (b, t-1)

            # Rotation loss (cross entropy for each of roll, pitch, yaw)
            num_bins = rotation_logits.shape[-1] // 3
            rotation_logits_reshaped = rotation_logits.view(
                rotation_logits.shape[0], rotation_logits.shape[1], 3, num_bins
            )  # (b, t-1, 3, num_bins)

            loss_roll = F.cross_entropy(
                rotation_logits_reshaped[:, :, 0, :].reshape(-1, num_bins),
                rotation_targets[:, :, 0].reshape(-1).long(),
                reduction='mean'
            )
            loss_pitch = F.cross_entropy(
                rotation_logits_reshaped[:, :, 1, :].reshape(-1, num_bins),
                rotation_targets[:, :, 1].reshape(-1).long(),
                reduction='mean'
            )
            loss_yaw = F.cross_entropy(
                rotation_logits_reshaped[:, :, 2, :].reshape(-1, num_bins),
                rotation_targets[:, :, 2].reshape(-1).long(),
                reduction='mean'
            )
            loss_rotation = (loss_roll + loss_pitch + loss_yaw) / 3.0

            # Gripper loss (cross entropy)
            loss_gripper = F.cross_entropy(
                gripper_logits.reshape(-1, 2),
                gripper_targets.reshape(-1).long(),
                reduction='mean'
            )

            # Total loss
            loss = loss_rotation + loss_gripper

            # è®¡ç®—å‡†ç¡®ç‡
            with torch.no_grad():
                pred_roll = torch.argmax(rotation_logits_reshaped[:, :, 0, :], dim=-1)
                pred_pitch = torch.argmax(rotation_logits_reshaped[:, :, 1, :], dim=-1)
                pred_yaw = torch.argmax(rotation_logits_reshaped[:, :, 2, :], dim=-1)

                acc_roll = (pred_roll == rotation_targets[:, :, 0]).float().mean().item()
                acc_pitch = (pred_pitch == rotation_targets[:, :, 1]).float().mean().item()
                acc_yaw = (pred_yaw == rotation_targets[:, :, 2]).float().mean().item()

                rotation_acc_roll_sum += acc_roll
                rotation_acc_pitch_sum += acc_pitch
                rotation_acc_yaw_sum += acc_yaw

                pred_gripper = torch.argmax(gripper_logits, dim=-1)
                acc_gripper = (pred_gripper == gripper_targets).float().mean().item()
                gripper_acc_sum += acc_gripper

                acc_step_count += 1

            # åå‘ä¼ æ’­
            accelerator.backward(loss)

            # æ¢¯åº¦è£å‰ª
            if hasattr(args, 'max_grad_norm') and args.max_grad_norm > 0:
                accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()
            if scheduler is not None:
                scheduler.step()

            # è®°å½•
            epoch_loss += loss.item()
            epoch_loss_rotation += loss_rotation.item()
            epoch_loss_gripper += loss_gripper.item()
            step_count += 1

            # SwanLabæ—¥å¿—
            global_step = step + epoch_id * len(dataloader)
            should_log = (accelerator.is_main_process and
                         hasattr(args, 'swanlab_run') and args.swanlab_run is not None and
                         hasattr(args, 'logging_steps') and global_step % args.logging_steps == 0)

            if should_log:
                try:
                    import swanlab
                    current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']

                    log_data = {
                        "train_loss": loss.item(),
                        "train_loss_rotation": loss_rotation.item(),
                        "train_loss_roll": loss_roll.item(),
                        "train_loss_pitch": loss_pitch.item(),
                        "train_loss_yaw": loss_yaw.item(),
                        "train_loss_gripper": loss_gripper.item(),
                        "learning_rate": current_lr,
                        "epoch": epoch_id,
                        "step": global_step
                    }

                    # æ¯200æ­¥è®°å½•å‡†ç¡®ç‡
                    if global_step % 200 == 0 and acc_step_count > 0:
                        avg_acc_roll = rotation_acc_roll_sum / acc_step_count
                        avg_acc_pitch = rotation_acc_pitch_sum / acc_step_count
                        avg_acc_yaw = rotation_acc_yaw_sum / acc_step_count
                        avg_acc_rotation = (avg_acc_roll + avg_acc_pitch + avg_acc_yaw) / 3.0
                        avg_acc_gripper = gripper_acc_sum / acc_step_count

                        log_data.update({
                            "train_acc_roll": avg_acc_roll,
                            "train_acc_pitch": avg_acc_pitch,
                            "train_acc_yaw": avg_acc_yaw,
                            "train_acc_rotation": avg_acc_rotation,
                            "train_acc_gripper": avg_acc_gripper,
                        })

                        # é‡ç½®
                        rotation_acc_roll_sum = 0.0
                        rotation_acc_pitch_sum = 0.0
                        rotation_acc_yaw_sum = 0.0
                        gripper_acc_sum = 0.0
                        acc_step_count = 0

                    swanlab.log(log_data, step=global_step)

                    # æ‰“å°
                    log_msg = f"Step {global_step}: loss={loss.item():.4f}, "
                    log_msg += f"loss_rot={loss_rotation.item():.4f} (r={loss_roll.item():.4f},p={loss_pitch.item():.4f},y={loss_yaw.item():.4f}), "
                    log_msg += f"loss_grip={loss_gripper.item():.4f}"
                    if "train_acc_rotation" in log_data:
                        log_msg += f", acc_rot={log_data['train_acc_rotation']:.4f}"
                        log_msg += f" (r={log_data['train_acc_roll']:.4f},p={log_data['train_acc_pitch']:.4f},y={log_data['train_acc_yaw']:.4f})"
                        log_msg += f", acc_grip={log_data['train_acc_gripper']:.4f}"
                    log_msg += f", lr={current_lr:.2e}"
                    print(log_msg)
                except Exception as e:
                    print(f"Warning: Failed to log to SwanLab: {e}")

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'loss_rot': f"{loss_rotation.item():.4f}",
                'loss_grip': f"{loss_gripper.item():.4f}",
                'avg_loss': f"{epoch_loss/step_count:.4f}"
            })

    avg_loss = epoch_loss / step_count if step_count > 0 else 0
    avg_loss_rotation = epoch_loss_rotation / step_count if step_count > 0 else 0
    avg_loss_gripper = epoch_loss_gripper / step_count if step_count > 0 else 0

    return avg_loss, avg_loss_rotation, avg_loss_gripper


def main():
    parser = argparse.ArgumentParser(description='Multi-View Rotation and Gripper Prediction Training')

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_root', type=str, nargs='+', required=True,
                       help='Data root directory (single path or list of task paths for multi-task training)')
    parser.add_argument('--trail_start', type=int, default=None,
                       help='Starting trail number (e.g., 1 for trail_1). If None, use all trails.')
    parser.add_argument('--trail_end', type=int, default=None,
                       help='Ending trail number (e.g., 50 for trail_50). If None, use all trails.')
    parser.add_argument('--sequence_length', type=int, default=5, help='Sequence length (including initial frame)')
    parser.add_argument('--image_size', type=int, default=256, help='Image size')
    parser.add_argument('--num_workers', type=int, default=4, help='Number of dataloader workers')

    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--scene_bounds', type=str,
                       default="0,-0.45,-0.05,0.8,0.55,0.6",
                       help='Scene bounds as comma-separated values: x_min,y_min,z_min,x_max,y_max,z_max')
    parser.add_argument('--transform_augmentation_xyz', type=float, nargs=3,
                       default=[0.0, 0.0, 0.0],
                       help='Transform augmentation for xyz')
    parser.add_argument('--transform_augmentation_rpy', type=float, nargs=3,
                       default=[0.0, 0.0, 0.0],
                       help='Transform augmentation for roll/pitch/yaw')
    parser.add_argument('--wan_type', type=str,
                       default='5B_TI2V_RGB_HEATMAP_MV_ROT_GRIP',
                       help='Wan model type')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_dim', type=int, default=512, help='Hidden dimension')
    parser.add_argument('--num_rotation_bins', type=int, default=72, help='Number of rotation bins')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout rate')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--num_epochs', type=int, default=50, help='Number of epochs')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay')
    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Max gradient norm')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=4, help='Gradient accumulation steps')

    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument('--output_path', type=str, required=True, help='Output directory')
    parser.add_argument('--save_epoch_interval', type=int, default=5, help='Save checkpoint every N epochs')
    parser.add_argument('--logging_steps', type=int, default=10, help='Log every N steps')
    parser.add_argument('--swanlab_project', type=str, default='mv_rot_grip', help='SwanLab project name')
    parser.add_argument('--swanlab_experiment', type=str, default=None, help='SwanLab experiment name')

    # VAEå‚æ•°
    parser.add_argument('--model_base_path', type=str,
                       default='/data/lpy/huggingface/Wan2.2-TI2V-5B-fused',
                       help='Base model path for VAE')
    parser.add_argument('--heatmap_latent_scale', type=float, default=1.0, help='Heatmap latent scale factor')
    parser.add_argument('--latent_noise_std', type=float, default=0.1,
                       help='Standard deviation of Gaussian noise added to latents (for robustness training)')

    args = parser.parse_args()

    # è§£æ scene_bounds å­—ç¬¦ä¸²ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨
    if isinstance(args.scene_bounds, str):
        args.scene_bounds = [float(x.strip()) for x in args.scene_bounds.split(',')]
        if len(args.scene_bounds) != 6:
            raise ValueError(f"scene_bounds must have 6 values, got {len(args.scene_bounds)}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_path, exist_ok=True)

    # å…ˆåˆå§‹åŒ– Accelerator ä»¥è·å–æ­£ç¡®çš„è®¾å¤‡
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=False)],
    )

    # è·å–å½“å‰è¿›ç¨‹çš„è®¾å¤‡
    current_device = accelerator.device
    print(f"Process {accelerator.process_index}: using device {current_device}")

    # åˆå§‹åŒ–SwanLab (åªåœ¨ä¸»è¿›ç¨‹)
    if accelerator.is_main_process:
        try:
            import swanlab
            args.swanlab_run = swanlab.init(
                project=args.swanlab_project,
                experiment_name=args.swanlab_experiment,
                config=vars(args)
            )
            print("âœ“ SwanLab initialized")
        except Exception as e:
            print(f"Warning: Failed to initialize SwanLab: {e}")
            args.swanlab_run = None
    else:
        args.swanlab_run = None

    # åŠ è½½VAEåˆ°å½“å‰è¿›ç¨‹çš„GPU
    print(f"Loading VAE to {current_device}...")
    model_manager = ModelManager(torch_dtype=torch.bfloat16, device=current_device)
    model_manager.load_model(f"{args.model_base_path}/Wan2.2_VAE.pth")
    vae = model_manager.fetch_model("wan_video_vae")
    vae_extractor = VAEFeatureExtractor(vae, device=current_device, torch_dtype=torch.bfloat16)
    print(f"âœ“ VAE loaded to {current_device}")

    # åˆ›å»ºæ•°æ®é›†
    print("Creating dataset...")

    # å¤„ç†data_root - æ”¯æŒå•ä»»åŠ¡æˆ–å¤šä»»åŠ¡
    data_roots = args.data_root if isinstance(args.data_root, list) else [args.data_root]

    # å¤šä»»åŠ¡è®­ç»ƒ: ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºæ•°æ®é›†
    if len(data_roots) > 1:
        print(f"Multi-task training mode: {len(data_roots)} tasks")
        datasets = []
        for task_idx, task_root in enumerate(data_roots):
            print(f"  Loading task {task_idx+1}/{len(data_roots)}: {task_root}")
            task_dataset = HeatmapDatasetFactory.create_robot_trajectory_dataset(
                data_root=task_root,
                sequence_length=args.sequence_length,
                step_interval=1,
                min_trail_length=10,
                image_size=(args.image_size, args.image_size),
                sigma=1.5,
                augmentation=True,
                mode="train",
                scene_bounds=args.scene_bounds,
                transform_augmentation_xyz=args.transform_augmentation_xyz,
                transform_augmentation_rpy=args.transform_augmentation_rpy,
                debug=args.debug,
                colormap_name="jet",
                repeat=1,
                wan_type=args.wan_type,
                rotation_resolution=360//args.num_rotation_bins,
                trail_start=args.trail_start,
                trail_end=args.trail_end,
            )
            print(f"    âœ“ Task {task_idx+1} loaded: {len(task_dataset)} samples")
            datasets.append(task_dataset)

        # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
        dataset = ConcatDataset(datasets)
        print(f"âœ“ Multi-task dataset created: {len(dataset)} samples (from {len(data_roots)} tasks)")
    else:
        # å•ä»»åŠ¡è®­ç»ƒ
        print(f"Single-task training mode: {data_roots[0]}")
        dataset = HeatmapDatasetFactory.create_robot_trajectory_dataset(
            data_root=data_roots[0],
            sequence_length=args.sequence_length,
            step_interval=1,
            min_trail_length=10,
            image_size=(args.image_size, args.image_size),
            sigma=1.5,
            augmentation=True,
            mode="train",
            scene_bounds=args.scene_bounds,
            transform_augmentation_xyz=args.transform_augmentation_xyz,
            transform_augmentation_rpy=args.transform_augmentation_rpy,
            debug=args.debug,
            colormap_name="jet",
            repeat=1,
            wan_type=args.wan_type,
            rotation_resolution=360//args.num_rotation_bins,
            trail_start=args.trail_start,
            trail_end=args.trail_end,
        )
        print(f"âœ“ Dataset created: {len(dataset)} samples")

    # åˆ›å»ºDataLoader
    from functools import partial
    collate_fn = partial(
        collate_fn_with_vae,
        vae_extractor=vae_extractor,
        heatmap_latent_scale=args.heatmap_latent_scale,
        latent_noise_std=args.latent_noise_std  # æ·»åŠ latentå™ªå£°å¢å¼º
    )

    dataloader = DataLoader(
        dataset,
        batch_size=1,  # å›ºå®šä¸º1
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=False,  # Must be False since collate_fn returns CUDA tensors
        persistent_workers=args.num_workers > 0,
        collate_fn=collate_fn,
        drop_last=True,
    )
    print(f"âœ“ DataLoader created")

    # åˆ›å»ºæ¨¡å‹
    print("Creating model...")
    model = MultiViewRotationGripperPredictor(
        rgb_channels=48,  # VAE latent channels for RGB
        heatmap_channels=48,  # VAE latent channels for Heatmap
        hidden_dim=args.hidden_dim,
        num_views=3, 
        num_rotation_bins=args.num_rotation_bins,
        temporal_upsample_factor=4,
        dropout=args.dropout,
    )
    # Convert model to bfloat16 to match VAE feature dtype
    model = model.to(dtype=torch.bfloat16)
    print("âœ“ Model created (dtype: bfloat16)")

    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay,
        eps=1e-6,
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=args.num_epochs * len(dataloader),
        eta_min=1e-6
    )

    # ä½¿ç”¨å‰é¢å·²åˆ›å»ºçš„ accelerator æ¥å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨ç­‰
    model, optimizer, dataloader, scheduler = accelerator.prepare(
        model, optimizer, dataloader, scheduler
    )

    print("âœ“ Training setup complete")
    print(f"  - Device: {accelerator.device}")
    print(f"  - Num GPUs: {accelerator.num_processes}")
    print(f"  - Gradient accumulation steps: {args.gradient_accumulation_steps}")

    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ Starting training...")
    for epoch_id in range(args.num_epochs):
        if accelerator.is_main_process:
            print(f"\n{'='*60}")
            print(f"Epoch {epoch_id+1}/{args.num_epochs}")
            print(f"{'='*60}")

        avg_loss, avg_loss_rotation, avg_loss_gripper = train_epoch(
            model=model,
            dataloader=dataloader,
            optimizer=optimizer,
            scheduler=scheduler,
            accelerator=accelerator,
            epoch_id=epoch_id,
            args=args,
        )

        if accelerator.is_main_process:
            print(f"\nEpoch {epoch_id+1} Summary:")
            print(f"  Avg Loss: {avg_loss:.4f}")
            print(f"  Avg Rotation Loss: {avg_loss_rotation:.4f}")
            print(f"  Avg Gripper Loss: {avg_loss_gripper:.4f}")

            # ä¿å­˜checkpoint
            if (epoch_id + 1) % args.save_epoch_interval == 0 or (epoch_id + 1) == args.num_epochs:
                checkpoint_path = os.path.join(args.output_path, f"epoch-{epoch_id+1}.pth")
                unwrapped_model = accelerator.unwrap_model(model)
                torch.save({
                    'epoch': epoch_id + 1,
                    'model_state_dict': unwrapped_model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'avg_loss': avg_loss,
                }, checkpoint_path)
                print(f"  âœ“ Checkpoint saved: {checkpoint_path}")

    if accelerator.is_main_process:
        print("\nğŸ‰ Training completed!")
        if args.swanlab_run is not None:
            args.swanlab_run.finish()


if __name__ == "__main__":
    main()
