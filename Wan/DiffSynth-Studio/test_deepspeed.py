#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•DeepSpeedæ˜¯å¦æ­£å¸¸å·¥ä½œçš„è„šæœ¬
ä½¿ç”¨æ–¹æ³•: accelerate launch --use_deepspeed --deepspeed_config_file=config.json --num_processes=2 test_deepspeed.py
"""

import torch
from accelerate import Accelerator
import os

def main():
    print(f"ğŸ” Environment Variables:")
    print(f"   - LOCAL_RANK: {os.environ.get('LOCAL_RANK', 'Not set')}")
    print(f"   - WORLD_SIZE: {os.environ.get('WORLD_SIZE', 'Not set')}")
    print(f"   - RANK: {os.environ.get('RANK', 'Not set')}")

    # åˆå§‹åŒ–Accelerator
    print(f"\nğŸš€ Initializing Accelerator...")
    accelerator = Accelerator()

    # æ£€æŸ¥DeepSpeedçŠ¶æ€
    print(f"\nğŸ” Accelerator State:")
    print(f"   - distributed_type: {accelerator.state.distributed_type}")
    print(f"   - use_deepspeed: {accelerator.state.use_deepspeed}")
    print(f"   - deepspeed_plugin: {accelerator.state.deepspeed_plugin is not None}")

    if accelerator.state.deepspeed_plugin:
        print(f"   âœ… DeepSpeed is ENABLED!")
        config = accelerator.state.deepspeed_plugin.deepspeed_config
        print(f"   - ZeRO stage: {config.get('zero_optimization', {}).get('stage', 'Unknown')}")
    else:
        print(f"   âŒ DeepSpeed is NOT enabled!")

    # æ£€æŸ¥GPUæ˜¾å­˜
    if torch.cuda.is_available():
        print(f"\nğŸ” GPU Memory Status:")
        for i in range(torch.cuda.device_count()):
            mem_allocated = torch.cuda.memory_allocated(i) / 1024**3
            mem_reserved = torch.cuda.memory_reserved(i) / 1024**3
            mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   - GPU {i}: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved, {mem_total:.2f}GB total")

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹æµ‹è¯•
    print(f"\nğŸ§ª Testing with a small model...")
    model = torch.nn.Linear(1000, 1000)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # ä½¿ç”¨accelerator.prepare
    model, optimizer = accelerator.prepare(model, optimizer)

    print(f"   - Model type: {type(model)}")
    print(f"   - Is DeepSpeed model: {hasattr(model, 'module') and 'DeepSpeed' in str(type(model))}")

    # æ£€æŸ¥æ˜¾å­˜å˜åŒ–
    if torch.cuda.is_available():
        print(f"\nğŸ” GPU Memory After Model Preparation:")
        for i in range(torch.cuda.device_count()):
            mem_allocated = torch.cuda.memory_allocated(i) / 1024**3
            mem_reserved = torch.cuda.memory_reserved(i) / 1024**3
            print(f"   - GPU {i}: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved")

    print(f"\nâœ… DeepSpeed test completed!")

if __name__ == "__main__":
    main()